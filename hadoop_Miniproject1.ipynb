{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hadoop_Miniproject1.ipynb","provenance":[],"collapsed_sections":["j9bT9M1yvyXG","WHuUqOJX1eZ9","kPPGOa-5OfeG","jTWgkcUtuoVq","5DwbxpLMt2aN","Iiefrk-guRC9","SLc08BSmvx81","mSNCx9O1I57c","M6Yd-EVi9-WL"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3b1ptr6CQ87Y","executionInfo":{"status":"ok","timestamp":1637609163453,"user_tz":-120,"elapsed":62019,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaGRMxyeD3t85VD8VHhTPNz3NiYujdbYaaDwHMw=s64","userId":"07034748551478156399"}},"outputId":"1d5d070c-4071-4542-b2f7-bf81d899a779"},"source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","root_path ='/content/drive/MyDrive/mp1-bigdata/' # create this folder on drive"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"Fum4CbSj9KRH"},"source":["# visual declutter\n","!rm -r sample_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hybWJk_jChUy","executionInfo":{"status":"ok","timestamp":1637609164984,"user_tz":-120,"elapsed":42,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaGRMxyeD3t85VD8VHhTPNz3NiYujdbYaaDwHMw=s64","userId":"07034748551478156399"}},"outputId":"1999768a-3ea5-4744-e00d-2e731f2043b0"},"source":["## setting up the most frequently used commands as environment variable to make life easier\n","import os\n","%env hadoop=/usr/local/hadoop-3.3.0/bin/hadoop\n","os.environ[\"hdfs\"] = \"/usr/local/hadoop-3.3.0/bin/hdfs\"\n","os.environ['hadoop_stream']=\"/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar\"\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["env: hadoop=/usr/local/hadoop-3.3.0/bin/hadoop\n"]}]},{"cell_type":"markdown","metadata":{"id":"j9bT9M1yvyXG"},"source":["#download & install Hadoop "]},{"cell_type":"code","metadata":{"id":"bijZAdD_cBMK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637609187742,"user_tz":-120,"elapsed":22791,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaGRMxyeD3t85VD8VHhTPNz3NiYujdbYaaDwHMw=s64","userId":"07034748551478156399"}},"outputId":"9bfd186f-7f76-4529-94cd-0cf7564ae8c8"},"source":["!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-11-22 19:25:59--  https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n","Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 135.181.214.104, 2a01:4f8:10a:201a::2, ...\n","Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 500749234 (478M) [application/x-gzip]\n","Saving to: ‘hadoop-3.3.0.tar.gz’\n","\n","hadoop-3.3.0.tar.gz 100%[===================>] 477.55M  20.9MB/s    in 23s     \n","\n","2021-11-22 19:26:22 (21.2 MB/s) - ‘hadoop-3.3.0.tar.gz’ saved [500749234/500749234]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"nVce513-cBHm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637609213538,"user_tz":-120,"elapsed":25878,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaGRMxyeD3t85VD8VHhTPNz3NiYujdbYaaDwHMw=s64","userId":"07034748551478156399"}},"outputId":"333d63b4-1b79-4706-9d70-e81d159fcaa0"},"source":["!tar -xzvf hadoop-3.3.0.tar.gz"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSDataOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/TrashPolicyDefault.Emptier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/HarFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/PathExistsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/XAttrSetFlag.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/BatchedRemoteIterator.BatchedEntries.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ParentNotDirectoryException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.CreateOpts.CreateParent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/PathIsNotDirectoryException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/PathHandle.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/class-use/CryptoFSDataInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/class-use/CryptoFSDataOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/AbstractFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FsConstants.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/FTPException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/class-use/FTPException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/class-use/FTPFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/FTPFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/UnsupportedMultipartUploaderException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileSystemMultipartUploader.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/BlockLocation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclStatus.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclEntryType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/FsCreateModes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/FsPermission.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/FsAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclEntryScope.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Trash.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/HarFs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FilterFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/BatchListingOperations.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.Rename.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/PathData.PathType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/TouchCommands.Touchz.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.DuplicatedOptionException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.NotEnoughArgumentsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/TouchCommands.Touch.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.UnknownOptionException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.IllegalNumberOfArgumentsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/PathData.FileTypeRequirement.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.TooManyArgumentsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/FindOptions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/Result.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/FilterExpression.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/FindOptions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/Result.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/FilterExpression.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/Expression.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/BaseExpression.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/Expression.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/BaseExpression.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/RawLocalFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSDataInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.CreateOpts.ChecksumParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/StreamCapabilities.StreamCapability.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/DelegationTokenRenewer.Renewable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FsShell.Help.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/PathNotFoundException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ByteBufferReadable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.ChecksumCombineMode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/CanUnbuffer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileContext.Util.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/LocatedFileStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FsShellPermissions.Chown.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.FsPermissionProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/StreamCapabilitiesPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/UnsupportedFileSystemException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/BatchedRemoteIterator.BatchedListEntries.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/OpenFileParameters.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/FunctionsRaisingIOE.BiFunctionRaisingIOE.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/FunctionsRaisingIOE.CallableRaisingIOE.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/AbstractFSBuilderImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/FunctionsRaisingIOE.FunctionRaisingIOE.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/FutureDataInputStreamBuilderImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/FsLinkResolution.FsLinkResolutionFunction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ReadOption.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/GlobalStorageStatistics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.CreateOpts.BufferSize.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.FileStatusProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.FsPermissionProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.LocalFileSystemPathHandleProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/DUHelper.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/StorageType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.HandleOpt.Data.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/CommonConfigurationKeysPublic.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/GetSpaceUsed.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileSystem.Statistics.StatisticsData.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/GetSpaceUsed.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/DelegationTokenRenewer.RenewAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileAlreadyExistsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.FsPermissionProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ByteBufferPositionedReadable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ChecksumFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ViewFileSystem.MountPoint.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ViewFs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ViewFs.MountPoint.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ViewFileSystem.MountPoint.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ViewFs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ViewFs.MountPoint.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ViewFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/NotInMountpointException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/Constants.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ConfigUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ViewFileSystemUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ViewFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/NotInMountpointException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/Constants.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ConfigUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/QuotaUsage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ContentSummary.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FsServerDefaults.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/PartialListing.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.CreateOpts.Perms.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileSystem.Statistics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FsShellPermissions.Chgrp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/IOUtils.NullOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/IOUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/VIntWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/CompressedWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/LongWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/EnumSetWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/RawComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BytesWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/VLongWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Sorter.RawKeyValueIterator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SetFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SetFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/util/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/util/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/util/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/util/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/util/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/class-use/DummyErasureDecoder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/class-use/DummyErasureEncoder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/class-use/ErasureCodeConstants.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/class-use/ErasureCodeNative.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/class-use/ECSchema.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/grouper/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/grouper/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/grouper/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/grouper/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/grouper/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/codec/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/codec/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/codec/class-use/DummyErasureCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/ErasureCodeConstants.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/ErasureCodeNative.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/ECSchema.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/NullWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ShortWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/GenericWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.ValueBytes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Sorter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ByteBufferPool.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/WritableFactories.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/FloatWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/NullWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/WritableComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ByteWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ElasticByteBufferPool.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/Writable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.Reader.Option.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SecureIOUtils.AlreadyExistsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/IOUtils.NullOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/IOUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/VIntWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/CompressedWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/LongWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/EnumSetWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/RawComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BytesWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/VLongWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Sorter.RawKeyValueIterator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SetFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SetFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/NullWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ShortWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/GenericWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.ValueBytes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Sorter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ByteBufferPool.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/WritableFactories.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/FloatWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/NullWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/WritableComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ByteWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ElasticByteBufferPool.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/Writable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.Reader.Option.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SecureIOUtils.AlreadyExistsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ArrayFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MultipleIOException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BinaryComparable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/WritableUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.CompressionType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MD5Hash.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/WritableComparable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/TwoDArrayWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ByteWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/Stringifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MD5Hash.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/DoubleWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SetFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/WritableFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/AbstractMapWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/Closeable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Writer.Option.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/IntWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/UTF8.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ArrayPrimitiveWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BloomMapFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ReadaheadPool.ReadaheadRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ArrayFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/FloatWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/Text.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ShortWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.Writer.Option.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ArrayWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/DataInputByteBuffer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Metadata.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/VersionedWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ObjectWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SortedMapWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SecureIOUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MultipleIOException.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/IntWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/LongWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BooleanWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/LongWritable.DecreasingComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/VersionMismatchException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/DataOutputOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.Merger.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Reader.Option.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Sorter.SegmentDescriptor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BloomMapFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ArrayFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/Text.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BooleanWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/DoubleWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/DefaultStringifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BloomMapFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BytesWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ArrayFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MultipleIOException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIOException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/Errno.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.PmemMappedRegion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.NoMlockCacheManipulator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.SupportState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.Pmem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.CacheManipulator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.Stat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.Windows.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.Windows.AccessRight.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BinaryComparable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/WritableUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.CompressionType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MD5Hash.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/WritableComparable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/TwoDArrayWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ByteWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/Stringifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MD5Hash.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/DoubleWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SetFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/WritableFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/AbstractMapWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/Closeable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Writer.Option.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/IntWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/UTF8.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/Utils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/RawComparable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/Compression.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/MetaBlockDoesNotExist.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/Compression.Algorithm.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/Utils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/RawComparable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/Compression.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/MetaBlockDoesNotExist.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/Compression.Algorithm.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/Utils.Version.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/SimpleBufferedOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/TFile.Reader.Scanner.Entry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/TFile.Reader.Scanner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/BoundedRangeFileInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/TFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/TFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/MetaBlockAlreadyExists.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/TFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/Utils.Version.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/TFile.Reader.Scanner.Entry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/TFile.Reader.Scanner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/TFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/TFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/MetaBlockAlreadyExists.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/TFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ArrayPrimitiveWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BloomMapFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/JavaSerialization.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/WritableSerialization.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/class-use/JavaSerialization.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/class-use/WritableSerialization.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/class-use/JavaSerializationComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/AvroSpecificSerialization.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/AvroReflectSerializable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/class-use/AvroSpecificSerialization.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/class-use/AvroReflectSerializable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/class-use/AvroReflectSerialization.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/class-use/AvroSerialization.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/AvroSerialization.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/JavaSerializationComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressionCodecFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CodecPool.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressionCodec.Util.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressionOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/SnappyCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/BlockDecompressorStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/BZip2DummyCompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/Bzip2Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/Bzip2Decompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/CBZip2InputStream.STATE.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/BZip2DummyDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/CBZip2InputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/CBZip2OutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/BZip2Constants.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/Bzip2Compressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CodecConstants.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressionInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/SplittableCompressionCodec.READ_MODE.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/BZip2Codec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/SplittableCompressionCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/BlockCompressorStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressionCodecFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CodecPool.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressionCodec.Util.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressionOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/SnappyCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/BlockDecompressorStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CodecConstants.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressionInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/SplittableCompressionCodec.READ_MODE.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/BZip2Codec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/SplittableCompressionCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/BlockCompressorStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/SplitCompressionInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DoNotPool.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressionCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DirectDecompressionCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/Lz4Codec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/Compressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DirectDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/GzipCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/Decompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DefaultCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/GzipCodec.GzipOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/ZStandardCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DeflateCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressorStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/PassthroughCodec.PassthroughDecompressorStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/PassthroughCodec.StubDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DecompressorStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/PassthroughCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/lz4/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/lz4/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/lz4/class-use/Lz4Decompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/lz4/class-use/Lz4Compressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/SplitCompressionInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DoNotPool.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zstd/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zstd/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zstd/class-use/ZStandardCompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zstd/class-use/ZStandardDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zstd/class-use/ZStandardDecompressor.ZStandardDirectDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressionCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DirectDecompressionCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/Lz4Codec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/Compressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DirectDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/GzipCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/BuiltInGzipDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/BuiltInZlibDeflater.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibDecompressor.CompressionHeader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibCompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibCompressor.CompressionStrategy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibCompressor.CompressionLevel.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibDecompressor.ZlibDirectDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibCompressor.CompressionHeader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/BuiltInZlibInflater.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/Decompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DefaultCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/GzipCodec.GzipOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/ZStandardCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DeflateCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressorStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/PassthroughCodec.PassthroughDecompressorStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/PassthroughCodec.StubDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DecompressorStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/snappy/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/snappy/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/snappy/class-use/SnappyCompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/snappy/class-use/SnappyDecompressor.SnappyDirectDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/snappy/class-use/SnappyDecompressor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/PassthroughCodec.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ReadaheadPool.ReadaheadRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ArrayFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/FloatWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/Text.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ShortWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.Writer.Option.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ArrayWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/DataInputByteBuffer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Metadata.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/VersionedWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ObjectWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SortedMapWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SecureIOUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MultipleIOException.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/IntWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/LongWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BooleanWritable.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/LongWritable.DecreasingComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/VersionMismatchException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/DataOutputOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.Merger.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/Idempotent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/MultiException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicies.MultipleLinearRandomRetry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/FailoverProxyProvider.ProxyInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/DefaultFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicies.MultipleLinearRandomRetry.Pair.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicy.RetryAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/AtMostOnce.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryProxy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/FailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicy.RetryAction.RetryDecision.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicies.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Reader.Option.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Sorter.SegmentDescriptor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BloomMapFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ArrayFile.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/Text.Comparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BooleanWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/DoubleWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/DefaultStringifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BloomMapFile.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BytesWritable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/UnsupportedCodecException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/class-use/UnsupportedCodecException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyShell.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.DelegationTokenExtension.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderExtension.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/CachingKeyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProvider.KeyVersion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/UserProvider.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyShell.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderDelegationTokenExtension.DelegationTokenExtension.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderExtension.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderDelegationTokenExtension.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/CachingKeyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProvider.KeyVersion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/UserProvider.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderCryptoExtension.EncryptedKeyVersion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderExtension.Extension.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderCryptoExtension.CryptoExtension.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/JavaKeyStoreProvider.KeyMetadata.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProvider.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProvider.Metadata.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/JavaKeyStoreProvider.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.EncryptedKeyVersion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderExtension.Extension.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.CryptoExtension.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.KeyMetadata.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProvider.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProvider.Metadata.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSClientProvider.KMSTokenRenewer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSClientProvider.KMSMetadata.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSClientProvider.KMSKeyVersion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSClientProvider.KMSEncryptedKeyVersion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSClientProvider.KMSTokenRenewer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSClientProvider.KMSMetadata.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSClientProvider.KMSKeyVersion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSClientProvider.KMSEncryptedKeyVersion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/ValueQueue.SyncGenerationPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSDelegationToken.KMSDelegationTokenIdentifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/LoadBalancingKMSClientProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSClientProvider.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/ValueQueue.QueueRefiller.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/ValueQueue.SyncGenerationPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSDelegationToken.KMSDelegationTokenIdentifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSClientProvider.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/ValueQueue.QueueRefiller.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/random/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/random/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/random/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/random/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/random/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/HadoopIllegalArgumentException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/IrqHandler.Interrupted.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/LaunchableService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/IrqHandler.InterruptData.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/LauncherExitCodes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/AbstractLaunchableService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/IrqHandler.Interrupted.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/LaunchableService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/HadoopUncaughtExceptionHandler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/IrqHandler.InterruptData.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/LauncherExitCodes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/AbstractLaunchableService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/ServiceLauncher.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/ServiceLaunchException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/ServiceLauncher.MinimalGenericOptionsParser.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/LauncherArguments.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/InterruptEscalator.ServiceForcedShutdown.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/ServiceLauncher.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/ServiceLaunchException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/ServiceLauncher.MinimalGenericOptionsParser.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/LauncherArguments.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/InterruptEscalator.ServiceForcedShutdown.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/LoggingStateChangeListener.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/CompositeService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/AbstractService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/ServiceStateChangeListener.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/ServiceOperations.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/ServiceStateException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/LoggingStateChangeListener.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/CompositeService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/AbstractService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/ServiceStateChangeListener.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/ServiceOperations.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/ServiceStateException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/Service.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/CompositeService.CompositeServiceShutdownHook.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/LifecycleEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/Service.STATE.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/ServiceOperations.ServiceListeners.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/ServiceStateModel.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/Service.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/CompositeService.CompositeServiceShutdownHook.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/LifecycleEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/Service.STATE.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/ServiceOperations.ServiceListeners.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/ServiceStateModel.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ConfigRedactor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Configuration.DeprecationDelta.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ConfigRedactor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Configuration.DeprecationDelta.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ReconfigurableBase.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/StorageUnit.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Configurable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Configuration.IntegerRanges.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ReconfigurationUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Configured.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Reconfigurable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ConfServlet.BadFormatException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Configuration.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ReconfigurationException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ReconfigurationServlet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ReconfigurationUtil.PropertyChange.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/StorageSize.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ReconfigurableBase.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/StorageUnit.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Configurable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Configuration.IntegerRanges.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ReconfigurationUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Configured.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Reconfigurable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ConfServlet.BadFormatException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Configuration.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ReconfigurationException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ReconfigurationServlet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ReconfigurationUtil.PropertyChange.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/StorageSize.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.TraceAdminService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.SpanReceiverListInfo.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.TraceAdminService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.SpanReceiverListInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.TraceAdminService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/SpanReceiverInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.TraceAdminService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.SpanReceiverListInfo.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.TraceAdminService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.SpanReceiverListInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.TraceAdminService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/SpanReceiverInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.SpanReceiverListInfoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/SpanReceiverInfoBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminProtocol.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.TraceAdminService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ConfigPair.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ConfigPair.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ConfigPairOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminProtocolPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.SpanReceiverListInfoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/SpanReceiverInfoBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminProtocol.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.TraceAdminService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ConfigPair.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ConfigPair.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ConfigPairOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminProtocolPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtocolInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngine2.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcMultiplexer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ExternalCall.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RefreshResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/WeightedTimeCostProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/Client.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/Server.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DecayRpcScheduler.MetricsProxy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DefaultRpcScheduler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RetryCache.CacheEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RPC.RpcKind.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/StandbyException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcClientException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DecayRpcScheduler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RPC.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/Server.Call.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RemoteException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/WeightedRoundRobinMultiplexer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtocolMetaInfoServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcScheduler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcEngine.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/UnexpectedServerException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtocolMetaInfoPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProcessingDetails.Timing.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/FairCallQueue.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DecayRpcScheduler.DecayTask.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngineCallback2.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DecayRpcSchedulerMXBean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/IpcException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngine.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/UserIdentityProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcNoSuchProtocolException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProxyCombiner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcClientUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/WritableRpcEngine.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/CallQueueManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcInvocationHandler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RefreshHandler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/CallerContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/Server.Connection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RetryCache.CacheEntryWithPayload.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtocolSignature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngine2.Server.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcWritable.Buffer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DefaultCostProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/WritableRpcEngine.Server.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcServerException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RetriableException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/FairCallQueueMXBean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngineCallback.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RefreshRegistry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtocolProxy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngine.Server.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcNoSuchMethodException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/CostProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/AsyncCallLimitExceededException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RPC.VersionMismatch.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RPC.Server.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/VersionedProtocol.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/CallerContext.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/class-use/GenericRefreshProtocolClientSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/class-use/GenericRefreshProtocolServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/class-use/RefreshCallQueueProtocolClientSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/class-use/RefreshCallQueueProtocolServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.SaslAuthOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.IpcConnectionContextProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolSignatureProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolVersionProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCCallerContextProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcResponseHeaderProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.SaslAuth.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCTraceInfoProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcRequestHeaderProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolVersionProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos.RequestHeaderProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.SaslAuth.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcRequestHeaderProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcResponseHeaderProto.RpcErrorCodeProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.UserInformationProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngine2Protos.RequestHeaderProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCTraceInfoProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngine2Protos.RequestHeaderProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolInfoService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.SaslAuthOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.IpcConnectionContextProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolSignatureProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolVersionProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCCallerContextProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcResponseHeaderProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.SaslAuth.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCTraceInfoProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcRequestHeaderProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolVersionProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngineProtos.RequestHeaderProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.SaslAuth.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcRequestHeaderProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcResponseHeaderProto.RpcErrorCodeProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.UserInformationProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngineProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngine2Protos.RequestHeaderProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCTraceInfoProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngine2Protos.RequestHeaderProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolInfoService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.SaslState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcResponseHeaderProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcRequestHeaderProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolSignatureProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcRequestHeaderProto.OperationProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.UserInformationProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.IpcConnectionContextProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolInfoService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcResponseHeaderProto.RpcStatusProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngine2Protos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngineProtos.RequestHeaderProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolSignatureProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCCallerContextProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngineProtos.RequestHeaderProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcResponseHeaderProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolInfoService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcKindProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCTraceInfoProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.IpcConnectionContextProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.UserInformationProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngine2Protos.RequestHeaderProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCCallerContextProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolVersionProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolInfoService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.SaslState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcResponseHeaderProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcRequestHeaderProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolSignatureProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcRequestHeaderProto.OperationProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.UserInformationProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.IpcConnectionContextProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolInfoService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcResponseHeaderProto.RpcStatusProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngine2Protos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos.RequestHeaderProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolSignatureProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCCallerContextProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos.RequestHeaderProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcResponseHeaderProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolInfoService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcKindProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCTraceInfoProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.IpcConnectionContextProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.UserInformationProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngine2Protos.RequestHeaderProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCCallerContextProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolVersionProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolInfoService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/FailoverFailedException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceProtocolHelper.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAStateChangeRequestInfoProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAServiceStateProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAStateChangeRequestInfoProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAStateChangeRequestInfoProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.ZKFCProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAStateChangeRequestInfoProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAServiceStateProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAStateChangeRequestInfoProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAStateChangeRequestInfoProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.ZKFCProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.ZKFCProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAServiceProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.ZKFCProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.ZKFCProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAServiceProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAServiceProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HARequestSource.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAServiceProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.ZKFCProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAServiceProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.ZKFCProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.ZKFCProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAServiceProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAServiceProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HARequestSource.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAServiceProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/FenceMethod.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/ActiveStandbyElector.ActiveNotFoundException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/ShellCommandFencer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/PowerShellFencer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/FailoverFailedException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceProtocolHelper.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/FenceMethod.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/ActiveStandbyElector.ActiveNotFoundException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/ShellCommandFencer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/PowerShellFencer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/ActiveStandbyElector.ActiveStandbyElectorCallback.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceProtocol.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAAdmin.UsageInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceProtocol.HAServiceState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/ServiceFailedException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HealthCheckFailedException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceTarget.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/BadFencingConfigurationException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceProtocol.RequestSource.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/SshFenceByTcpPort.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceProtocol.StateChangeRequestInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/HAServiceProtocolPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/class-use/ZKFCProtocolClientSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/class-use/HAServiceProtocolPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/class-use/ZKFCProtocolPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/ZKFCProtocolPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/ActiveStandbyElector.ActiveStandbyElectorCallback.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceProtocol.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAAdmin.UsageInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceProtocol.HAServiceState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/ServiceFailedException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HealthCheckFailedException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceTarget.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/BadFencingConfigurationException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceProtocol.RequestSource.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/SshFenceByTcpPort.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceProtocol.StateChangeRequestInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSystem.AbstractCallback.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsJsonBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsRecordBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSource.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/GlobFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/RegexFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/class-use/GlobFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/class-use/RegexFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/MBeans.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/SampleStat.MinMax.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/QuantileEstimator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/MetricsCache.Record.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/MetricsCache.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/MBeans.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/SampleStat.MinMax.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/QuantileEstimator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/MetricsCache.Record.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/MetricsCache.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/Servers.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/Servers.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsRecord.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSystem.AbstractCallback.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsJsonBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsRecordBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSource.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsRecord.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/AbstractMetric.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsCollector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsTag.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricStringBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsPlugin.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSystem.Callback.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSystemMXBean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsVisitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/AbstractMetric.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsCollector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsTag.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricStringBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsPlugin.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/source/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/source/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/source/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/source/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/source/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSystem.Callback.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSystemMXBean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsVisitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/GraphiteSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/StatsDSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/RollingFileSystemSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/GraphiteSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/StatsDSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/StatsDSink.StatsD.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/GraphiteSink.Graphite.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/PrometheusMetricsSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/FileSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/StatsDSink.StatsD.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/GraphiteSink.Graphite.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.GangliaConfType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/GangliaSink30.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/AbstractGangliaSink.GangliaConfType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/AbstractGangliaSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/AbstractGangliaSink.GangliaSlope.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/GangliaSink31.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.GangliaSlope.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/FileSink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/Metric.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/Metrics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/class-use/Metric.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/class-use/Metrics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/class-use/Metric.Type.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/Metric.Type.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableGaugeInt.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableRollingAverages.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableCounterInt.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MetricsRegistry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/Interns.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableGaugeInt.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableRollingAverages.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableRatesWithAggregation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableCounterInt.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MetricsRegistry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/Interns.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableRates.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableCounter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableGaugeLong.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableStat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableRate.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableQuantiles.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableGauge.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/DefaultMetricsSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableGaugeFloat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableCounterLong.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableMetric.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableRates.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableCounter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableGaugeLong.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableStat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableRate.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableQuantiles.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableGauge.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableCounterLong.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableMetric.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/overview-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/overview-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/allclasses-noframe.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/package-list\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/DeprecatedProperties.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/core-default.xml\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/GroupsMapping.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/UnixShellAPI.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/CommandsManual.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/CredentialProviderAPI.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/DownstreamDev.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/RackAwareness.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/AdminCompatibilityGuide.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/serialized-form.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/index.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/stylesheet.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/index-all.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/constant-values.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/overview-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/help-doc.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/deprecated-list.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/allclasses-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/script.js\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSUtilClient.CorruptedBlocks.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/ReplicaAccessorBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/UnknownCipherSuiteException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.DiffStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/AddErasureCodingPolicyResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/DatanodeInfo.AdminStates.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotAccessControlException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/DatanodeInfo.DatanodeInfoBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshottableDirectoryStatus.Bean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/ZoneReencryptionStatus.State.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.DiffType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/RollingUpgradeInfo.Bean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/EncryptionZone.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/ZoneReencryptionStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CorruptFileBlocks.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/BlockType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReportListing.DiffReportListingEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshottableDirectoryStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CachePoolEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/ECBlockGroupStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CachePoolInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/TrustedChannelResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/ReplaceDatanodeOnFailure.Policy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/BlockPinningException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/PipelineAck.ECN.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/TrustedChannelResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/ReplaceDatanodeOnFailure.Policy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/BlockPinningException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/PipelineAck.ECN.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/DatanodeAdminProperties.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReport.DiffStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/AddErasureCodingPolicyResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/DatanodeInfo.AdminStates.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotAccessControlException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/DatanodeInfo.DatanodeInfoBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshottableDirectoryStatus.Bean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/ZoneReencryptionStatus.State.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReport.DiffType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/RollingUpgradeInfo.Bean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/EncryptionZone.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/ZoneReencryptionStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CorruptFileBlocks.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/BlockType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReportListing.DiffReportListingEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshottableDirectoryStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CachePoolEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/ECBlockGroupStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CachePoolInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/DatanodeAdminProperties.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.SafeModeAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CachePoolStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveInfo.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.UpgradeAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.StoragePolicySatisfierMode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsNamedFileStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CachePoolStats.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReportListing.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsFileStatus.Flags.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveInfo.Expiration.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/ReplicatedBlockStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReport.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.ReencryptAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/OpenFilesIterator.OpenFilesType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.DatanodeReportType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/ZoneReencryptionStatus.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.RollingUpgradeAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReport.DiffReportEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveStats.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.SafeModeAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CachePoolStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveInfo.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.UpgradeAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.StoragePolicySatisfierMode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsNamedFileStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CachePoolStats.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReportListing.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsFileStatus.Flags.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveInfo.Expiration.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/ReplicatedBlockStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.ReencryptAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/OpenFilesIterator.OpenFilesType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.DatanodeReportType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/ZoneReencryptionStatus.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.RollingUpgradeAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.DiffReportEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveStats.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/BasicInetPeer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/class-use/BasicInetPeer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/class-use/NioInetPeer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/NioInetPeer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/CannotObtainBlockLengthException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DeadNodeDetector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSMultipartUploaderFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.StripingChunk.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.VerticalRange.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/LongBitFormat.Enum.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.BlockReadStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.StripingCell.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.AlignedStripe.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.StripingChunk.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.VerticalRange.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/LongBitFormat.Enum.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.BlockReadStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.StripingCell.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.AlignedStripe.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.ChunkByteBuffer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/LongBitFormat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.StripeRange.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/ByteArrayManager.Conf.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/IOUtilsClient.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.StripingChunkReadResult.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.ChunkByteBuffer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/LongBitFormat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.StripeRange.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/ByteArrayManager.Conf.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/IOUtilsClient.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.StripingChunkReadResult.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSOpsCountStatistics.OpType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/ReplicaAccessor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/InMemoryAliasMapFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/ClientHAProxyFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/WrappedFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/InMemoryAliasMapFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/ClientHAProxyFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/WrappedFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/ConfiguredFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/AbstractNNFailoverProxyProvider.NNProxyInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/AbstractNNFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/IPFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/RequestHedgingProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/ConfiguredFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/AbstractNNFailoverProxyProvider.NNProxyInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/AbstractNNFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/IPFailoverProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaNotFoundException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/DiskBalancerWorkStatus.DiskBalancerWorkEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaNotFoundException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskBalancerWorkStatus.DiskBalancerWorkEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/CachingStrategy.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/CachingStrategy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskBalancerWorkStatus.Result.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/CorruptMetaHeaderException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/CachingStrategy.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/CachingStrategy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/DiskBalancerWorkStatus.Result.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/CorruptMetaHeaderException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/SlowDiskReports.DiskOp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/DatanodeStorage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/DatanodeStorageReport.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/StorageReport.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/DataNodeUsageReport.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/DatanodeStorage.State.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.SlotId.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DfsClientShm.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.PathInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplicaInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitShm.SlotId.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DfsClientShm.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DomainSocketFactory.PathInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitReplicaInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitShm.ShmId.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitCache.ShortCircuitReplicaCreator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DomainSocketFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DomainSocketFactory.PathState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DfsClientShmManager.PerDatanodeVisitorInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitShm.Slot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DfsClientShmManager.Visitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitShm.SlotIterator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitCache.CacheVisitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitShm.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.ShmId.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.ShortCircuitReplicaCreator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.PathState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.PerDatanodeVisitorInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.Slot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.Visitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.SlotIterator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.CacheVisitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSUtilClient.CorruptedBlocks.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/ReplicaAccessorBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/UnknownCipherSuiteException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/CannotObtainBlockLengthException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DeadNodeDetector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSMultipartUploaderFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSOpsCountStatistics.OpType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/ReplicaAccessor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/NameNodeProxiesClient.ProxyAndInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/UnknownCryptoProtocolVersionException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/ReadStatistics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DistributedFileSystem.HdfsDataOutputStreamBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSClient.DFSDataInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSOpsCountStatistics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSInotifyEventInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/NameNodeProxiesClient.ProxyAndInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/PBHelperClient.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/ReconfigurationProtocolUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/ReconfigurationProtocolPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.AccessMode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/class-use/BlockTokenIdentifier.AccessMode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/class-use/DelegationTokenIdentifier.WebHdfsDelegationTokenIdentifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/class-use/DelegationTokenIdentifier.SWebHdfsDelegationTokenIdentifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenIdentifier.WebHdfsDelegationTokenIdentifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenIdentifier.SWebHdfsDelegationTokenIdentifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.UnlinkEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.EventType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.CreateEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/EventBatch.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.TruncateEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.UnlinkEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.EventType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.CreateEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/EventBatch.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.TruncateEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.AppendEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.MetadataUpdateEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.RenameEvent.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.CreateEvent.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.MetadataUpdateEvent.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.AppendEvent.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.CloseEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.UnlinkEvent.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.CreateEvent.INodeType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.RenameEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/MissingEventsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.MetadataUpdateEvent.MetadataType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.AppendEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.MetadataUpdateEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.RenameEvent.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.CreateEvent.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.MetadataUpdateEvent.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.AppendEvent.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.CloseEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.UnlinkEvent.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.CreateEvent.INodeType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.RenameEvent.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/MissingEventsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.MetadataUpdateEvent.MetadataType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/UnknownCryptoProtocolVersionException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/ReadStatistics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Retry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.HttpClient.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsDataOutputStream.SyncFlag.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.DeprecatedKeys.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.StripedRead.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Write.ByteArrayManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Failover.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Retry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.HttpClient.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsDataOutputStream.SyncFlag.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.DeprecatedKeys.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.StripedRead.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Write.ByteArrayManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Failover.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/DfsPathCapabilities.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsDataInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Read.ShortCircuit.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Mmap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Write.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Read.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/BlockReportOptions.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/BlockReportOptions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.ShortCircuit.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsDataOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.BlockWrite.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsAdmin.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/CreateEncryptionZoneFlag.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.HedgedRead.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/DfsPathCapabilities.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsDataInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Read.ShortCircuit.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Mmap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Write.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Read.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/BlockReportOptions.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/BlockReportOptions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.ShortCircuit.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/metrics/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/metrics/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/metrics/class-use/BlockReaderIoProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.FailureInjector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/BlockReaderFactory.FailureInjector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/DfsClientConf.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/DfsClientConf.ShortCircuitConf.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/SnapshotDiffReportGenerator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/BlockReaderFactory.BlockReaderPeer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/DfsClientConf.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/DfsClientConf.ShortCircuitConf.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/SnapshotDiffReportGenerator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.BlockReaderPeer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsDataOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.BlockWrite.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsAdmin.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/CreateEncryptionZoneFlag.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.HedgedRead.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DistributedFileSystem.HdfsDataOutputStreamBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.WebHdfsInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/OAuth2ConnectionConfigurator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/AccessTokenProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/OAuth2ConnectionConfigurator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/AccessTokenProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/OAuth2Constants.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/ConfRefreshTokenBasedAccessTokenProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/AccessTokenTimer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/ConfCredentialBasedAccessTokenProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/CredentialBasedAccessTokenProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/OAuth2Constants.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/ConfRefreshTokenBasedAccessTokenProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/AccessTokenTimer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/ConfCredentialBasedAccessTokenProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/CredentialBasedAccessTokenProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/WebHdfsFileSystem.WebHdfsInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/WebHdfsFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/KerberosUgiAuthenticator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/WebHdfsFileSystem.ReadRunner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/ByteRangeInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/SWebHdfsFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/JsonUtilClient.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/SSLConnectionConfigurator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/ByteRangeInputStream.URLOpener.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/KerberosUgiAuthenticator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.ReadRunner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/ByteRangeInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/SWebHdfsFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/JsonUtilClient.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/SSLConnectionConfigurator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/OverwriteParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/StorageSpaceQuotaParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/FsActionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/OldSnapshotNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/ECPolicyParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/SnapshotNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/DestinationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/HttpOpParam.TemporaryRedirectOp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/OffsetParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/AccessTimeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/NameSpaceQuotaParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/BufferSizeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/RecursiveParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/PermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/Param.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/UserParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/PostOpParam.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/OverwriteParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/StorageSpaceQuotaParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/FsActionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/OldSnapshotNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ECPolicyParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/SnapshotNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/DestinationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/HttpOpParam.TemporaryRedirectOp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/OffsetParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/AccessTimeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/NameSpaceQuotaParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/BufferSizeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/RecursiveParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/PermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/Param.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/UserParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/PostOpParam.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/XAttrNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ConcatSourcesParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/HttpOpParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/GetOpParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/CreateParentParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ExcludeDatanodesParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/StorageTypeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/DeleteOpParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/StartAfterParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/OwnerParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/CreateFlagParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/TokenArgumentParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/NoRedirectParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/PutOpParam.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/RenameOptionSetParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/LengthParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/HttpOpParam.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/XAttrEncodingParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ModificationTimeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/GetOpParam.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ReplicationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/HttpOpParam.Type.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/AclPermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/RenewerParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/NewLengthParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/DoAsParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/XAttrSetFlagParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/StoragePolicyParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/UnmaskedPermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/PostOpParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/DelegationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/XAttrValueParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/DeleteOpParam.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/BlockSizeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/PutOpParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/GroupParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/XAttrNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/ConcatSourcesParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/HttpOpParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/GetOpParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/CreateParentParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/ExcludeDatanodesParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/StorageTypeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/DeleteOpParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/StartAfterParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/OwnerParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/CreateFlagParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/TokenArgumentParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/NoRedirectParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/PutOpParam.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/RenameOptionSetParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/LengthParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/HttpOpParam.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/XAttrEncodingParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/ModificationTimeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/GetOpParam.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/ReplicationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/HttpOpParam.Type.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/AclPermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/RenewerParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/NewLengthParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/DoAsParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/XAttrSetFlagParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/StoragePolicyParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/UnmaskedPermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/PostOpParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/DelegationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/XAttrValueParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/DeleteOpParam.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/BlockSizeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/PutOpParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/GroupParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/ByteRangeInputStream.URLOpener.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSClient.DFSDataInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSOpsCountStatistics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSInotifyEventInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/XAttr.NameSpace.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/XAttr.NameSpace.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/WebHdfs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/XAttr.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/SWebHdfs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/CacheFlag.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/WebHdfs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/XAttr.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/SWebHdfs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/CacheFlag.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/overview-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/overview-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/allclasses-noframe.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/package-list\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/WebHDFS.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/user_comments_for_Apache_Hadoop_HDFS_3.2.1_to_Apache_Hadoop_HDFS_3.3.0.xml\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/alldiffs_index_removals.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/fields_index_removals.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/packages_index_removals.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/fields_index_changes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/pkg_org.apache.hadoop.hdfs.server.namenode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/jdiff_topleftframe.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/methods_index_changes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/packages_index_additions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/fields_index_all.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/classes_index_additions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/packages_index_changes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/classes_index_all.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/changes-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/jdiff_statistics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/constructors_index_removals.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/methods_index_all.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/alldiffs_index_additions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/pkg_org.apache.hadoop.hdfs.server.aliasmap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/constructors_index_additions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/jdiff_help.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/org.apache.hadoop.hdfs.server.aliasmap.InMemoryAliasMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/constructors_index_all.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/alldiffs_index_changes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/alldiffs_index_all.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/methods_index_additions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/classes_index_changes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/classes_index_removals.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/methods_index_removals.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/constructors_index_changes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/packages_index_all.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/fields_index_additions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/Apache_Hadoop_HDFS_3.3.0.xml\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/stylesheet-jdiff.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/ViewFs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/configuration.xsl\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsProvidedStorage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/federation.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/viewfs_TypicalMountTable.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsproxy-forward.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/federation-background.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsproxy-server.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsdatanodes.odg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/caching.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.odg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/LazyPersistWrites.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsdatanodes.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsdatanodes.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsproxy-overview.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfs-logo.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/Federation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsDataNodeAdminGuide.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/serialized-form.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/index.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/stylesheet.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/index-all.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/constant-values.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/overview-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/help-doc.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/deprecated-list.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/allclasses-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/script.js\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/LayoutVersion.Feature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/BlackListBasedTrustedChannelResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/WhitelistBasedTrustedChannelResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/LayoutVersion.Feature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/FSLimitException.PathComponentTooLongException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotInfo.Bean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/BlockListAsLongs.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/LayoutVersion.FeatureInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/FSLimitException.MaxDirectoryItemsExceededException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/LayoutVersion.LayoutFeature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/FSLimitException.PathComponentTooLongException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/SnapshotInfo.Bean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/LayoutVersion.FeatureInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/FSLimitException.MaxDirectoryItemsExceededException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/LayoutVersion.LayoutFeature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/SnapshotException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/class-use/DFSTopologyNodeImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/class-use/DFSNetworkTopology.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/DFSNetworkTopology.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/WebHdfsDtFetcher.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/DFSUtil.ServiceComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/AtomicFileOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/FoldedTreeSet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/XMLUtils.Stanza.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/ConstEnumCounters.ConstEnumException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Diff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/ConstEnumCounters.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/EnumCounters.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/AtomicFileOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/FoldedTreeSet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/XMLUtils.Stanza.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/ConstEnumCounters.ConstEnumException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Diff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/ConstEnumCounters.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/EnumCounters.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/ReadOnlyList.Util.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/RwLock.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/LightWeightHashSet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/XMLUtils.UnmanglingError.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/MD5FileUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/DataTransferThrottler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/ReferenceCountMap.ReferenceCounter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/LightWeightLinkedSet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Holder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Diff.Container.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Diff.UndoInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Diff.Element.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/EnumDoubles.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/XMLUtils.InvalidXmlException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Diff.Processor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/ReadOnlyList.Util.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/RwLock.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/LightWeightHashSet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/XMLUtils.UnmanglingError.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/MD5FileUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/DataTransferThrottler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/ReferenceCountMap.ReferenceCounter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/LightWeightLinkedSet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Holder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Diff.Container.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Diff.UndoInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Diff.Element.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/EnumDoubles.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/XMLUtils.InvalidXmlException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Diff.Processor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/class-use/Mover.Cli.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/Mover.Cli.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.DelegationKey.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/class-use/NameNodeMetrics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NameNode.NameNodeHAContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeReference.WithCount.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/JournalManager.CorruptionException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeSymlink.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummary.Section.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeReference.DstReference.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSection.Snapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.NameSystemSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/startupprogress/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/startupprogress/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/startupprogress/class-use/StartupProgress.Counter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DirectoryDiffOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/metrics/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/metrics/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/metrics/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/metrics/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/metrics/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.User.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.TopWindow.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/class-use/RollingWindowManager.User.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/class-use/RollingWindowManager.TopWindow.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/class-use/RollingWindowManager.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.Op.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrCompactProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.NameSystemSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ContentCounts.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/StoragePolicySatisfyManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/StoragePolicySatisfier.DatanodeMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/BlockStorageMovementNeeded.DirPendingWorkInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/BlockStorageMovementAttemptedItems.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/StoragePolicySatisfier.DatanodeWithStorage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.Loader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummary.Section.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DirectoryDiff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.FileUnderConstructionFeature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSection.SnapshotOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INode.BlocksMapUpdateInfo.UpdatedReplicationInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.Loader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INode.Feature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NNStorage.NameNodeFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeDirectory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSTreeTraverser.TraverseInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.CacheManagerSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/QuotaCounts.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeFileOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DiffEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSection.Entry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.CreatedListEntryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/DirectoryWithQuotaFeature.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.ErasureCodingSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.PositionTrackingInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NameNode.OperationCategory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.CreatedListEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.TransferResult.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.CacheManagerSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummary.SectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/AuditLogger.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/DefaultINodeAttributesProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeAttributes.SnapshotCopy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.FileDiffOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySection.DirEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSection.INodeReference.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.DelegationKey.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NameNode.NameNodeHAContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeReference.WithCount.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/JournalManager.CorruptionException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeSymlink.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummary.Section.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeReference.DstReference.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSection.Snapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.NameSystemSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DirectoryDiffOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrCompactProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.NameSystemSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/ContentCounts.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatPBINode.Loader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummary.Section.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSEditLogOp.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DirectoryDiff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.FileUnderConstructionFeature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSection.SnapshotOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INode.BlocksMapUpdateInfo.UpdatedReplicationInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.Loader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INode.Feature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NNStorage.NameNodeFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeDirectory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSTreeTraverser.TraverseInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.CacheManagerSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/QuotaCounts.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeFileOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DiffEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NNStorageRetentionManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSection.Entry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.CreatedListEntryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/DirectoryWithQuotaFeature.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.ErasureCodingSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSEditLogLoader.PositionTrackingInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NameNode.OperationCategory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.CreatedListEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/TransferFsImage.TransferResult.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.CacheManagerSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSPermissionChecker.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummary.SectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/AuditLogger.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/DefaultINodeAttributesProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeAttributes.SnapshotCopy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.FileDiffOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySection.DirEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSection.INodeReference.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/CachePool.DirectiveList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodesInPath.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrCompactProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.FileUnderConstructionFeature.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySection.DirEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeDirectory.SnapshotAndINode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.PersistToken.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INode.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.DelegationKey.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSection.INodeReference.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/ContentCounts.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/Quota.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/DirectoryWithQuotaFeature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.SaverContext.DeduplicationMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/QuotaCounts.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/AclEntryStatusFormat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSEditLogOp.OpInstanceCache.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.AclFeatureProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NNUpgradeUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.SectionName.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/MetaRecoveryContext.RequestStopException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeSymlink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DiffEntry.Type.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSDirectory.DirOp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrFeatureProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.ErasureCodingSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.NameSystemSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NameNodeLayoutVersion.Feature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/SerialNumberManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummary.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeDirectoryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSection.EntryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.CreatedListEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/HdfsAuditLogger.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormat.Loader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/QuotaByStorageTypeEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.LoaderContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/CacheManager.PersistState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSection.Entry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeDirectory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeReference.WithName.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DirectoryDiff.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/Content.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrFeatureProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.PersistToken.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INode.BlocksMapUpdateInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeAttributeProvider.AuthorizationContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummaryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSDirAttrOp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeFile.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.SaverContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.DelegationKeyOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.PersistTokenOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrCompactProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.FileDiff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/EncryptionZoneManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DiffEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeDirectoryAttributes.CopyWithQuota.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSection.Snapshot.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeFileAttributes.SnapshotCopy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.ErasureCodingSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeAttributeProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatPBINode.Saver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeDirectoryAttributes.SnapshotCopy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DiffEntryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.AclFeatureProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/SerialNumberManager.StringTable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeDirectory.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/IsNameNodeActiveServlet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/EncryptionFaultInjector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.CacheManagerSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.Saver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INode.ReclaimContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/QuotaByStorageTypeEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/DefaultAuditLogger.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeReference.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeSymlinkOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/CheckpointFaultInjector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/XAttrFormat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeEntryProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/StoragePolicySummary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NNStorage.NameNodeDirType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSection.INodeReferenceOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrFeatureProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeAttributeProvider.AccessControlEnforcer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSEditLogOp.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySection.DirEntryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/DfsServlet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.AclFeatureProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INode.Type.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeEntryProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeAttributeProvider.AuthorizationContext.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/Quota.Counts.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INode.QuotaDelta.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeEntryProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.FileDiff.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/CachePool.DirectiveList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodesInPath.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrCompactProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.FileUnderConstructionFeature.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySection.DirEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.SnapshotAndINode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.PersistToken.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INode.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.DelegationKey.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSection.INodeReference.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ContentCounts.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/Quota.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/DirectoryWithQuotaFeature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.SaverContext.DeduplicationMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/QuotaCounts.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/AclEntryStatusFormat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.OpInstanceCache.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeFile.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.AclFeatureProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.SectionName.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.RequestStopException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeSymlink.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DiffEntry.Type.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSDirectory.DirOp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrFeatureProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.ErasureCodingSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.NameSystemSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NameNodeLayoutVersion.Feature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/SerialNumberManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummary.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeDirectoryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSection.EntryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.CreatedListEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/HdfsAuditLogger.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.Loader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.LoaderContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/CacheManager.PersistState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSection.Entry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeReference.WithName.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DirectoryDiff.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/Content.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrFeatureProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.PersistToken.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INode.BlocksMapUpdateInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.AuthorizationContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/class-use/NamenodeWebHdfsMethods.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummaryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeFile.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.SaverContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.DelegationKeyOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.PersistTokenOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrCompactProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.FileDiff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/EncryptionZoneManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DiffEntry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryAttributes.CopyWithQuota.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSection.Snapshot.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeFileAttributes.SnapshotCopy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.ErasureCodingSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.Saver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryAttributes.SnapshotCopy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DiffEntryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.AclFeatureProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/SerialNumberManager.StringTable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeDirectory.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/IsNameNodeActiveServlet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/EncryptionFaultInjector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.CacheManagerSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.Saver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INode.ReclaimContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/DefaultAuditLogger.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeReference.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeSymlinkOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSection.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/CheckpointFaultInjector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/XAttrFormat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeEntryProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/StoragePolicySummary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NNStorage.NameNodeDirType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSection.INodeReferenceOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/RemoteNameNodeInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.ActiveNodeInfoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.ActiveNodeInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/class-use/HAZKInfoProtos.ActiveNodeInfoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/class-use/HAZKInfoProtos.ActiveNodeInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/class-use/HAZKInfoProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/class-use/HAZKInfoProtos.ActiveNodeInfo.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.ActiveNodeInfo.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/RemoteNameNodeInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/NameNodeHAProxyFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/NameNodeHAProxyFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrFeatureProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSectionOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.AccessControlEnforcer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySection.DirEntryOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/DfsServlet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.DirectoryDiffList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListBySkipList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotStatsMXBean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.DirectoryDiff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotFSImageFormat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryDiffListFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DirectoryWithSnapshotFeature.DirectoryDiffList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DiffListBySkipList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/SnapshotStatsMXBean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DirectoryWithSnapshotFeature.DirectoryDiff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/SnapshotFSImageFormat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DirectoryDiffListFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/Snapshot.Root.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/FileDiff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/SnapshotFSImageFormat.ReferenceMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/FSImageFormatPBSnapshot.Loader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DiffListByArrayList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/FSImageFormatPBSnapshot.Saver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/SnapshotManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DiffList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/FileDiffList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/Snapshot.Root.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotFSImageFormat.ReferenceMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.Loader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListByArrayList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.Saver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DiffList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.AclFeatureProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INode.Type.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeEntryProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.AuthorizationContext.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/Quota.Counts.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INode.QuotaDelta.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeEntryProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.FileDiff.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.RecoveryTaskStriped.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/class-use/DataNodeMetricHelper.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetricHelper.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DiskFileCorruptException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaAlreadyExistsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ErrorReportAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.BlockPoolReport.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/FSCachingGetSpaceUsed.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaWaitingToBeRecovered.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.DiskBalancerMover.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/UnexpectedReplicaStateException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/checker/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/checker/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/checker/class-use/DatasetVolumeChecker.Callback.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/checker/class-use/AbstractFuture.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/checker/class-use/DatasetVolumeChecker.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/StorageLocation.CheckContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/SecureDataNodeStarter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/BlockRecoveryWorker.RecoveryTaskStriped.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskFileCorruptException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaAlreadyExistsException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ErrorReportAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DirectoryScanner.BlockPoolReport.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/FSCachingGetSpaceUsed.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaWaitingToBeRecovered.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/VolumeScanner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskBalancer.DiskBalancerMover.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/UnexpectedReplicaStateException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/StorageLocation.CheckContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/SecureDataNodeStarter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaInPipeline.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ChunkChecksum.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ShortCircuitRegistry.NewShmInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/FileIoProvider.OPERATION.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DataNodeLayoutVersion.Feature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/FinalizedReplica.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/LocalReplica.ReplicaDirInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/BPServiceActorActionException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/LocalReplicaInPipeline.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ShortCircuitRegistry.Visitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ShortCircuitRegistry.RegisteredShm.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaBeingWritten.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ShortCircuitRegistry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/SecureDataNodeStarter.SecureResources.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DirectoryScanner.ScanInfoVolumeReport.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskBalancer.VolumePair.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/BPServiceActorAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReportBadBlockAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/LocalReplica.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaUnderRecovery.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaHandler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskBalancer.BlockMover.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DirectoryScanner.ReportCompiler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.NewShmInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/FileIoProvider.OPERATION.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DataNodeLayoutVersion.Feature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/LocalReplica.ReplicaDirInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/BPServiceActorActionException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.Visitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.VolumeCheckContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.FsVolumeReferences.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsDatasetSpi.Factory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsVolumeSpi.VolumeCheckContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsVolumeSpi.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsDatasetSpi.FsVolumeReferences.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/AvailableSpaceVolumeChoosingPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsVolumeSpi.BlockIterator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsVolumeReference.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/LengthInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/ReplicaInputStreams.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/RoundRobinVolumeChoosingPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/ReplicaOutputStreams.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsVolumeSpi.ScanInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.BlockIterator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeReference.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/LengthInputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaInputStreams.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/RoundRobinVolumeChoosingPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaOutputStreams.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.ReservedSpaceCalculatorAggressive.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.ReservedSpaceCalculatorConservative.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.ReservedSpaceCalculatorAbsolute.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.ReservedSpaceCalculatorAggressive.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.ReservedSpaceCalculatorConservative.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.ReservedSpaceCalculatorAbsolute.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/AddBlockPoolException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.ReservedSpaceCalculatorPercentage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/FsDatasetFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/FsVolumeImplBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/FsVolumeImpl.BlockDirFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/AddBlockPoolException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.ReservedSpaceCalculatorPercentage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImplBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.BlockDirFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.ScanInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.RegisteredShm.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaBeingWritten.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/SecureDataNodeStarter.SecureResources.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.ScanInfoVolumeReport.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/DataNodeUGIProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/class-use/DataNodeUGIProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/class-use/WebHdfsHandler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/class-use/DatanodeHttpServer.MapBasedFilterConfig.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/class-use/DatanodeHttpServer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.MapBasedFilterConfig.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.VolumePair.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/BPServiceActorAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReportBadBlockAction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/LocalReplica.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaHandler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.BlockMover.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.ReportCompiler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/RemoteEditLog.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BlocksStorageMoveAttemptFinished.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BlocksWithLocations.StripedBlockWithLocations.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BalancerBandwidthCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BlockStorageMovementCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/RemoteEditLogManifest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/VolumeFailureSummary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/NamespaceInfo.Capability.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/ReceivedDeletedBlockInfo.BlockStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/StorageReceivedDeletedBlocks.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/ReceivedDeletedBlockInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/FencedException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/StorageBlockReport.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BlockStorageMovementCommand.BlockMovingInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BlockRecoveryCommand.RecoveringStripedBlock.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.BlockUCState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/TokenVerifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/FileRegion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.NamenodeRole.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/MetricsLoggerTask.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.BlockUCState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/TokenVerifier.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/FileRegion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.NamenodeRole.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/MetricsLoggerTask.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HostRestrictingAuthorizationFilter.HttpInteraction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.ReplicaState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/BlockAlias.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.StartupOption.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HostRestrictingAuthorizationFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.NodeType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.RollingUpgradeStartupOption.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/Storage.StorageState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.HttpInteraction.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.ReplicaState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/BlockAlias.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.StartupOption.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.NodeType.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.RollingUpgradeStartupOption.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.Reader.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.Reader.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.ImmutableIterator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.Writer.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.Reader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.ImmutableIterator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.LevelDBWriter.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.LevelDBOptions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.WriterOptions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.ReaderOptions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.LevelDBReader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.LevelDBWriter.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.LevelDBOptions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.WriterOptions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.ReaderOptions.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.LevelDBReader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.LevelDBWriter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.TextWriter.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.LevelDBReader.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.TextReader.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.TextReader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.TextWriter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.LevelDBWriter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.TextWriter.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.LevelDBReader.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.TextReader.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.TextReader.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.TextWriter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.Writer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.Writer.Options.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/Storage.StorageState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerDataNode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolume.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/class-use/DiskBalancerCluster.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/class-use/DiskBalancerDataNode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/class-use/DiskBalancerVolume.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/class-use/DiskBalancerVolumeSet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolumeSet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/HelpCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/ExecuteCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/PlanCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/Command.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/QueryCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/ReportCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/CancelCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/CancelCommand.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/class-use/DiskBalancerException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/class-use/DiskBalancerException.Result.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/MoveStep.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/MoveStep.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/GreedyPlanner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/PlannerFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/NodePlan.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/Planner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/Step.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/PlannerFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/NodePlan.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/Planner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/Step.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/class-use/ClusterConnector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/class-use/JsonNodeConnector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/class-use/ConnectorFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/ClusterConnector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/JsonNodeConnector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/ConnectorFactory.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/DiskBalancerException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/DiskBalancerException.Result.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.DBlockStriped.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.DDatanode.StorageGroup.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/MovedBlocks.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.DBlockStriped.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.DDatanode.StorageGroup.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/MovedBlocks.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.StorageGroupMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.Source.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.DBlock.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Matcher.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.PendingMove.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/MovedBlocks.Locations.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/ExitStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.DDatanode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.StorageGroupMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.Source.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.DBlock.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Matcher.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.PendingMove.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/MovedBlocks.Locations.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/ExitStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.DDatanode.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMap.CheckedFunction2.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/class-use/InMemoryAliasMap.CheckedFunction2.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/class-use/InMemoryAliasMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/class-use/InMemoryAliasMapProtocol.IterationResult.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMap.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMapProtocol.IterationResult.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockIdManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/NumberReplicas.StoredReplicaState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockStatsMXBean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/UnresolvedTopologyException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementStatusDefault.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.CachedBlocksList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.LeavingServiceStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminMonitorInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.StorageAndBlockIndex.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/HostFileManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeAdminBackoffMonitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockIdManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeStorageInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/NumberReplicas.StoredReplicaState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeAdminDefaultMonitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockStatsMXBean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/UnresolvedTopologyException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockPlacementStatusDefault.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeDescriptor.CachedBlocksList.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeDescriptor.LeavingServiceStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeAdminMonitorInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockInfoStriped.StorageAndBlockIndex.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/HostFileManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeDescriptor.CachedBlocksList.Type.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/ProvidedStorageMap.ProvidedDescriptor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/SlowDiskTracker.DiskLatency.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockManagerFaultInjector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/AvailableSpaceBlockPlacementPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockUnderConstructionFeature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/CombinedHostFileManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockStoragePolicySuite.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeAdminMonitorBase.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/HostSet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeStatistics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/CorruptReplicasMap.Reason.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockPlacementPolicyWithNodeGroup.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/NumberReplicas.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/SlowPeerTracker.ReportForJson.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.CachedBlocksList.Type.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.ProvidedDescriptor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.DiskLatency.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerFaultInjector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminMonitorBase.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/HostSet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStatistics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.Reason.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/NumberReplicas.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.ReportForJson.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/DFSUtil.ConfiguredNNAddress.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/WebHdfsDtFetcher.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/DFSUtil.ServiceComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/DFSUtil.ConfiguredNNAddress.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/SWebHdfsDtFetcher.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/HdfsDtFetcher.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/ReconfigurationProtocolServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/NamenodeProtocolServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/DatanodeProtocolServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/ReconfigurationProtocolServerSideUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/PBHelper.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolServerSideUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/PBHelper.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/class-use/BlockPoolTokenSecretManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/BlockPoolTokenSecretManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.SecretManagerState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/class-use/DelegationTokenSecretManager.SecretManagerState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/class-use/OfflineEditsViewer.Flags.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/class-use/TeeOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsViewer.Flags.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TeeOutputStream.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/DiskBalancerCLI.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/DiskBalancerCLI.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/DFSHAAdmin.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/AdminHelper.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/GetConf.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/StoragePolicyAdmin.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/DFSHAAdmin.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/AdminHelper.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/GetConf.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageDelimitedTextWriter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/IgnoreSnapshotException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/PBImageDelimitedTextWriter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/IgnoreSnapshotException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/WebImageViewer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/PBImageCorruptionDetector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/XmlImageVisitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/PBImageCorruption.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/WebImageViewer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruptionDetector.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/XmlImageVisitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruption.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/StoragePolicyAdmin.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/snapshot/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/snapshot/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/snapshot/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/snapshot/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/snapshot/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/SWebHdfsDtFetcher.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/AuthFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/AuthFilterInitializer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/ParamFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/class-use/AuthFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/class-use/AuthFilterInitializer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/class-use/ParamFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/class-use/JsonUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/JsonUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/TokenServiceParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/ExceptionHandler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/TokenKindParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/TokenServiceParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ExceptionHandler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/TokenKindParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/UserProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/UriFsPathParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/NamenodeAddressParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/UserProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/UriFsPathParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/NamenodeAddressParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.QJournalProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalIdProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PersistedRecoveryPaxosData.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalIdProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.RequestInfoProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.SegmentStateProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.QJournalProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.SegmentStateProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.QJournalProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.RequestInfoProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.QJournalProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalIdProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PersistedRecoveryPaxosData.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalIdProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.RequestInfoProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.SegmentStateProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.QJournalProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.SegmentStateProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.QJournalProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.RequestInfoProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/InterQJournalProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.SegmentStateProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/InterQJournalProtocolProtos.InterQJournalProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/InterQJournalProtocolProtos.InterQJournalProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PersistedRecoveryPaxosDataOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.RequestInfoProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/InterQJournalProtocolProtos.InterQJournalProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PersistedRecoveryPaxosData.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/InterQJournalProtocolProtos.InterQJournalProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.QJournalProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalIdProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.SegmentStateProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.InterQJournalProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.InterQJournalProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PersistedRecoveryPaxosDataOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.RequestInfoProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.InterQJournalProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PersistedRecoveryPaxosData.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.InterQJournalProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.QJournalProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalIdProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/JournalNodeMXBean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/class-use/JournalNodeMXBean.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/class-use/Journal.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/Journal.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocolPB/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocolPB/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocolPB/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocolPB/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocolPB/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/client/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/client/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/client/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/client/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/client/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/HdfsDtFetcher.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/overview-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/overview-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/allclasses-noframe.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/package-list\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/SLGUserGuide.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/LibHdfs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/hdfs-rbf-default.xml\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/configuration.xsl\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/routerfederation.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/serialized-form.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/index.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/stylesheet.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/index-all.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/constant-values.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/overview-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/help-doc.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/deprecated-list.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/allclasses-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/script.js\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/RouterProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/RouterProtocolProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/RouterProtocolProtos.RouterAdminProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/RouterProtocolProtos.RouterAdminProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/RouterProtocolProtos.RouterAdminProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/RouterProtocolProtos.RouterAdminProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/RouterProtocolProtos.RouterAdminProtocolService.Interface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/RouterProtocolProtos.RouterAdminProtocolService.Stub.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/RouterProtocolProtos.RouterAdminProtocolService.BlockingInterface.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/RouterProtocolProtos.RouterAdminProtocolService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterHttpServer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterStateManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterQuotaUsage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterQuotaUpdateService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterCacheAdmin.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterQuotaManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/MountTableRefresherService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/ConnectionContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RemoteResult.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterStoragePolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/PeriodicService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/ConnectionNullException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/MountTableRefresherThread.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RemoteLocationContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/Quota.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/FederationUtil.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterNamenodeProtocol.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/NoNamenodesAvailableException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/NameserviceManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterClientProtocol.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/IsRouterActiveServlet.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterWebHdfsMethods.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RemoteMethod.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterAdminServer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterPermissionChecker.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/SubClusterTimeoutException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/DFSRouter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterRpcClient.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterServiceState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterHeartbeatService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/NamenodeHeartbeatService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RemoteParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/ErasureCoding.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterMetricsService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterRpcMonitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/ConnectionManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterUserProtocol.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterSafemodeService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterMetrics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterQuotaUsage.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterRpcServer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/class-use/RouterSecurityManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/token/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/token/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/token/class-use/ZKDelegationTokenSecretManagerImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/NamenodeBeanMetrics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/FederationRPCPerformanceMonitor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/NullStateStoreMetrics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/RBFMetrics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/StateStoreMetrics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/FederationRPCMetrics.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetNamespaceInfoResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/LeaveSafeModeRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetNamenodeRegistrationsRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/EnableNameserviceResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/UpdateNamenodeRegistrationRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RefreshSuperUserGroupsConfigurationResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/EnableNameserviceRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetDestinationResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RemoveMountTableEntryRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetRouterRegistrationsRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RefreshSuperUserGroupsConfigurationRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetMountTableEntriesRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/DisableNameserviceRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetDestinationRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetSafeModeResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RemoveMountTableEntryResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RefreshMountTableEntriesRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/EnterSafeModeRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/UpdateMountTableEntryResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/LeaveSafeModeResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetSafeModeRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/UpdateMountTableEntryRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/NamenodeHeartbeatResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RouterHeartbeatRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetRouterRegistrationsResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RefreshMountTableEntriesResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetNamenodeRegistrationsResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetRouterRegistrationRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetMountTableEntriesResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetDisabledNameservicesResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/DisableNameserviceResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/NamenodeHeartbeatRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/EnterSafeModeResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetDisabledNameservicesRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/UpdateNamenodeRegistrationResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/AddMountTableEntryResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RouterHeartbeatResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/AddMountTableEntryRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetNamespaceInfoRequest.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetRouterRegistrationResponse.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetNamenodeRegistrationsRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetDestinationRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetDisabledNameservicesRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/LeaveSafeModeResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/EnableNameserviceResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/EnterSafeModeResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetNamespaceInfoResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/FederationProtocolPBTranslator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetRouterRegistrationResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RouterHeartbeatResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetRouterRegistrationsRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/UpdateNamenodeRegistrationRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/NamenodeHeartbeatResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetDisabledNameservicesResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RefreshSuperUserGroupsConfigurationRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RefreshMountTableEntriesResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetSafeModeRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/NamenodeHeartbeatRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RouterHeartbeatRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/UpdateNamenodeRegistrationResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetNamenodeRegistrationsResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/DisableNameserviceResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetSafeModeResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetMountTableEntriesResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RefreshSuperUserGroupsConfigurationResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/EnableNameserviceRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/AddMountTableEntryResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetRouterRegistrationRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/EnterSafeModeRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RefreshMountTableEntriesRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/UpdateMountTableEntryResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetRouterRegistrationsResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/DisableNameserviceRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RemoveMountTableEntryRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetMountTableEntriesRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RemoveMountTableEntryResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/AddMountTableEntryRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/UpdateMountTableEntryRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetDestinationResponsePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetNamespaceInfoRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/LeaveSafeModeRequestPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/CachedRecordStore.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/StateStoreConnectionMonitorService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/StateStoreUnavailableException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/StateStoreCacheUpdateService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/StateStoreUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/StateStoreCache.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/class-use/StateStoreSerializer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/class-use/StateStoreDriver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreZooKeeperImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreFileImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreFileSystemImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreBaseImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreSerializableImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreFileBaseImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreSerializerPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/QueryResult.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/StateStoreVersion.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/MountTable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/MembershipState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/BaseRecord.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/DisabledNameservice.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/Query.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/RouterState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/MembershipStats.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/StateStoreVersionPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/MembershipStatsPBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/DisabledNameservicePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/PBRecord.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/MembershipStatePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/MountTablePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/RouterStatePBImpl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/NamenodePriorityComparator.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/PathLocation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/RouterResolveException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/FederationNamespaceInfo.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/MembershipNamenodeResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/RouterGenericManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/RemoteLocation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/MountTableResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/NamenodeStatusReport.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/FederationNamenodeContext.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/MountTableManager.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/FederationNamenodeServiceState.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/MultipleDestinationMountTableResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/HashFirstResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/AvailableSpaceResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/DestinationOrder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/OrderedResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/RouterResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/LocalResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/HashResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/RandomResolver.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/utils/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/utils/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/utils/class-use/ConsistentHashRing.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocolPB/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocolPB/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocolPB/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocolPB/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocolPB/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoteLocationProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.MountTableRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisabledNameserviceRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.StateStoreVersionRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.MountTableRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoteLocationProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoteLocationProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.MountTableRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisabledNameserviceRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.StateStoreVersionRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.MountTableRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoteLocationProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisabledNameserviceRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.FederationNamespaceInfoProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.MountTableRecordProto.DestOrder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisabledNameserviceRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.MountTableRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoteLocationProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisabledNameserviceRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.FederationNamespaceInfoProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.MountTableRecordProto.DestOrder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisabledNameserviceRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.MountTableRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatResponseProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatResponseProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoResponseProto.Builder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoteLocationProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatRequestProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterRecordProtoOrBuilder.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/federation/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/federation/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/federation/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/federation/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/federation/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/overview-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/overview-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/allclasses-noframe.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/package-list\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/dependency-analysis.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/index.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/expanded.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/banner.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/icon_warning_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/collapsed.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logo_apache.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/newwindow.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/h5.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/apache-maven-project-2.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/external.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/h3.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/icon_success_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/maven-logo-2.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logo_maven.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/breadcrumbs.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logos/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logos/build-by-maven-white.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logos/maven-feather.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logos/build-by-maven-black.png\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/icon_info_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/bg.jpg\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/icon_error_sml.gif\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/project-reports.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/ServerSetup.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/UsingHttpTools.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/css/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/css/maven-base.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/css/maven-theme.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/css/site.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/css/print.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/serialized-form.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/index.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/stylesheet.css\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/index-all.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/constant-values.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/overview-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/help-doc.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/deprecated-list.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/allclasses-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/script.js\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSDeleteSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetAllStoragePolicies.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSCreateSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSExceptionProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetReplication.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSListStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.SourcesParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.SnapshotNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSServerWebServer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetXAttr.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.XAttrValueParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.StartAfterParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetStoragePolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.ReplicationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.ECPolicyParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetSnapshotDiff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.OperationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.UnmaskedPermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRemoveAcl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.OldSnapshotNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/CheckUploadContentTypeFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.ModifiedTimeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSatisyStoragePolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.NoRedirectParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSDeleteSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetAllStoragePolicies.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSCreateSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSExceptionProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetReplication.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSListStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.SourcesParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.SnapshotNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSServerWebServer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetXAttr.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.XAttrValueParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.StartAfterParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetStoragePolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.ReplicationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.ECPolicyParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetSnapshotDiff.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.OperationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.UnmaskedPermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRemoveAcl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.OldSnapshotNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/CheckUploadContentTypeFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.ModifiedTimeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSatisyStoragePolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.NoRedirectParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.GroupParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRename.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetPermission.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.BlockSizeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.XAttrNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSListXAttrs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.OverwriteParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSServerWebApp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSServer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSFileStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.FsActionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSOpen.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSAclStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetErasureCodingPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetErasureCodingPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.FilterParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetOwner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetServerDefaults.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetStoragePolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.RecursiveParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSAppend.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSFileChecksum.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSReleaseFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.XAttrSetFlagParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSAllowSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.NewLengthParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSAccess.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.OffsetParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRenameSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSUnSetErasureCodingPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSQuotaUsage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSModifyAclEntries.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.DataParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSDelete.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSHomeDir.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetTimes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetAcl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.AccessTimeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRemoveDefaultAcl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.PolicyNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSTruncate.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.PermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSCreate.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSTrashRoot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSAuthenticationFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRemoveAclEntries.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSConcat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.OwnerParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSDisallowSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSListStatusBatch.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.DestinationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetXAttrs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRemoveXAttr.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.LenParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.AclPermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSContentSummary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSMkdirs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetSnapshottableDirListing.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.XAttrEncodingParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSUnsetStoragePolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.GroupParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRename.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetPermission.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.BlockSizeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.XAttrNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSListXAttrs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.OverwriteParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSServerWebApp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSServer.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSFileStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.FsActionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSOpen.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSAclStatus.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetErasureCodingPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetErasureCodingPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.FilterParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetOwner.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetServerDefaults.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetStoragePolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.RecursiveParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSAppend.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSFileChecksum.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSReleaseFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.XAttrSetFlagParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSAllowSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.NewLengthParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSAccess.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.OffsetParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRenameSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSUnSetErasureCodingPolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSQuotaUsage.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSModifyAclEntries.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.DataParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSDelete.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSHomeDir.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetTimes.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetAcl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.AccessTimeParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRemoveDefaultAcl.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.PolicyNameParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSTruncate.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.PermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSCreate.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSTrashRoot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSAuthenticationFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRemoveAclEntries.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSConcat.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.OwnerParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSDisallowSnapshot.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSListStatusBatch.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.DestinationParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetXAttrs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRemoveXAttr.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.LenParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.AclPermissionParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSContentSummary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSMkdirs.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetSnapshottableDirListing.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.XAttrEncodingParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSUnsetStoragePolicy.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/HttpFSFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/HttpFSUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/HttpsFSFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/HttpFSFileSystem.Operation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/HttpFSFileSystem.FILE_TYPE.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/HttpFSFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/HttpFSUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/HttpsFSFileSystem.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/HttpFSFileSystem.Operation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/HttpFSFileSystem.FILE_TYPE.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/RunnableCallable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/class-use/RunnableCallable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/class-use/XException.ERROR.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/class-use/XException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/XException.ERROR.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/XException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/ConfigurationUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/Check.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/class-use/ConfigurationUtils.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/class-use/Check.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/Server.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/Server.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/Service.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/ServerException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/Server.Status.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/BaseService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/ServerException.ERROR.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/ServiceException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/Service.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/ServerException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/Server.Status.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/BaseService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/ServerException.ERROR.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/ServiceException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/MDCFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/ServerWebApp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/HostnameFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/class-use/MDCFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/class-use/ServerWebApp.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/class-use/HostnameFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/class-use/FileSystemReleaseFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/FileSystemReleaseFilter.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/Scheduler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/SchedulerService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/class-use/SchedulerService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/Instrumentation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/class-use/InstrumentationService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/InstrumentationService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/Groups.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/FileSystemAccess.FileSystemExecutor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/Scheduler.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/Instrumentation.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/Groups.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/FileSystemAccess.FileSystemExecutor.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/Instrumentation.Cron.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/FileSystemAccessException.ERROR.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/FileSystemAccessException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/Instrumentation.Variable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/FileSystemAccess.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/Instrumentation.Cron.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/class-use/GroupsService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/GroupsService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/class-use/FileSystemAccessService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/FileSystemAccessException.ERROR.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/FileSystemAccessException.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/Instrumentation.Variable.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/FileSystemAccess.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/EnumSetParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/Parameters.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/BooleanParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/ShortParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/StringParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/JSONMapProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/ExceptionProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/IntegerParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/Param.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/EnumSetParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/Parameters.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/BooleanParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/ShortParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/StringParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/JSONMapProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/ExceptionProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/IntegerParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/Param.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/LongParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/ParametersProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/JSONProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/EnumParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/ByteParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/InputStreamEntity.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/LongParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/ParametersProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/JSONProvider.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/EnumParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/ByteParam.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/package-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/InputStreamEntity.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/package-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/package-use.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/package-tree.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/overview-summary.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/overview-frame.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/allclasses-noframe.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/package-list\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/httpfs-default.html\n","hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/dependency-analysis.html\n","hadoop-3.3.0/lib/\n","hadoop-3.3.0/lib/native/\n","hadoop-3.3.0/lib/native/libhadoop.a\n","hadoop-3.3.0/lib/native/libhadooputils.a\n","hadoop-3.3.0/lib/native/libhadoop.so.1.0.0\n","hadoop-3.3.0/lib/native/examples/\n","hadoop-3.3.0/lib/native/examples/wordcount-simple\n","hadoop-3.3.0/lib/native/examples/pipes-sort\n","hadoop-3.3.0/lib/native/examples/wordcount-nopipe\n","hadoop-3.3.0/lib/native/examples/wordcount-part\n","hadoop-3.3.0/lib/native/libnativetask.so\n","hadoop-3.3.0/lib/native/libhadoop.so\n","hadoop-3.3.0/lib/native/libhadooppipes.a\n","hadoop-3.3.0/lib/native/libnativetask.so.1.0.0\n","hadoop-3.3.0/lib/native/libhdfs.so.0.0.0\n","hadoop-3.3.0/lib/native/libhdfs.a\n","hadoop-3.3.0/lib/native/libhdfs.so\n","hadoop-3.3.0/lib/native/libnativetask.a\n","hadoop-3.3.0/etc/\n","hadoop-3.3.0/etc/hadoop/\n","hadoop-3.3.0/etc/hadoop/hadoop-env.cmd\n","hadoop-3.3.0/etc/hadoop/workers\n","hadoop-3.3.0/etc/hadoop/log4j.properties\n","hadoop-3.3.0/etc/hadoop/yarnservice-log4j.properties\n","hadoop-3.3.0/etc/hadoop/hadoop-policy.xml\n","hadoop-3.3.0/etc/hadoop/kms-acls.xml\n","hadoop-3.3.0/etc/hadoop/hdfs-rbf-site.xml\n","hadoop-3.3.0/etc/hadoop/configuration.xsl\n","hadoop-3.3.0/etc/hadoop/container-executor.cfg\n","hadoop-3.3.0/etc/hadoop/ssl-client.xml.example\n","hadoop-3.3.0/etc/hadoop/httpfs-env.sh\n","hadoop-3.3.0/etc/hadoop/hadoop-user-functions.sh.example\n","hadoop-3.3.0/etc/hadoop/ssl-server.xml.example\n","hadoop-3.3.0/etc/hadoop/shellprofile.d/\n","hadoop-3.3.0/etc/hadoop/shellprofile.d/example.sh\n","hadoop-3.3.0/etc/hadoop/hdfs-site.xml\n","hadoop-3.3.0/etc/hadoop/kms-env.sh\n","hadoop-3.3.0/etc/hadoop/core-site.xml\n","hadoop-3.3.0/etc/hadoop/hadoop-env.sh\n","hadoop-3.3.0/etc/hadoop/mapred-queues.xml.template\n","hadoop-3.3.0/etc/hadoop/yarn-env.cmd\n","hadoop-3.3.0/etc/hadoop/mapred-env.cmd\n","hadoop-3.3.0/etc/hadoop/yarn-env.sh\n","hadoop-3.3.0/etc/hadoop/httpfs-site.xml\n","hadoop-3.3.0/etc/hadoop/kms-log4j.properties\n","hadoop-3.3.0/etc/hadoop/mapred-site.xml\n","hadoop-3.3.0/etc/hadoop/yarn-site.xml\n","hadoop-3.3.0/etc/hadoop/capacity-scheduler.xml\n","hadoop-3.3.0/etc/hadoop/hadoop-metrics2.properties\n","hadoop-3.3.0/etc/hadoop/user_ec_policies.xml.template\n","hadoop-3.3.0/etc/hadoop/mapred-env.sh\n","hadoop-3.3.0/etc/hadoop/kms-site.xml\n","hadoop-3.3.0/etc/hadoop/httpfs-log4j.properties\n"]}]},{"cell_type":"code","metadata":{"id":"0JNT_W-1YZN3"},"source":["! rm hadoop-3.3.0.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OUc19ZtcBG5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637609213937,"user_tz":-120,"elapsed":408,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaGRMxyeD3t85VD8VHhTPNz3NiYujdbYaaDwHMw=s64","userId":"07034748551478156399"}},"outputId":"a946d750-fb2f-4a78-c07f-0410b7106a6f"},"source":["#To find the default Java path\n","!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n","\n","# edit the java home folder, hdfs-site.xml, and core-site.xml"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/lib/jvm/java-11-openjdk-amd64/\n"]}]},{"cell_type":"code","metadata":{"id":"37MwfuXVod9N"},"source":["# this cell copies the already edited <hadoop_env.sh, hdfs_site.xml, and core_site.xml>\n","# files I've put on my drive to this hadoop installation to save some time\n","!cp /content/drive/MyDrive/mp1-bigdata/hadoop-3.3.0/core-site.xml  hadoop-3.3.0/etc/hadoop\n","!cp  /content/drive/MyDrive/mp1-bigdata/hadoop-3.3.0/hdfs-site.xml  hadoop-3.3.0/etc/hadoop\n","!cp  /content/drive/MyDrive/mp1-bigdata/hadoop-3.3.0/hadoop-env.sh  hadoop-3.3.0/etc/hadoop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JF-ze-YOdync"},"source":["#copy  hadoop file to user/local\n","!cp -r hadoop-3.3.0/ /usr/local/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehRrSeBGEvHh","executionInfo":{"status":"ok","timestamp":1637609219787,"user_tz":-120,"elapsed":482,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaGRMxyeD3t85VD8VHhTPNz3NiYujdbYaaDwHMw=s64","userId":"07034748551478156399"}},"outputId":"250d9f34-1316-4ab0-bee7-002459fb55d9"},"source":["! ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n","!cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n","!chmod 0600 ~/.ssh/authorized_keys"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating public/private rsa key pair.\n","Created directory '/root/.ssh'.\n","Your identification has been saved in /root/.ssh/id_rsa.\n","Your public key has been saved in /root/.ssh/id_rsa.pub.\n","The key fingerprint is:\n","SHA256:jBVFodfhwCHSWK8x7d33JxliC4EoKfl6EYciAOfPrSk root@9ccd6bc14e0d\n","The key's randomart image is:\n","+---[RSA 2048]----+\n","|+ .   .+++*o.    |\n","|.o . o.o.B.+ .   |\n","|. = = o * = o    |\n","| . * = + * o .   |\n","|    = o S o + o .|\n","|   . +     o o +.|\n","|  E +       . o o|\n","|   o           ..|\n","|                 |\n","+----[SHA256]-----+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ufXj0kUH7RL","executionInfo":{"status":"ok","timestamp":1637609223727,"user_tz":-120,"elapsed":3948,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaGRMxyeD3t85VD8VHhTPNz3NiYujdbYaaDwHMw=s64","userId":"07034748551478156399"}},"outputId":"d90b62fe-bff0-4c2b-bfed-23d6cd5a6a9c"},"source":["!  /usr/local/hadoop-3.3.0/bin/hdfs namenode -format\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING: /usr/local/hadoop-3.3.0/logs does not exist. Creating.\n","2021-11-22 19:26:56,625 INFO namenode.NameNode: STARTUP_MSG: \n","/************************************************************\n","STARTUP_MSG: Starting NameNode\n","STARTUP_MSG:   host = 9ccd6bc14e0d/172.28.0.2\n","STARTUP_MSG:   args = [-format]\n","STARTUP_MSG:   version = 3.3.0\n","STARTUP_MSG:   classpath = /usr/local/hadoop-3.3.0/etc/hadoop:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jetty-util-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/nimbus-jose-jwt-7.9.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jetty-http-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jackson-databind-2.10.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jackson-annotations-2.10.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.0.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/hadoop-auth-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-compress-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-io-2.5.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jackson-core-2.10.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/zookeeper-jute-3.5.6.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jetty-security-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jetty-server-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/zookeeper-3.5.6.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/curator-framework-4.2.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jetty-xml-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jetty-io-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/curator-recipes-4.2.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/curator-client-4.2.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/lib/hadoop-annotations-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/hadoop-nfs-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/hadoop-registry-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/hadoop-common-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/hadoop-kms-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/common/hadoop-common-3.3.0-tests.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-7.9.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jackson-databind-2.10.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jackson-annotations-2.10.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.0.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/hadoop-auth-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-compress-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/netty-all-4.1.50.Final.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jackson-core-2.10.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/zookeeper-jute-3.5.6.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/zookeeper-3.5.6.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/curator-framework-4.2.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/curator-recipes-4.2.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/curator-client-4.2.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/lib/hadoop-annotations-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/hadoop-hdfs-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.0-tests.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/hadoop-hdfs-client-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.0-tests.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/hadoop-hdfs-3.3.0-tests.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/hdfs/hadoop-hdfs-client-3.3.0-tests.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.0-tests.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/websocket-client-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/websocket-server-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jetty-client-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/asm-tree-7.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jetty-jndi-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jline-3.9.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jetty-plus-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/websocket-servlet-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/asm-analysis-7.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/websocket-common-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.3.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/asm-commons-7.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/websocket-api-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/lib/jetty-annotations-9.4.20.v20190813.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-registry-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-common-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-services-core-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-server-router-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-api-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-services-api-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-client-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-server-common-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.0.jar:/usr/local/hadoop-3.3.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.0.jar\n","STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r aa96f1871bfd858f9bac59cf2a81ec470da649af; compiled by 'brahma' on 2020-07-06T18:44Z\n","STARTUP_MSG:   java = 11.0.11\n","************************************************************/\n","2021-11-22 19:26:56,716 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n","2021-11-22 19:26:56,891 INFO namenode.NameNode: createNameNode [-format]\n","2021-11-22 19:26:57,927 INFO namenode.NameNode: Formatting using clusterid: CID-21b1cc42-f5ae-4aee-a20f-f1400b42865a\n","2021-11-22 19:26:57,983 INFO namenode.FSEditLog: Edit logging is async:true\n","2021-11-22 19:26:58,039 INFO namenode.FSNamesystem: KeyProvider: null\n","2021-11-22 19:26:58,041 INFO namenode.FSNamesystem: fsLock is fair: true\n","2021-11-22 19:26:58,041 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n","2021-11-22 19:26:58,149 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n","2021-11-22 19:26:58,149 INFO namenode.FSNamesystem: supergroup             = supergroup\n","2021-11-22 19:26:58,150 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n","2021-11-22 19:26:58,150 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n","2021-11-22 19:26:58,150 INFO namenode.FSNamesystem: HA Enabled: false\n","2021-11-22 19:26:58,203 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n","2021-11-22 19:26:58,227 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n","2021-11-22 19:26:58,227 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n","2021-11-22 19:26:58,233 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n","2021-11-22 19:26:58,234 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Nov 22 19:26:58\n","2021-11-22 19:26:58,236 INFO util.GSet: Computing capacity for map BlocksMap\n","2021-11-22 19:26:58,236 INFO util.GSet: VM type       = 64-bit\n","2021-11-22 19:26:58,238 INFO util.GSet: 2.0% max memory 3.2 GB = 65.0 MB\n","2021-11-22 19:26:58,238 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n","2021-11-22 19:26:58,269 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n","2021-11-22 19:26:58,269 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n","2021-11-22 19:26:58,275 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n","2021-11-22 19:26:58,275 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n","2021-11-22 19:26:58,275 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n","2021-11-22 19:26:58,276 INFO blockmanagement.BlockManager: defaultReplication         = 1\n","2021-11-22 19:26:58,276 INFO blockmanagement.BlockManager: maxReplication             = 512\n","2021-11-22 19:26:58,276 INFO blockmanagement.BlockManager: minReplication             = 1\n","2021-11-22 19:26:58,276 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n","2021-11-22 19:26:58,276 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n","2021-11-22 19:26:58,276 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n","2021-11-22 19:26:58,276 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n","2021-11-22 19:26:58,300 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n","2021-11-22 19:26:58,300 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n","2021-11-22 19:26:58,301 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n","2021-11-22 19:26:58,301 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n","2021-11-22 19:26:58,315 INFO util.GSet: Computing capacity for map INodeMap\n","2021-11-22 19:26:58,315 INFO util.GSet: VM type       = 64-bit\n","2021-11-22 19:26:58,316 INFO util.GSet: 1.0% max memory 3.2 GB = 32.5 MB\n","2021-11-22 19:26:58,316 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n","2021-11-22 19:26:58,329 INFO namenode.FSDirectory: ACLs enabled? true\n","2021-11-22 19:26:58,329 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n","2021-11-22 19:26:58,329 INFO namenode.FSDirectory: XAttrs enabled? true\n","2021-11-22 19:26:58,329 INFO namenode.NameNode: Caching file names occurring more than 10 times\n","2021-11-22 19:26:58,334 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n","2021-11-22 19:26:58,336 INFO snapshot.SnapshotManager: SkipList is disabled\n","2021-11-22 19:26:58,340 INFO util.GSet: Computing capacity for map cachedBlocks\n","2021-11-22 19:26:58,340 INFO util.GSet: VM type       = 64-bit\n","2021-11-22 19:26:58,341 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n","2021-11-22 19:26:58,341 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n","2021-11-22 19:26:58,351 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n","2021-11-22 19:26:58,351 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n","2021-11-22 19:26:58,351 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n","2021-11-22 19:26:58,355 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n","2021-11-22 19:26:58,358 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n","2021-11-22 19:26:58,361 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n","2021-11-22 19:26:58,361 INFO util.GSet: VM type       = 64-bit\n","2021-11-22 19:26:58,361 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 997.8 KB\n","2021-11-22 19:26:58,361 INFO util.GSet: capacity      = 2^17 = 131072 entries\n","2021-11-22 19:26:58,399 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1291916315-172.28.0.2-1637609218388\n","2021-11-22 19:26:58,446 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n","2021-11-22 19:26:58,506 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n","2021-11-22 19:26:58,701 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n","2021-11-22 19:26:58,724 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n","2021-11-22 19:26:58,732 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n","2021-11-22 19:26:58,732 INFO namenode.NameNode: SHUTDOWN_MSG: \n","/************************************************************\n","SHUTDOWN_MSG: Shutting down NameNode at 9ccd6bc14e0d/172.28.0.2\n","************************************************************/\n"]}]},{"cell_type":"code","metadata":{"id":"TgJnn9iB_hrL"},"source":["%%bash\n","/usr/local/hadoop-3.3.0/bin/hdfs --daemon start namenode && /usr/local/hadoop-3.3.0/bin/hdfs --daemon start datanode\n","/usr/local/hadoop-3.3.0/bin/yarn --daemon start resourcemanager && /usr/local/hadoop-3.3.0/bin/yarn --daemon start nodemanager"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpt4ZWDcF28i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637609236045,"user_tz":-120,"elapsed":1549,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaGRMxyeD3t85VD8VHhTPNz3NiYujdbYaaDwHMw=s64","userId":"07034748551478156399"}},"outputId":"a5f659b2-3d2a-4394-c806-69e5455e91e8"},"source":["!jps"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["659 ResourceManager\n","789 Jps\n","761 NodeManager\n","508 NameNode\n","590 DataNode\n"]}]},{"cell_type":"markdown","metadata":{"id":"WHuUqOJX1eZ9"},"source":["#Downloading and extracting the dataset into a drive folder. RUN ONLY ONCE"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wiZTAeafR-Uc","executionInfo":{"status":"ok","timestamp":1636491377095,"user_tz":-120,"elapsed":229149,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"777fe0a8-b449-4857-f1fd-1aa178af4e09"},"source":["# download the dataset\n","!gdown --id 1-D_uHkn37M5ptWVQl8a5-q8NBv9jaLWr"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1-D_uHkn37M5ptWVQl8a5-q8NBv9jaLWr\n","To: /content/RC_2015-01.bz2\n","100% 5.45G/5.45G [02:00<00:00, 45.1MB/s]\n"]}]},{"cell_type":"code","metadata":{"id":"UHpadadJkOPq"},"source":["# move the dataset to drive folder\n","!cp RC_2015-01.bz2 /content/drive/MyDrive/mp1-bigdata/compressed_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"afkOZkgXS3AT"},"source":["# unzip the data\n","import os\n","!bunzip2 /content/drive/MyDrive/mp1-bigdata/input/RC_2015-01.bz2   \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"03x7yLp7zES1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPPGOa-5OfeG"},"source":["#ANALYSIS TASKS:"]},{"cell_type":"markdown","metadata":{"id":"HXqa7EIyxkn9"},"source":["## data cleaning job RUN ONLY ONCE"]},{"cell_type":"code","metadata":{"id":"bQyeZwcgtEwh"},"source":["# put the compressed raw data into the HDFS\n","!${hdfs}  dfs -put /content/drive/MyDrive/mp1-bigdata/compressed_data/ /user/$USER/job\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NBu7ccl0tRau","executionInfo":{"status":"ok","timestamp":1636504193935,"user_tz":-120,"elapsed":2183496,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"04ab7a11-3127-4952-8542-21156287728e"},"source":["# run a cleaning mapreduce job\n","!${hadoop_stream}  -input /user/$USER/job/compressed_data/ -output /user/$USER/job/clean_data  -file /content/drive/MyDrive/mp1-bigdata/data_cleaning_job/mapper.py  -file /content/drive/MyDrive/mp1-bigdata/data_cleaning_job/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","2021-11-09 22:33:25,841 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:33:25,841 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:33:25,841 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:33:25,849 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:33:25,858 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:33:25,892 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:33:25,893 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:33:25,898 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:33:25,954 INFO streaming.PipeMapRed: Records R/W=211/1\n","2021-11-09 22:33:25,992 INFO streaming.PipeMapRed: R/W/S=1000/834/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:33:26,261 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:33:26,560 INFO streaming.PipeMapRed: R/W/S=10000/9799/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:33:31,744 INFO streaming.PipeMapRed: R/W/S=100000/99895/0 in:20000=100000/5 [rec/s] out:19979=99895/5 [rec/s]\n","2021-11-09 22:33:31,798 INFO mapred.LocalJobRunner: Records R/W=211/1 > map\n","2021-11-09 22:33:32,263 INFO mapreduce.Job:  map 46% reduce 0%\n","2021-11-09 22:33:35,965 INFO streaming.PipeMapRed: Records R/W=175766/175533\n","2021-11-09 22:33:37,235 INFO streaming.PipeMapRed: R/W/S=200000/199786/0 in:18181=200000/11 [rec/s] out:18162=199786/11 [rec/s]\n","2021-11-09 22:33:37,799 INFO mapred.LocalJobRunner: Records R/W=175766/175533 > map\n","2021-11-09 22:33:38,265 INFO mapreduce.Job:  map 47% reduce 0%\n","2021-11-09 22:33:39,769 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:33:39,769 INFO mapred.MapTask: bufstart = 0; bufend = 79953927; bufvoid = 104857600\n","2021-11-09 22:33:39,769 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25231296(100925184); length = 983101/6553600\n","2021-11-09 22:33:39,769 INFO mapred.MapTask: (EQUATOR) 80937911 kvi 20234472(80937888)\n","2021-11-09 22:33:40,649 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:33:40,650 INFO mapred.MapTask: (RESET) equator 80937911 kv 20234472(80937888) kvi 20194016(80776064)\n","2021-11-09 22:33:43,125 INFO streaming.PipeMapRed: R/W/S=300000/299777/0 in:17647=300000/17 [rec/s] out:17633=299777/17 [rec/s]\n","2021-11-09 22:33:43,799 INFO mapred.LocalJobRunner: Records R/W=175766/175533 > map\n","2021-11-09 22:33:45,978 INFO streaming.PipeMapRed: Records R/W=351467/351233\n","2021-11-09 22:33:48,812 INFO streaming.PipeMapRed: R/W/S=400000/399791/0 in:18181=400000/22 [rec/s] out:18172=399791/22 [rec/s]\n","2021-11-09 22:33:49,800 INFO mapred.LocalJobRunner: Records R/W=351467/351233 > map\n","2021-11-09 22:33:53,887 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:33:53,887 INFO mapred.MapTask: bufstart = 80937911; bufend = 55945198; bufvoid = 104857385\n","2021-11-09 22:33:53,887 INFO mapred.MapTask: kvstart = 20234472(80937888); kvend = 19229180(76916720); length = 1005293/6553600\n","2021-11-09 22:33:53,887 INFO mapred.MapTask: (EQUATOR) 56940862 kvi 14235208(56940832)\n","2021-11-09 22:33:54,123 INFO streaming.PipeMapRed: R/W/S=500000/499890/0 in:17857=500000/28 [rec/s] out:17853=499890/28 [rec/s]\n","2021-11-09 22:33:54,847 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:33:54,847 INFO mapred.MapTask: (RESET) equator 56940862 kv 14235208(56940832) kvi 14190328(56761312)\n","2021-11-09 22:33:55,801 INFO mapred.LocalJobRunner: Records R/W=351467/351233 > map\n","2021-11-09 22:33:55,979 INFO streaming.PipeMapRed: Records R/W=529081/528951\n","2021-11-09 22:33:59,747 INFO streaming.PipeMapRed: R/W/S=600000/599843/0 in:18181=600000/33 [rec/s] out:18177=599843/33 [rec/s]\n","2021-11-09 22:34:01,801 INFO mapred.LocalJobRunner: Records R/W=529081/528951 > map\n","2021-11-09 22:34:05,203 INFO streaming.PipeMapRed: R/W/S=700000/699803/0 in:17948=700000/39 [rec/s] out:17943=699803/39 [rec/s]\n","2021-11-09 22:34:05,980 INFO streaming.PipeMapRed: Records R/W=715191/715051\n","2021-11-09 22:34:07,802 INFO mapred.LocalJobRunner: Records R/W=715191/715051 > map\n","2021-11-09 22:34:08,120 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:34:08,120 INFO mapred.MapTask: bufstart = 56940862; bufend = 31833240; bufvoid = 104857450\n","2021-11-09 22:34:08,120 INFO mapred.MapTask: kvstart = 14235208(56940832); kvend = 13201140(52804560); length = 1034069/6553600\n","2021-11-09 22:34:08,120 INFO mapred.MapTask: (EQUATOR) 32840856 kvi 8210208(32840832)\n","2021-11-09 22:34:09,316 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:34:09,323 INFO mapred.MapTask: (RESET) equator 32840856 kv 8210208(32840832) kvi 8157612(32630448)\n","2021-11-09 22:34:11,017 INFO streaming.PipeMapRed: R/W/S=800000/799867/0 in:17777=800000/45 [rec/s] out:17774=799867/45 [rec/s]\n","2021-11-09 22:34:13,802 INFO mapred.LocalJobRunner: Records R/W=715191/715051 > map\n","2021-11-09 22:34:15,981 INFO streaming.PipeMapRed: Records R/W=896062/895946\n","2021-11-09 22:34:16,205 INFO streaming.PipeMapRed: R/W/S=900000/899874/0 in:18000=900000/50 [rec/s] out:17997=899874/50 [rec/s]\n","2021-11-09 22:34:19,803 INFO mapred.LocalJobRunner: Records R/W=896062/895946 > map\n","2021-11-09 22:34:20,279 INFO mapreduce.Job:  map 48% reduce 0%\n","2021-11-09 22:34:21,580 INFO streaming.PipeMapRed: R/W/S=1000000/999809/0 in:18181=1000000/55 [rec/s] out:18178=999809/55 [rec/s]\n","2021-11-09 22:34:22,572 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:34:22,572 INFO mapred.MapTask: bufstart = 32840856; bufend = 7608187; bufvoid = 104857165\n","2021-11-09 22:34:22,572 INFO mapred.MapTask: kvstart = 8210208(32840832); kvend = 7144868(28579472); length = 1065341/6553600\n","2021-11-09 22:34:22,572 INFO mapred.MapTask: (EQUATOR) 8631163 kvi 2157784(8631136)\n","2021-11-09 22:34:23,735 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:34:23,736 INFO mapred.MapTask: (RESET) equator 8631163 kv 2157784(8631136) kvi 2105428(8421712)\n","2021-11-09 22:34:25,803 INFO mapred.LocalJobRunner: Records R/W=896062/895946 > map\n","2021-11-09 22:34:25,982 INFO streaming.PipeMapRed: Records R/W=1075833/1075608\n","2021-11-09 22:34:27,169 INFO streaming.PipeMapRed: R/W/S=1100000/1099810/0 in:18032=1100000/61 [rec/s] out:18029=1099810/61 [rec/s]\n","2021-11-09 22:34:31,804 INFO mapred.LocalJobRunner: Records R/W=1075833/1075608 > map\n","2021-11-09 22:34:32,435 INFO streaming.PipeMapRed: R/W/S=1200000/1199749/0 in:18181=1200000/66 [rec/s] out:18178=1199752/66 [rec/s]\n","2021-11-09 22:34:35,983 INFO streaming.PipeMapRed: Records R/W=1265256/1265111\n","2021-11-09 22:34:36,672 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:34:36,672 INFO mapred.MapTask: bufstart = 8631163; bufend = 88417504; bufvoid = 104857600\n","2021-11-09 22:34:36,672 INFO mapred.MapTask: kvstart = 2157784(8631136); kvend = 1132780(4531120); length = 1025005/6553600\n","2021-11-09 22:34:36,672 INFO mapred.MapTask: (EQUATOR) 89440480 kvi 22360116(89440464)\n","2021-11-09 22:34:37,760 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:34:37,765 INFO mapred.MapTask: (RESET) equator 89440480 kv 22360116(89440464) kvi 22310340(89241360)\n","2021-11-09 22:34:37,805 INFO mapred.LocalJobRunner: Records R/W=1265256/1265111 > map\n","2021-11-09 22:34:38,242 INFO streaming.PipeMapRed: R/W/S=1300000/1299845/0 in:18055=1300000/72 [rec/s] out:18053=1299845/72 [rec/s]\n","2021-11-09 22:34:39,884 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:34:39,884 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:34:39,885 INFO mapred.LocalJobRunner: Records R/W=1265256/1265111 > map\n","2021-11-09 22:34:39,886 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:34:39,886 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:34:39,886 INFO mapred.MapTask: bufstart = 89440480; bufend = 975974; bufvoid = 104857541\n","2021-11-09 22:34:39,886 INFO mapred.MapTask: kvstart = 22360116(89440464); kvend = 22158780(88635120); length = 201337/6553600\n","2021-11-09 22:34:39,994 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:34:39,996 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:34:39,997 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 419873900 bytes\n","2021-11-09 22:34:42,593 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000019_0 is done. And is in the process of committing\n","2021-11-09 22:34:42,602 INFO mapred.LocalJobRunner: Records R/W=1265256/1265111 > sort\n","2021-11-09 22:34:42,602 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000019_0' done.\n","2021-11-09 22:34:42,603 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000019_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=8424365592\n","\t\tFILE: Number of bytes written=16849199740\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2689344512\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=43\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1328541\n","\t\tMap output records=1328541\n","\t\tMap output bytes=415372299\n","\t\tMap output materialized bytes=419875635\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2657082\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134498304\n","2021-11-09 22:34:42,603 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000019_0\n","2021-11-09 22:34:42,603 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000020_0\n","2021-11-09 22:34:42,604 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:34:42,604 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:34:42,604 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:34:42,605 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:2684354560+134217728\n","2021-11-09 22:34:42,653 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:34:42,669 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:34:42,669 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:34:42,669 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:34:42,669 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:34:42,669 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:34:42,679 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:34:42,686 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:34:42,711 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:34:42,712 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:34:42,715 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:34:42,776 INFO streaming.PipeMapRed: Records R/W=223/1\n","2021-11-09 22:34:42,820 INFO streaming.PipeMapRed: R/W/S=1000/821/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:34:43,287 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:34:43,307 INFO streaming.PipeMapRed: R/W/S=10000/9883/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:34:47,825 INFO streaming.PipeMapRed: R/W/S=100000/99841/0 in:20000=100000/5 [rec/s] out:19968=99841/5 [rec/s]\n","2021-11-09 22:34:52,785 INFO streaming.PipeMapRed: Records R/W=186012/185770\n","2021-11-09 22:34:53,577 INFO streaming.PipeMapRed: R/W/S=200000/199812/0 in:20000=200000/10 [rec/s] out:19981=199812/10 [rec/s]\n","2021-11-09 22:34:54,605 INFO mapred.LocalJobRunner: Records R/W=186012/185770 > map\n","2021-11-09 22:34:55,291 INFO mapreduce.Job:  map 49% reduce 0%\n","2021-11-09 22:34:56,127 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:34:56,127 INFO mapred.MapTask: bufstart = 0; bufend = 79930879; bufvoid = 104857600\n","2021-11-09 22:34:56,127 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25225572(100902288); length = 988825/6553600\n","2021-11-09 22:34:56,127 INFO mapred.MapTask: (EQUATOR) 80920671 kvi 20230160(80920640)\n","2021-11-09 22:34:56,994 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:34:56,997 INFO mapred.MapTask: (RESET) equator 80920671 kv 20230160(80920640) kvi 20192756(80771024)\n","2021-11-09 22:34:59,403 INFO streaming.PipeMapRed: R/W/S=300000/299811/0 in:18750=300000/16 [rec/s] out:18738=299811/16 [rec/s]\n","2021-11-09 22:35:00,606 INFO mapred.LocalJobRunner: Records R/W=186012/185770 > map\n","2021-11-09 22:35:02,786 INFO streaming.PipeMapRed: Records R/W=359656/359492\n","2021-11-09 22:35:04,989 INFO streaming.PipeMapRed: R/W/S=400000/399889/0 in:18181=400000/22 [rec/s] out:18176=399889/22 [rec/s]\n","2021-11-09 22:35:06,606 INFO mapred.LocalJobRunner: Records R/W=359656/359492 > map\n","2021-11-09 22:35:10,233 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:35:10,233 INFO mapred.MapTask: bufstart = 80920671; bufend = 56003481; bufvoid = 104857428\n","2021-11-09 22:35:10,233 INFO mapred.MapTask: kvstart = 20230160(80920640); kvend = 19243704(76974816); length = 986457/6553600\n","2021-11-09 22:35:10,233 INFO mapred.MapTask: (EQUATOR) 56993273 kvi 14248312(56993248)\n","2021-11-09 22:35:10,811 INFO streaming.PipeMapRed: R/W/S=500000/499831/0 in:17857=500000/28 [rec/s] out:17851=499831/28 [rec/s]\n","2021-11-09 22:35:11,177 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:35:11,194 INFO mapred.MapTask: (RESET) equator 56993273 kv 14248312(56993248) kvi 14208604(56834416)\n","2021-11-09 22:35:12,607 INFO mapred.LocalJobRunner: Records R/W=359656/359492 > map\n","2021-11-09 22:35:12,787 INFO streaming.PipeMapRed: Records R/W=531186/531123\n","2021-11-09 22:35:16,815 INFO streaming.PipeMapRed: R/W/S=600000/599829/0 in:17647=600000/34 [rec/s] out:17642=599829/34 [rec/s]\n","2021-11-09 22:35:18,608 INFO mapred.LocalJobRunner: Records R/W=531186/531123 > map\n","2021-11-09 22:35:19,299 INFO mapreduce.Job:  map 50% reduce 0%\n","2021-11-09 22:35:22,131 INFO streaming.PipeMapRed: R/W/S=700000/699828/0 in:17948=700000/39 [rec/s] out:17944=699828/39 [rec/s]\n","2021-11-09 22:35:22,791 INFO streaming.PipeMapRed: Records R/W=711640/711411\n","2021-11-09 22:35:24,608 INFO mapred.LocalJobRunner: Records R/W=711640/711411 > map\n","2021-11-09 22:35:24,678 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:35:24,678 INFO mapred.MapTask: bufstart = 56993273; bufend = 32024206; bufvoid = 104857546\n","2021-11-09 22:35:24,678 INFO mapred.MapTask: kvstart = 14248312(56993248); kvend = 13248892(52995568); length = 999421/6553600\n","2021-11-09 22:35:24,678 INFO mapred.MapTask: (EQUATOR) 33016926 kvi 8254224(33016896)\n","2021-11-09 22:35:25,687 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:35:25,687 INFO mapred.MapTask: (RESET) equator 33016926 kv 8254224(33016896) kvi 8208740(32834960)\n","2021-11-09 22:35:28,200 INFO streaming.PipeMapRed: R/W/S=800000/799880/0 in:17777=800000/45 [rec/s] out:17775=799880/45 [rec/s]\n","2021-11-09 22:35:30,609 INFO mapred.LocalJobRunner: Records R/W=711640/711411 > map\n","2021-11-09 22:35:32,792 INFO streaming.PipeMapRed: Records R/W=885309/885098\n","2021-11-09 22:35:33,571 INFO streaming.PipeMapRed: R/W/S=900000/899869/0 in:18000=900000/50 [rec/s] out:17997=899869/50 [rec/s]\n","2021-11-09 22:35:36,610 INFO mapred.LocalJobRunner: Records R/W=885309/885098 > map\n","2021-11-09 22:35:38,608 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:35:38,608 INFO mapred.MapTask: bufstart = 33016926; bufend = 8010127; bufvoid = 104857351\n","2021-11-09 22:35:38,608 INFO mapred.MapTask: kvstart = 8254224(33016896); kvend = 7245308(28981232); length = 1008917/6553600\n","2021-11-09 22:35:38,608 INFO mapred.MapTask: (EQUATOR) 9008735 kvi 2252176(9008704)\n","2021-11-09 22:35:38,962 INFO streaming.PipeMapRed: R/W/S=1000000/999974/0 in:17857=1000000/56 [rec/s] out:17856=999974/56 [rec/s]\n","2021-11-09 22:35:39,580 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:35:39,587 INFO mapred.MapTask: (RESET) equator 9008735 kv 2252176(9008704) kvi 2208552(8834208)\n","2021-11-09 22:35:42,610 INFO mapred.LocalJobRunner: Records R/W=885309/885098 > map\n","2021-11-09 22:35:42,794 INFO streaming.PipeMapRed: Records R/W=1067267/1067025\n","2021-11-09 22:35:44,522 INFO streaming.PipeMapRed: R/W/S=1100000/1099887/0 in:18032=1100000/61 [rec/s] out:18030=1099887/61 [rec/s]\n","2021-11-09 22:35:48,611 INFO mapred.LocalJobRunner: Records R/W=1067267/1067025 > map\n","2021-11-09 22:35:49,601 INFO streaming.PipeMapRed: R/W/S=1200000/1199828/0 in:18181=1200000/66 [rec/s] out:18179=1199828/66 [rec/s]\n","2021-11-09 22:35:52,528 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:35:52,528 INFO mapred.MapTask: bufstart = 9008735; bufend = 88719183; bufvoid = 104857600\n","2021-11-09 22:35:52,528 INFO mapred.MapTask: kvstart = 2252176(9008704); kvend = 1208272(4833088); length = 1043905/6553600\n","2021-11-09 22:35:52,528 INFO mapred.MapTask: (EQUATOR) 89726815 kvi 22431696(89726784)\n","2021-11-09 22:35:52,796 INFO streaming.PipeMapRed: Records R/W=1260153/1259891\n","2021-11-09 22:35:53,547 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:35:53,550 INFO mapred.MapTask: (RESET) equator 89726815 kv 22431696(89726784) kvi 22384192(89536768)\n","2021-11-09 22:35:54,629 INFO mapred.LocalJobRunner: Records R/W=1260153/1259891 > map\n","2021-11-09 22:35:55,160 INFO streaming.PipeMapRed: R/W/S=1300000/1299873/0 in:18055=1300000/72 [rec/s] out:18053=1299873/72 [rec/s]\n","2021-11-09 22:35:55,635 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:35:55,636 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:35:55,636 INFO mapred.LocalJobRunner: Records R/W=1260153/1259891 > map\n","2021-11-09 22:35:55,636 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:35:55,636 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:35:55,636 INFO mapred.MapTask: bufstart = 89726815; bufend = 37498; bufvoid = 104857143\n","2021-11-09 22:35:55,636 INFO mapred.MapTask: kvstart = 22431696(89726784); kvend = 22223204(88892816); length = 208493/6553600\n","2021-11-09 22:35:55,749 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:35:55,751 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:35:55,752 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 418941646 bytes\n","2021-11-09 22:35:57,976 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000020_0 is done. And is in the process of committing\n","2021-11-09 22:35:57,981 INFO mapred.LocalJobRunner: Records R/W=1260153/1259891 > sort\n","2021-11-09 22:35:57,981 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000020_0' done.\n","2021-11-09 22:35:57,981 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000020_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=8843311886\n","\t\tFILE: Number of bytes written=17687087050\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2823816192\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=45\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1309009\n","\t\tMap output records=1309009\n","\t\tMap output bytes=414488422\n","\t\tMap output materialized bytes=418943624\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2618018\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134471680\n","2021-11-09 22:35:57,981 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000020_0\n","2021-11-09 22:35:57,981 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000021_0\n","2021-11-09 22:35:57,982 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:35:57,982 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:35:57,982 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:35:57,983 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:2818572288+134217728\n","2021-11-09 22:35:58,023 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:35:58,036 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:35:58,037 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:35:58,037 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:35:58,037 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:35:58,037 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:35:58,040 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:35:58,051 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:35:58,064 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:35:58,065 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:35:58,082 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:35:58,137 INFO streaming.PipeMapRed: Records R/W=236/1\n","2021-11-09 22:35:58,165 INFO streaming.PipeMapRed: R/W/S=1000/843/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:35:58,319 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:35:58,655 INFO streaming.PipeMapRed: R/W/S=10000/9889/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:36:03,224 INFO streaming.PipeMapRed: R/W/S=100000/99769/0 in:20000=100000/5 [rec/s] out:19953=99769/5 [rec/s]\n","2021-11-09 22:36:08,138 INFO streaming.PipeMapRed: Records R/W=199004/198821\n","2021-11-09 22:36:08,193 INFO streaming.PipeMapRed: R/W/S=200000/199815/0 in:20000=200000/10 [rec/s] out:19981=199815/10 [rec/s]\n","2021-11-09 22:36:09,983 INFO mapred.LocalJobRunner: Records R/W=199004/198821 > map\n","2021-11-09 22:36:10,322 INFO mapreduce.Job:  map 51% reduce 0%\n","2021-11-09 22:36:11,645 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:36:11,645 INFO mapred.MapTask: bufstart = 0; bufend = 79580927; bufvoid = 104857600\n","2021-11-09 22:36:11,645 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25138112(100552448); length = 1076285/6553600\n","2021-11-09 22:36:11,645 INFO mapred.MapTask: (EQUATOR) 80659839 kvi 20164952(80659808)\n","2021-11-09 22:36:12,617 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:36:12,618 INFO mapred.MapTask: (RESET) equator 80659839 kv 20164952(80659808) kvi 20117136(80468544)\n","2021-11-09 22:36:13,654 INFO streaming.PipeMapRed: R/W/S=300000/299806/0 in:20000=300000/15 [rec/s] out:19987=299806/15 [rec/s]\n","2021-11-09 22:36:15,984 INFO mapred.LocalJobRunner: Records R/W=199004/198821 > map\n","2021-11-09 22:36:16,326 INFO mapreduce.Job:  map 52% reduce 0%\n","2021-11-09 22:36:18,140 INFO streaming.PipeMapRed: Records R/W=379492/379253\n","2021-11-09 22:36:19,243 INFO streaming.PipeMapRed: R/W/S=400000/399756/0 in:19047=400000/21 [rec/s] out:19036=399756/21 [rec/s]\n","2021-11-09 22:36:21,984 INFO mapred.LocalJobRunner: Records R/W=379492/379253 > map\n","2021-11-09 22:36:24,554 INFO streaming.PipeMapRed: R/W/S=500000/499905/0 in:19230=500000/26 [rec/s] out:19227=499905/26 [rec/s]\n","2021-11-09 22:36:25,942 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:36:25,942 INFO mapred.MapTask: bufstart = 80659839; bufend = 55606934; bufvoid = 104857535\n","2021-11-09 22:36:25,942 INFO mapred.MapTask: kvstart = 20164952(80659808); kvend = 19144616(76578464); length = 1020337/6553600\n","2021-11-09 22:36:25,943 INFO mapred.MapTask: (EQUATOR) 56655494 kvi 14163868(56655472)\n","2021-11-09 22:36:26,905 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:36:26,906 INFO mapred.MapTask: (RESET) equator 56655494 kv 14163868(56655472) kvi 14121136(56484544)\n","2021-11-09 22:36:27,985 INFO mapred.LocalJobRunner: Records R/W=379492/379253 > map\n","2021-11-09 22:36:28,142 INFO streaming.PipeMapRed: Records R/W=557430/557199\n","2021-11-09 22:36:30,449 INFO streaming.PipeMapRed: R/W/S=600000/599802/0 in:18750=600000/32 [rec/s] out:18743=599802/32 [rec/s]\n","2021-11-09 22:36:33,986 INFO mapred.LocalJobRunner: Records R/W=557430/557199 > map\n","2021-11-09 22:36:35,622 INFO streaming.PipeMapRed: R/W/S=700000/699768/0 in:18918=700000/37 [rec/s] out:18912=699768/37 [rec/s]\n","2021-11-09 22:36:38,159 INFO streaming.PipeMapRed: Records R/W=748114/747887\n","2021-11-09 22:36:39,836 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:36:39,836 INFO mapred.MapTask: bufstart = 56655494; bufend = 31614776; bufvoid = 104857209\n","2021-11-09 22:36:39,836 INFO mapred.MapTask: kvstart = 14163868(56655472); kvend = 13146304(52585216); length = 1017565/6553600\n","2021-11-09 22:36:39,836 INFO mapred.MapTask: (EQUATOR) 32653544 kvi 8163380(32653520)\n","2021-11-09 22:36:39,986 INFO mapred.LocalJobRunner: Records R/W=748114/747887 > map\n","2021-11-09 22:36:40,798 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:36:40,799 INFO mapred.MapTask: (RESET) equator 32653544 kv 8163380(32653520) kvi 8118572(32474288)\n","2021-11-09 22:36:41,288 INFO streaming.PipeMapRed: R/W/S=800000/799875/0 in:18604=800000/43 [rec/s] out:18601=799875/43 [rec/s]\n","2021-11-09 22:36:45,987 INFO mapred.LocalJobRunner: Records R/W=748114/747887 > map\n","2021-11-09 22:36:46,611 INFO streaming.PipeMapRed: R/W/S=900000/899868/0 in:18750=900000/48 [rec/s] out:18747=899868/48 [rec/s]\n","2021-11-09 22:36:48,160 INFO streaming.PipeMapRed: Records R/W=928842/928719\n","2021-11-09 22:36:51,988 INFO mapred.LocalJobRunner: Records R/W=928842/928719 > map\n","2021-11-09 22:36:52,080 INFO streaming.PipeMapRed: R/W/S=1000000/999795/0 in:18518=1000000/54 [rec/s] out:18514=999795/54 [rec/s]\n","2021-11-09 22:36:53,828 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:36:53,828 INFO mapred.MapTask: bufstart = 32653544; bufend = 7604934; bufvoid = 104857543\n","2021-11-09 22:36:53,828 INFO mapred.MapTask: kvstart = 8163380(32653520); kvend = 7144060(28576240); length = 1019321/6553600\n","2021-11-09 22:36:53,828 INFO mapred.MapTask: (EQUATOR) 8640550 kvi 2160132(8640528)\n","2021-11-09 22:36:54,830 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:36:54,836 INFO mapred.MapTask: (RESET) equator 8640550 kv 2160132(8640528) kvi 2117496(8469984)\n","2021-11-09 22:36:57,914 INFO streaming.PipeMapRed: R/W/S=1100000/1099754/0 in:18644=1100000/59 [rec/s] out:18639=1099754/59 [rec/s]\n","2021-11-09 22:36:57,988 INFO mapred.LocalJobRunner: Records R/W=928842/928719 > map\n","2021-11-09 22:36:58,168 INFO streaming.PipeMapRed: Records R/W=1104229/1103963\n","2021-11-09 22:36:58,339 INFO mapreduce.Job:  map 53% reduce 0%\n","2021-11-09 22:37:03,434 INFO streaming.PipeMapRed: R/W/S=1200000/1199914/0 in:18461=1200000/65 [rec/s] out:18460=1199914/65 [rec/s]\n","2021-11-09 22:37:03,989 INFO mapred.LocalJobRunner: Records R/W=1104229/1103963 > map\n","2021-11-09 22:37:08,169 INFO streaming.PipeMapRed: Records R/W=1288539/1288308\n","2021-11-09 22:37:08,339 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:37:08,339 INFO mapred.MapTask: bufstart = 8640550; bufend = 88406261; bufvoid = 104857600\n","2021-11-09 22:37:08,340 INFO mapred.MapTask: kvstart = 2160132(8640528); kvend = 1130008(4520032); length = 1030125/6553600\n","2021-11-09 22:37:08,340 INFO mapred.MapTask: (EQUATOR) 89441877 kvi 22360464(89441856)\n","2021-11-09 22:37:09,072 INFO streaming.PipeMapRed: R/W/S=1300000/1299767/0 in:18309=1300000/71 [rec/s] out:18306=1299767/71 [rec/s]\n","2021-11-09 22:37:09,371 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:37:09,381 INFO mapred.MapTask: (RESET) equator 89441877 kv 22360464(89441856) kvi 22311636(89246544)\n","2021-11-09 22:37:09,989 INFO mapred.LocalJobRunner: Records R/W=1288539/1288308 > map\n","2021-11-09 22:37:11,773 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:37:11,774 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:37:11,776 INFO mapred.LocalJobRunner: Records R/W=1288539/1288308 > map\n","2021-11-09 22:37:11,776 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:37:11,776 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:37:11,776 INFO mapred.MapTask: bufstart = 89441877; bufend = 1588308; bufvoid = 104856051\n","2021-11-09 22:37:11,776 INFO mapred.MapTask: kvstart = 22360464(89441856); kvend = 22136544(88546176); length = 223921/6553600\n","2021-11-09 22:37:11,887 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:37:11,889 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:37:11,889 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 420327894 bytes\n","2021-11-09 22:37:14,147 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000021_0 is done. And is in the process of committing\n","2021-11-09 22:37:14,155 INFO mapred.LocalJobRunner: Records R/W=1288539/1288308 > sort\n","2021-11-09 22:37:14,155 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000021_0' done.\n","2021-11-09 22:37:14,155 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000021_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=9263643740\n","\t\tFILE: Number of bytes written=18527745480\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2958263296\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=47\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1346893\n","\t\tMap output records=1346893\n","\t\tMap output bytes=415779174\n","\t\tMap output materialized bytes=420329184\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2693786\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134447104\n","2021-11-09 22:37:14,155 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000021_0\n","2021-11-09 22:37:14,155 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000022_0\n","2021-11-09 22:37:14,156 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:37:14,156 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:37:14,156 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:37:14,157 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:2952790016+134217728\n","2021-11-09 22:37:14,202 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:37:14,220 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:37:14,220 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:37:14,220 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:37:14,220 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:37:14,220 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:37:14,220 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:37:14,226 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:37:14,259 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:37:14,260 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:37:14,263 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:37:14,323 INFO streaming.PipeMapRed: Records R/W=213/1\n","2021-11-09 22:37:14,344 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:37:14,353 INFO streaming.PipeMapRed: R/W/S=1000/831/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:37:14,809 INFO streaming.PipeMapRed: R/W/S=10000/9837/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:37:19,361 INFO streaming.PipeMapRed: R/W/S=100000/99832/0 in:20000=100000/5 [rec/s] out:19966=99832/5 [rec/s]\n","2021-11-09 22:37:24,340 INFO streaming.PipeMapRed: Records R/W=195901/195644\n","2021-11-09 22:37:24,533 INFO streaming.PipeMapRed: R/W/S=200000/199777/0 in:20000=200000/10 [rec/s] out:19977=199777/10 [rec/s]\n","2021-11-09 22:37:26,168 INFO mapred.LocalJobRunner: Records R/W=195901/195644 > map\n","2021-11-09 22:37:26,348 INFO mapreduce.Job:  map 54% reduce 0%\n","2021-11-09 22:37:28,051 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:37:28,051 INFO mapred.MapTask: bufstart = 0; bufend = 79642922; bufvoid = 104857600\n","2021-11-09 22:37:28,051 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25153608(100614432); length = 1060789/6553600\n","2021-11-09 22:37:28,051 INFO mapred.MapTask: (EQUATOR) 80704762 kvi 20176184(80704736)\n","2021-11-09 22:37:29,098 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:37:29,113 INFO mapred.MapTask: (RESET) equator 80704762 kv 20176184(80704736) kvi 20133980(80535920)\n","2021-11-09 22:37:30,408 INFO streaming.PipeMapRed: R/W/S=300000/299863/0 in:18750=300000/16 [rec/s] out:18741=299863/16 [rec/s]\n","2021-11-09 22:37:32,169 INFO mapred.LocalJobRunner: Records R/W=195901/195644 > map\n","2021-11-09 22:37:34,343 INFO streaming.PipeMapRed: Records R/W=374582/374356\n","2021-11-09 22:37:35,760 INFO streaming.PipeMapRed: R/W/S=400000/399899/0 in:19047=400000/21 [rec/s] out:19042=399899/21 [rec/s]\n","2021-11-09 22:37:38,170 INFO mapred.LocalJobRunner: Records R/W=374582/374356 > map\n","2021-11-09 22:37:41,004 INFO streaming.PipeMapRed: R/W/S=500000/499876/0 in:19230=500000/26 [rec/s] out:19226=499876/26 [rec/s]\n","2021-11-09 22:37:42,270 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:37:42,270 INFO mapred.MapTask: bufstart = 80704762; bufend = 55571360; bufvoid = 104857309\n","2021-11-09 22:37:42,270 INFO mapred.MapTask: kvstart = 20176184(80704736); kvend = 19135660(76542640); length = 1040525/6553600\n","2021-11-09 22:37:42,270 INFO mapred.MapTask: (EQUATOR) 56623200 kvi 14155796(56623184)\n","2021-11-09 22:37:43,251 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:37:43,258 INFO mapred.MapTask: (RESET) equator 56623200 kv 14155796(56623184) kvi 14107196(56428784)\n","2021-11-09 22:37:44,170 INFO mapred.LocalJobRunner: Records R/W=374582/374356 > map\n","2021-11-09 22:37:44,344 INFO streaming.PipeMapRed: Records R/W=558461/558331\n","2021-11-09 22:37:46,461 INFO streaming.PipeMapRed: R/W/S=600000/599854/0 in:18750=600000/32 [rec/s] out:18745=599854/32 [rec/s]\n","2021-11-09 22:37:50,171 INFO mapred.LocalJobRunner: Records R/W=558461/558331 > map\n","2021-11-09 22:37:51,870 INFO streaming.PipeMapRed: R/W/S=700000/699775/0 in:18918=700000/37 [rec/s] out:18912=699775/37 [rec/s]\n","2021-11-09 22:37:54,345 INFO streaming.PipeMapRed: Records R/W=748227/748030\n","2021-11-09 22:37:55,880 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:37:55,880 INFO mapred.MapTask: bufstart = 56623200; bufend = 31663653; bufvoid = 104857028\n","2021-11-09 22:37:55,880 INFO mapred.MapTask: kvstart = 14155796(56623184); kvend = 13158780(52635120); length = 997017/6553600\n","2021-11-09 22:37:55,880 INFO mapred.MapTask: (EQUATOR) 32699269 kvi 8174812(32699248)\n","2021-11-09 22:37:56,171 INFO mapred.LocalJobRunner: Records R/W=748227/748030 > map\n","2021-11-09 22:37:56,357 INFO mapreduce.Job:  map 55% reduce 0%\n","2021-11-09 22:37:56,907 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:37:56,908 INFO mapred.MapTask: (RESET) equator 32699269 kv 8174812(32699248) kvi 8132268(32529072)\n","2021-11-09 22:37:57,705 INFO streaming.PipeMapRed: R/W/S=800000/799903/0 in:18604=800000/43 [rec/s] out:18602=799903/43 [rec/s]\n","2021-11-09 22:38:02,172 INFO mapred.LocalJobRunner: Records R/W=748227/748030 > map\n","2021-11-09 22:38:02,948 INFO streaming.PipeMapRed: R/W/S=900000/899872/0 in:18750=900000/48 [rec/s] out:18747=899872/48 [rec/s]\n","2021-11-09 22:38:04,346 INFO streaming.PipeMapRed: Records R/W=927571/927361\n","2021-11-09 22:38:07,992 INFO streaming.PipeMapRed: R/W/S=1000000/999819/0 in:18867=1000000/53 [rec/s] out:18864=999819/53 [rec/s]\n","2021-11-09 22:38:08,173 INFO mapred.LocalJobRunner: Records R/W=927571/927361 > map\n","2021-11-09 22:38:09,498 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:38:09,498 INFO mapred.MapTask: bufstart = 32699269; bufend = 7668109; bufvoid = 104857592\n","2021-11-09 22:38:09,498 INFO mapred.MapTask: kvstart = 8174812(32699248); kvend = 7159832(28639328); length = 1014981/6553600\n","2021-11-09 22:38:09,498 INFO mapred.MapTask: (EQUATOR) 8697357 kvi 2174332(8697328)\n","2021-11-09 22:38:10,505 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:38:10,505 INFO mapred.MapTask: (RESET) equator 8697357 kv 2174332(8697328) kvi 2129156(8516624)\n","2021-11-09 22:38:13,772 INFO streaming.PipeMapRed: R/W/S=1100000/1099877/0 in:18644=1100000/59 [rec/s] out:18641=1099877/59 [rec/s]\n","2021-11-09 22:38:14,173 INFO mapred.LocalJobRunner: Records R/W=927571/927361 > map\n","2021-11-09 22:38:14,358 INFO streaming.PipeMapRed: Records R/W=1110272/1110055\n","2021-11-09 22:38:19,076 INFO streaming.PipeMapRed: R/W/S=1200000/1199766/0 in:18750=1200000/64 [rec/s] out:18746=1199766/64 [rec/s]\n","2021-11-09 22:38:20,174 INFO mapred.LocalJobRunner: Records R/W=1110272/1110055 > map\n","2021-11-09 22:38:23,968 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:38:23,968 INFO mapred.MapTask: bufstart = 8697357; bufend = 88343637; bufvoid = 104857600\n","2021-11-09 22:38:23,968 INFO mapred.MapTask: kvstart = 2174332(8697328); kvend = 1114368(4457472); length = 1059965/6553600\n","2021-11-09 22:38:23,968 INFO mapred.MapTask: (EQUATOR) 89379253 kvi 22344808(89379232)\n","2021-11-09 22:38:24,366 INFO streaming.PipeMapRed: Records R/W=1298512/1298259\n","2021-11-09 22:38:24,481 INFO streaming.PipeMapRed: R/W/S=1300000/1299840/0 in:18571=1300000/70 [rec/s] out:18569=1299840/70 [rec/s]\n","2021-11-09 22:38:24,975 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:38:24,976 INFO mapred.MapTask: (RESET) equator 89379253 kv 22344808(89379232) kvi 22296716(89186864)\n","2021-11-09 22:38:26,174 INFO mapred.LocalJobRunner: Records R/W=1298512/1298259 > map\n","2021-11-09 22:38:27,796 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:38:27,797 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:38:27,797 INFO mapred.LocalJobRunner: Records R/W=1298512/1298259 > map\n","2021-11-09 22:38:27,798 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:38:27,798 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:38:27,798 INFO mapred.MapTask: bufstart = 89379253; bufend = 4294379; bufvoid = 104857452\n","2021-11-09 22:38:27,798 INFO mapred.MapTask: kvstart = 22344808(89379232); kvend = 22070000(88280000); length = 274809/6553600\n","2021-11-09 22:38:27,939 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:38:27,941 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:38:27,943 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 423096257 bytes\n","2021-11-09 22:38:30,204 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000022_0 is done. And is in the process of committing\n","2021-11-09 22:38:30,210 INFO mapred.LocalJobRunner: Records R/W=1298512/1298259 > sort\n","2021-11-09 22:38:30,211 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000022_0' done.\n","2021-11-09 22:38:30,211 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000022_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=9686745271\n","\t\tFILE: Number of bytes written=19373943264\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3092770816\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=49\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1362026\n","\t\tMap output records=1362026\n","\t\tMap output bytes=418509600\n","\t\tMap output materialized bytes=423098861\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2724052\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=6\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134507520\n","2021-11-09 22:38:30,211 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000022_0\n","2021-11-09 22:38:30,211 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000023_0\n","2021-11-09 22:38:30,213 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:38:30,213 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:38:30,213 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:38:30,214 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:3087007744+134217728\n","2021-11-09 22:38:30,262 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:38:30,277 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:38:30,277 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:38:30,277 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:38:30,277 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:38:30,277 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:38:30,278 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:38:30,286 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:38:30,313 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:38:30,314 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:38:30,318 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:38:30,376 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:38:30,380 INFO streaming.PipeMapRed: Records R/W=227/1\n","2021-11-09 22:38:30,424 INFO streaming.PipeMapRed: R/W/S=1000/888/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:38:30,881 INFO streaming.PipeMapRed: R/W/S=10000/9830/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:38:35,553 INFO streaming.PipeMapRed: R/W/S=100000/99746/0 in:20000=100000/5 [rec/s] out:19949=99746/5 [rec/s]\n","2021-11-09 22:38:40,395 INFO streaming.PipeMapRed: Records R/W=197249/197007\n","2021-11-09 22:38:40,516 INFO streaming.PipeMapRed: R/W/S=200000/199820/0 in:20000=200000/10 [rec/s] out:19982=199820/10 [rec/s]\n","2021-11-09 22:38:42,216 INFO mapred.LocalJobRunner: Records R/W=197249/197007 > map\n","2021-11-09 22:38:42,380 INFO mapreduce.Job:  map 56% reduce 0%\n","2021-11-09 22:38:44,539 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:38:44,539 INFO mapred.MapTask: bufstart = 0; bufend = 79425799; bufvoid = 104857600\n","2021-11-09 22:38:44,539 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25099276(100397104); length = 1115121/6553600\n","2021-11-09 22:38:44,539 INFO mapred.MapTask: (EQUATOR) 80544263 kvi 20136060(80544240)\n","2021-11-09 22:38:45,583 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:38:45,591 INFO mapred.MapTask: (RESET) equator 80544263 kv 20136060(80544240) kvi 20090160(80360640)\n","2021-11-09 22:38:46,179 INFO streaming.PipeMapRed: R/W/S=300000/299895/0 in:20000=300000/15 [rec/s] out:19993=299895/15 [rec/s]\n","2021-11-09 22:38:48,217 INFO mapred.LocalJobRunner: Records R/W=197249/197007 > map\n","2021-11-09 22:38:50,396 INFO streaming.PipeMapRed: Records R/W=381040/380903\n","2021-11-09 22:38:51,328 INFO streaming.PipeMapRed: R/W/S=400000/399822/0 in:19047=400000/21 [rec/s] out:19039=399822/21 [rec/s]\n","2021-11-09 22:38:54,218 INFO mapred.LocalJobRunner: Records R/W=381040/380903 > map\n","2021-11-09 22:38:54,384 INFO mapreduce.Job:  map 57% reduce 0%\n","2021-11-09 22:38:56,410 INFO streaming.PipeMapRed: R/W/S=500000/499861/0 in:19230=500000/26 [rec/s] out:19225=499861/26 [rec/s]\n","2021-11-09 22:38:59,040 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:38:59,040 INFO mapred.MapTask: bufstart = 80544263; bufend = 55298535; bufvoid = 104857116\n","2021-11-09 22:38:59,040 INFO mapred.MapTask: kvstart = 20136060(80544240); kvend = 19067456(76269824); length = 1068605/6553600\n","2021-11-09 22:38:59,040 INFO mapred.MapTask: (EQUATOR) 56391495 kvi 14097868(56391472)\n","2021-11-09 22:39:00,106 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:39:00,110 INFO mapred.MapTask: (RESET) equator 56391495 kv 14097868(56391472) kvi 14050560(56202240)\n","2021-11-09 22:39:00,218 INFO mapred.LocalJobRunner: Records R/W=381040/380903 > map\n","2021-11-09 22:39:00,397 INFO streaming.PipeMapRed: Records R/W=563327/563072\n","2021-11-09 22:39:02,471 INFO streaming.PipeMapRed: R/W/S=600000/599809/0 in:18750=600000/32 [rec/s] out:18744=599809/32 [rec/s]\n","2021-11-09 22:39:06,219 INFO mapred.LocalJobRunner: Records R/W=563327/563072 > map\n","2021-11-09 22:39:08,089 INFO streaming.PipeMapRed: R/W/S=700000/699788/0 in:18918=700000/37 [rec/s] out:18913=699788/37 [rec/s]\n","2021-11-09 22:39:10,398 INFO streaming.PipeMapRed: Records R/W=742726/742591\n","2021-11-09 22:39:12,219 INFO mapred.LocalJobRunner: Records R/W=742726/742591 > map\n","2021-11-09 22:39:13,684 INFO streaming.PipeMapRed: R/W/S=800000/799906/0 in:18604=800000/43 [rec/s] out:18602=799906/43 [rec/s]\n","2021-11-09 22:39:13,778 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:39:13,778 INFO mapred.MapTask: bufstart = 56391495; bufend = 31325449; bufvoid = 104857476\n","2021-11-09 22:39:13,778 INFO mapred.MapTask: kvstart = 14097868(56391472); kvend = 13074220(52296880); length = 1023649/6553600\n","2021-11-09 22:39:13,778 INFO mapred.MapTask: (EQUATOR) 32397465 kvi 8099360(32397440)\n","2021-11-09 22:39:14,813 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:39:14,814 INFO mapred.MapTask: (RESET) equator 32397465 kv 8099360(32397440) kvi 8052028(32208112)\n","2021-11-09 22:39:18,220 INFO mapred.LocalJobRunner: Records R/W=742726/742591 > map\n","2021-11-09 22:39:19,685 INFO streaming.PipeMapRed: R/W/S=900000/899909/0 in:18367=900000/49 [rec/s] out:18365=899909/49 [rec/s]\n","2021-11-09 22:39:20,399 INFO streaming.PipeMapRed: Records R/W=913894/913763\n","2021-11-09 22:39:24,220 INFO mapred.LocalJobRunner: Records R/W=913894/913763 > map\n","2021-11-09 22:39:25,157 INFO streaming.PipeMapRed: R/W/S=1000000/999886/0 in:18518=1000000/54 [rec/s] out:18516=999886/54 [rec/s]\n","2021-11-09 22:39:27,726 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:39:27,727 INFO mapred.MapTask: bufstart = 32397465; bufend = 7506637; bufvoid = 104857558\n","2021-11-09 22:39:27,727 INFO mapred.MapTask: kvstart = 8099360(32397440); kvend = 7119500(28478000); length = 979861/6553600\n","2021-11-09 22:39:27,727 INFO mapred.MapTask: (EQUATOR) 8555197 kvi 2138792(8555168)\n","2021-11-09 22:39:28,683 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:39:28,685 INFO mapred.MapTask: (RESET) equator 8555197 kv 2138792(8555168) kvi 2098296(8393184)\n","2021-11-09 22:39:30,222 INFO mapred.LocalJobRunner: Records R/W=913894/913763 > map\n","2021-11-09 22:39:30,404 INFO streaming.PipeMapRed: Records R/W=1088981/1088745\n","2021-11-09 22:39:31,086 INFO streaming.PipeMapRed: R/W/S=1100000/1099881/0 in:18333=1100000/60 [rec/s] out:18331=1099881/60 [rec/s]\n","2021-11-09 22:39:36,222 INFO mapred.LocalJobRunner: Records R/W=1088981/1088745 > map\n","2021-11-09 22:39:36,396 INFO mapreduce.Job:  map 58% reduce 0%\n","2021-11-09 22:39:36,508 INFO streaming.PipeMapRed: R/W/S=1200000/1199864/0 in:18181=1200000/66 [rec/s] out:18179=1199864/66 [rec/s]\n","2021-11-09 22:39:40,405 INFO streaming.PipeMapRed: Records R/W=1269253/1269122\n","2021-11-09 22:39:41,692 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:39:41,693 INFO mapred.MapTask: bufstart = 8555197; bufend = 88498694; bufvoid = 104857600\n","2021-11-09 22:39:41,693 INFO mapred.MapTask: kvstart = 2138792(8555168); kvend = 1153148(4612592); length = 985645/6553600\n","2021-11-09 22:39:41,693 INFO mapred.MapTask: (EQUATOR) 89534310 kvi 22383572(89534288)\n","2021-11-09 22:39:42,223 INFO mapred.LocalJobRunner: Records R/W=1269253/1269122 > map\n","2021-11-09 22:39:42,278 INFO streaming.PipeMapRed: R/W/S=1300000/1299967/0 in:18309=1300000/71 [rec/s] out:18309=1299967/71 [rec/s]\n","2021-11-09 22:39:42,641 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:39:42,649 INFO mapred.MapTask: (RESET) equator 89534310 kv 22383572(89534288) kvi 22339888(89359552)\n","2021-11-09 22:39:45,722 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:39:45,722 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:39:45,725 INFO mapred.LocalJobRunner: Records R/W=1269253/1269122 > map\n","2021-11-09 22:39:45,725 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:39:45,725 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:39:45,725 INFO mapred.MapTask: bufstart = 89534310; bufend = 5728851; bufvoid = 104857443\n","2021-11-09 22:39:45,725 INFO mapred.MapTask: kvstart = 22383572(89534288); kvend = 22120080(88480320); length = 263493/6553600\n","2021-11-09 22:39:45,866 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:39:45,867 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:39:45,868 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 424378855 bytes\n","2021-11-09 22:39:48,149 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000023_0 is done. And is in the process of committing\n","2021-11-09 22:39:48,161 INFO mapred.LocalJobRunner: Records R/W=1269253/1269122 > sort\n","2021-11-09 22:39:48,161 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000023_0' done.\n","2021-11-09 22:39:48,161 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000023_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=10111128439\n","\t\tFILE: Number of bytes written=20222705346\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3227278336\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=51\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1359098\n","\t\tMap output records=1359098\n","\t\tMap output bytes=419790828\n","\t\tMap output materialized bytes=424381010\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2718196\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134507520\n","2021-11-09 22:39:48,161 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000023_0\n","2021-11-09 22:39:48,161 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000024_0\n","2021-11-09 22:39:48,164 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:39:48,164 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:39:48,164 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:39:48,164 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:3221225472+134217728\n","2021-11-09 22:39:48,204 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:39:48,217 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:39:48,217 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:39:48,217 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:39:48,217 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:39:48,217 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:39:48,218 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:39:48,226 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:39:48,268 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:39:48,269 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:39:48,272 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:39:48,323 INFO streaming.PipeMapRed: Records R/W=220/1\n","2021-11-09 22:39:48,352 INFO streaming.PipeMapRed: R/W/S=1000/787/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:39:48,401 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:39:48,832 INFO streaming.PipeMapRed: R/W/S=10000/9798/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:39:53,645 INFO streaming.PipeMapRed: R/W/S=100000/99845/0 in:20000=100000/5 [rec/s] out:19969=99845/5 [rec/s]\n","2021-11-09 22:39:58,327 INFO streaming.PipeMapRed: Records R/W=187470/187218\n","2021-11-09 22:39:58,997 INFO streaming.PipeMapRed: R/W/S=200000/199855/0 in:20000=200000/10 [rec/s] out:19985=199855/10 [rec/s]\n","2021-11-09 22:40:00,166 INFO mapred.LocalJobRunner: Records R/W=187470/187218 > map\n","2021-11-09 22:40:00,404 INFO mapreduce.Job:  map 59% reduce 0%\n","2021-11-09 22:40:01,637 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:40:01,637 INFO mapred.MapTask: bufstart = 0; bufend = 79864905; bufvoid = 104857600\n","2021-11-09 22:40:01,637 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25209068(100836272); length = 1005329/6553600\n","2021-11-09 22:40:01,637 INFO mapred.MapTask: (EQUATOR) 80872537 kvi 20218128(80872512)\n","2021-11-09 22:40:02,532 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:40:02,533 INFO mapred.MapTask: (RESET) equator 80872537 kv 20218128(80872512) kvi 20169984(80679936)\n","2021-11-09 22:40:04,526 INFO streaming.PipeMapRed: R/W/S=300000/299836/0 in:18750=300000/16 [rec/s] out:18739=299836/16 [rec/s]\n","2021-11-09 22:40:06,167 INFO mapred.LocalJobRunner: Records R/W=187470/187218 > map\n","2021-11-09 22:40:08,332 INFO streaming.PipeMapRed: Records R/W=372359/372126\n","2021-11-09 22:40:09,824 INFO streaming.PipeMapRed: R/W/S=400000/399901/0 in:19047=400000/21 [rec/s] out:19042=399901/21 [rec/s]\n","2021-11-09 22:40:12,167 INFO mapred.LocalJobRunner: Records R/W=372359/372126 > map\n","2021-11-09 22:40:14,990 INFO streaming.PipeMapRed: R/W/S=500000/499794/0 in:19230=500000/26 [rec/s] out:19222=499794/26 [rec/s]\n","2021-11-09 22:40:15,351 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:40:15,351 INFO mapred.MapTask: bufstart = 80872537; bufend = 55812813; bufvoid = 104857553\n","2021-11-09 22:40:15,351 INFO mapred.MapTask: kvstart = 20218128(80872512); kvend = 19196064(76784256); length = 1022065/6553600\n","2021-11-09 22:40:15,351 INFO mapred.MapTask: (EQUATOR) 56826525 kvi 14206624(56826496)\n","2021-11-09 22:40:16,300 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:40:16,327 INFO mapred.MapTask: (RESET) equator 56826525 kv 14206624(56826496) kvi 14158932(56635728)\n","2021-11-09 22:40:18,168 INFO mapred.LocalJobRunner: Records R/W=372359/372126 > map\n","2021-11-09 22:40:18,335 INFO streaming.PipeMapRed: Records R/W=555545/555335\n","2021-11-09 22:40:20,691 INFO streaming.PipeMapRed: R/W/S=600000/599863/0 in:18750=600000/32 [rec/s] out:18745=599863/32 [rec/s]\n","2021-11-09 22:40:24,169 INFO mapred.LocalJobRunner: Records R/W=555545/555335 > map\n","2021-11-09 22:40:26,188 INFO streaming.PipeMapRed: R/W/S=700000/699757/0 in:18918=700000/37 [rec/s] out:18912=699757/37 [rec/s]\n","2021-11-09 22:40:28,343 INFO streaming.PipeMapRed: Records R/W=743062/742814\n","2021-11-09 22:40:29,567 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:40:29,567 INFO mapred.MapTask: bufstart = 56826525; bufend = 31677482; bufvoid = 104857353\n","2021-11-09 22:40:29,567 INFO mapred.MapTask: kvstart = 14206624(56826496); kvend = 13162240(52648960); length = 1044385/6553600\n","2021-11-09 22:40:29,567 INFO mapred.MapTask: (EQUATOR) 32703594 kvi 8175892(32703568)\n","2021-11-09 22:40:30,169 INFO mapred.LocalJobRunner: Records R/W=743062/742814 > map\n","2021-11-09 22:40:30,572 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:40:30,575 INFO mapred.MapTask: (RESET) equator 32703594 kv 8175892(32703568) kvi 8126232(32504928)\n","2021-11-09 22:40:31,646 INFO streaming.PipeMapRed: R/W/S=800000/799889/0 in:18604=800000/43 [rec/s] out:18602=799889/43 [rec/s]\n","2021-11-09 22:40:36,170 INFO mapred.LocalJobRunner: Records R/W=743062/742814 > map\n","2021-11-09 22:40:36,418 INFO mapreduce.Job:  map 60% reduce 0%\n","2021-11-09 22:40:36,760 INFO streaming.PipeMapRed: R/W/S=900000/899843/0 in:18750=900000/48 [rec/s] out:18746=899843/48 [rec/s]\n","2021-11-09 22:40:38,345 INFO streaming.PipeMapRed: Records R/W=930495/930227\n","2021-11-09 22:40:41,892 INFO streaming.PipeMapRed: R/W/S=1000000/999848/0 in:18867=1000000/53 [rec/s] out:18865=999848/53 [rec/s]\n","2021-11-09 22:40:42,170 INFO mapred.LocalJobRunner: Records R/W=930495/930227 > map\n","2021-11-09 22:40:43,640 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:40:43,640 INFO mapred.MapTask: bufstart = 32703594; bufend = 7484780; bufvoid = 104857498\n","2021-11-09 22:40:43,640 INFO mapred.MapTask: kvstart = 8175892(32703568); kvend = 7114064(28456256); length = 1061829/6553600\n","2021-11-09 22:40:43,640 INFO mapred.MapTask: (EQUATOR) 8520396 kvi 2130092(8520368)\n","2021-11-09 22:40:44,655 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:40:44,658 INFO mapred.MapTask: (RESET) equator 8520396 kv 2130092(8520368) kvi 2080656(8322624)\n","2021-11-09 22:40:47,276 INFO streaming.PipeMapRed: R/W/S=1100000/1099821/0 in:18644=1100000/59 [rec/s] out:18641=1099821/59 [rec/s]\n","2021-11-09 22:40:48,171 INFO mapred.LocalJobRunner: Records R/W=930495/930227 > map\n","2021-11-09 22:40:48,346 INFO streaming.PipeMapRed: Records R/W=1122501/1122408\n","2021-11-09 22:40:52,501 INFO streaming.PipeMapRed: R/W/S=1200000/1199783/0 in:18750=1200000/64 [rec/s] out:18746=1199783/64 [rec/s]\n","2021-11-09 22:40:54,171 INFO mapred.LocalJobRunner: Records R/W=1122501/1122408 > map\n","2021-11-09 22:40:57,176 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:40:57,176 INFO mapred.MapTask: bufstart = 8520396; bufend = 88367488; bufvoid = 104857600\n","2021-11-09 22:40:57,176 INFO mapred.MapTask: kvstart = 2130092(8520368); kvend = 1120180(4480720); length = 1009913/6553600\n","2021-11-09 22:40:57,176 INFO mapred.MapTask: (EQUATOR) 89396720 kvi 22349176(89396704)\n","2021-11-09 22:40:58,145 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:40:58,150 INFO mapred.MapTask: (RESET) equator 89396720 kv 22349176(89396704) kvi 22304696(89218784)\n","2021-11-09 22:40:58,273 INFO streaming.PipeMapRed: R/W/S=1300000/1299786/0 in:18571=1300000/70 [rec/s] out:18568=1299786/70 [rec/s]\n","2021-11-09 22:40:58,365 INFO streaming.PipeMapRed: Records R/W=1301809/1301558\n","2021-11-09 22:41:00,172 INFO mapred.LocalJobRunner: Records R/W=1301809/1301558 > map\n","2021-11-09 22:41:00,776 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:41:00,776 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:41:00,777 INFO mapred.LocalJobRunner: Records R/W=1301809/1301558 > map\n","2021-11-09 22:41:00,777 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:41:00,777 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:41:00,777 INFO mapred.MapTask: bufstart = 89396720; bufend = 4418252; bufvoid = 104857456\n","2021-11-09 22:41:00,777 INFO mapred.MapTask: kvstart = 22349176(89396704); kvend = 22106324(88425296); length = 242853/6553600\n","2021-11-09 22:41:00,901 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:41:00,903 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:41:00,903 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 423284445 bytes\n","2021-11-09 22:41:03,188 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000024_0 is done. And is in the process of committing\n","2021-11-09 22:41:03,195 INFO mapred.LocalJobRunner: Records R/W=1301809/1301558 > sort\n","2021-11-09 22:41:03,195 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000024_0' done.\n","2021-11-09 22:41:03,195 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000024_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=10534418113\n","\t\tFILE: Number of bytes written=21069280440\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3361795072\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=53\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1346598\n","\t\tMap output records=1346598\n","\t\tMap output bytes=418735808\n","\t\tMap output materialized bytes=423287516\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2693196\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=4\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134516736\n","2021-11-09 22:41:03,195 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000024_0\n","2021-11-09 22:41:03,195 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000025_0\n","2021-11-09 22:41:03,198 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:41:03,198 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:41:03,198 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:41:03,199 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:3355443200+134217728\n","2021-11-09 22:41:03,238 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:41:03,254 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:41:03,254 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:41:03,254 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:41:03,254 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:41:03,254 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:41:03,255 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:41:03,261 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:41:03,300 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:41:03,300 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:41:03,310 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:41:03,353 INFO streaming.PipeMapRed: Records R/W=234/1\n","2021-11-09 22:41:03,385 INFO streaming.PipeMapRed: R/W/S=1000/883/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:41:03,426 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:41:03,890 INFO streaming.PipeMapRed: R/W/S=10000/9845/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:41:08,737 INFO streaming.PipeMapRed: R/W/S=100000/99875/0 in:20000=100000/5 [rec/s] out:19975=99875/5 [rec/s]\n","2021-11-09 22:41:13,357 INFO streaming.PipeMapRed: Records R/W=186296/186065\n","2021-11-09 22:41:14,150 INFO streaming.PipeMapRed: R/W/S=200000/199901/0 in:20000=200000/10 [rec/s] out:19990=199901/10 [rec/s]\n","2021-11-09 22:41:15,204 INFO mapred.LocalJobRunner: Records R/W=186296/186065 > map\n","2021-11-09 22:41:15,432 INFO mapreduce.Job:  map 61% reduce 0%\n","2021-11-09 22:41:16,809 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:41:16,809 INFO mapred.MapTask: bufstart = 0; bufend = 79929662; bufvoid = 104857600\n","2021-11-09 22:41:16,809 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25225288(100901152); length = 989109/6553600\n","2021-11-09 22:41:16,809 INFO mapred.MapTask: (EQUATOR) 80919454 kvi 20229856(80919424)\n","2021-11-09 22:41:17,785 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:41:17,792 INFO mapred.MapTask: (RESET) equator 80919454 kv 20229856(80919424) kvi 20185760(80743040)\n","2021-11-09 22:41:20,023 INFO streaming.PipeMapRed: R/W/S=300000/299918/0 in:18750=300000/16 [rec/s] out:18744=299918/16 [rec/s]\n","2021-11-09 22:41:21,205 INFO mapred.LocalJobRunner: Records R/W=186296/186065 > map\n","2021-11-09 22:41:23,358 INFO streaming.PipeMapRed: Records R/W=361151/360997\n","2021-11-09 22:41:25,500 INFO streaming.PipeMapRed: R/W/S=400000/399913/0 in:18181=400000/22 [rec/s] out:18177=399913/22 [rec/s]\n","2021-11-09 22:41:27,205 INFO mapred.LocalJobRunner: Records R/W=361151/360997 > map\n","2021-11-09 22:41:27,435 INFO mapreduce.Job:  map 62% reduce 0%\n","2021-11-09 22:41:30,597 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:41:30,597 INFO mapred.MapTask: bufstart = 80919454; bufend = 56051937; bufvoid = 104857435\n","2021-11-09 22:41:30,597 INFO mapred.MapTask: kvstart = 20229856(80919424); kvend = 19255836(77023344); length = 974021/6553600\n","2021-11-09 22:41:30,597 INFO mapred.MapTask: (EQUATOR) 57035921 kvi 14258976(57035904)\n","2021-11-09 22:41:31,425 INFO streaming.PipeMapRed: R/W/S=500000/499907/0 in:17857=500000/28 [rec/s] out:17853=499907/28 [rec/s]\n","2021-11-09 22:41:31,541 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:41:31,548 INFO mapred.MapTask: (RESET) equator 57035921 kv 14258976(57035904) kvi 14217316(56869264)\n","2021-11-09 22:41:33,206 INFO mapred.LocalJobRunner: Records R/W=361151/360997 > map\n","2021-11-09 22:41:33,382 INFO streaming.PipeMapRed: Records R/W=533705/533464\n","2021-11-09 22:41:36,874 INFO streaming.PipeMapRed: R/W/S=600000/599807/0 in:18181=600000/33 [rec/s] out:18175=599807/33 [rec/s]\n","2021-11-09 22:41:39,207 INFO mapred.LocalJobRunner: Records R/W=533705/533464 > map\n","2021-11-09 22:41:42,291 INFO streaming.PipeMapRed: R/W/S=700000/699889/0 in:17948=700000/39 [rec/s] out:17945=699889/39 [rec/s]\n","2021-11-09 22:41:43,398 INFO streaming.PipeMapRed: Records R/W=721450/721212\n","2021-11-09 22:41:44,236 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:41:44,236 INFO mapred.MapTask: bufstart = 57035921; bufend = 32131806; bufvoid = 104857281\n","2021-11-09 22:41:44,236 INFO mapred.MapTask: kvstart = 14258976(57035904); kvend = 13275492(53101968); length = 983485/6553600\n","2021-11-09 22:41:44,236 INFO mapred.MapTask: (EQUATOR) 33115742 kvi 8278928(33115712)\n","2021-11-09 22:41:45,207 INFO mapred.LocalJobRunner: Records R/W=721450/721212 > map\n","2021-11-09 22:41:45,210 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:41:45,214 INFO mapred.MapTask: (RESET) equator 33115742 kv 8278928(33115712) kvi 8237692(32950768)\n","2021-11-09 22:41:47,996 INFO streaming.PipeMapRed: R/W/S=800000/799874/0 in:18181=800000/44 [rec/s] out:18178=799874/44 [rec/s]\n","2021-11-09 22:41:51,208 INFO mapred.LocalJobRunner: Records R/W=721450/721212 > map\n","2021-11-09 22:41:53,384 INFO streaming.PipeMapRed: R/W/S=900000/899787/0 in:18000=900000/50 [rec/s] out:17995=899787/50 [rec/s]\n","2021-11-09 22:41:53,399 INFO streaming.PipeMapRed: Records R/W=900254/900127\n","2021-11-09 22:41:57,208 INFO mapred.LocalJobRunner: Records R/W=900254/900127 > map\n","2021-11-09 22:41:58,128 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:41:58,128 INFO mapred.MapTask: bufstart = 33115742; bufend = 8133789; bufvoid = 104857495\n","2021-11-09 22:41:58,128 INFO mapred.MapTask: kvstart = 8278928(33115712); kvend = 7276296(29105184); length = 1002633/6553600\n","2021-11-09 22:41:58,128 INFO mapred.MapTask: (EQUATOR) 9123581 kvi 2280888(9123552)\n","2021-11-09 22:41:59,161 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:41:59,162 INFO mapred.MapTask: (RESET) equator 9123581 kv 2280888(9123552) kvi 2231084(8924336)\n","2021-11-09 22:41:59,166 INFO streaming.PipeMapRed: R/W/S=1000000/999889/0 in:18181=1000000/55 [rec/s] out:18179=999889/55 [rec/s]\n","2021-11-09 22:42:03,213 INFO mapred.LocalJobRunner: Records R/W=900254/900127 > map\n","2021-11-09 22:42:03,418 INFO streaming.PipeMapRed: Records R/W=1081877/1081629\n","2021-11-09 22:42:04,375 INFO streaming.PipeMapRed: R/W/S=1100000/1099892/0 in:18032=1100000/61 [rec/s] out:18031=1099892/61 [rec/s]\n","2021-11-09 22:42:09,213 INFO mapred.LocalJobRunner: Records R/W=1081877/1081629 > map\n","2021-11-09 22:42:09,603 INFO streaming.PipeMapRed: R/W/S=1200000/1199874/0 in:18181=1200000/66 [rec/s] out:18179=1199874/66 [rec/s]\n","2021-11-09 22:42:11,896 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:42:11,896 INFO mapred.MapTask: bufstart = 9123581; bufend = 88926940; bufvoid = 104857600\n","2021-11-09 22:42:11,896 INFO mapred.MapTask: kvstart = 2280888(9123552); kvend = 1260180(5040720); length = 1020709/6553600\n","2021-11-09 22:42:11,896 INFO mapred.MapTask: (EQUATOR) 89922604 kvi 22480644(89922576)\n","2021-11-09 22:42:12,878 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:42:12,879 INFO mapred.MapTask: (RESET) equator 89922604 kv 22480644(89922576) kvi 22433144(89732576)\n","2021-11-09 22:42:13,418 INFO streaming.PipeMapRed: Records R/W=1264996/1264857\n","2021-11-09 22:42:14,608 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:42:14,608 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:42:14,611 INFO mapred.LocalJobRunner: Records R/W=1081877/1081629 > map\n","2021-11-09 22:42:14,611 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:42:14,611 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:42:14,611 INFO mapred.MapTask: bufstart = 89922604; bufend = 104009622; bufvoid = 104857600\n","2021-11-09 22:42:14,611 INFO mapred.MapTask: kvstart = 22480644(89922576); kvend = 22293412(89173648); length = 187233/6553600\n","2021-11-09 22:42:14,701 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:42:14,702 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:42:14,704 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 418032683 bytes\n","2021-11-09 22:42:15,214 INFO mapred.LocalJobRunner: Records R/W=1264996/1264857 > sort > \n","2021-11-09 22:42:15,452 INFO mapreduce.Job:  map 63% reduce 0%\n","2021-11-09 22:42:16,979 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000025_0 is done. And is in the process of committing\n","2021-11-09 22:42:16,986 INFO mapred.LocalJobRunner: Records R/W=1264996/1264857 > sort\n","2021-11-09 22:42:16,986 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000025_0' done.\n","2021-11-09 22:42:16,986 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000025_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=10952456605\n","\t\tFILE: Number of bytes written=21905353170\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3496225792\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=55\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1289302\n","\t\tMap output records=1289302\n","\t\tMap output bytes=413638665\n","\t\tMap output materialized bytes=418036334\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2578604\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134430720\n","2021-11-09 22:42:16,986 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000025_0\n","2021-11-09 22:42:16,987 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000026_0\n","2021-11-09 22:42:16,992 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:42:16,992 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:42:16,993 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:42:16,993 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:3489660928+134217728\n","2021-11-09 22:42:17,028 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:42:17,046 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:42:17,046 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:42:17,046 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:42:17,046 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:42:17,046 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:42:17,047 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:42:17,053 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:42:17,078 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:42:17,079 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:42:17,083 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:42:17,151 INFO streaming.PipeMapRed: Records R/W=218/1\n","2021-11-09 22:42:17,184 INFO streaming.PipeMapRed: R/W/S=1000/832/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:42:17,452 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:42:17,729 INFO streaming.PipeMapRed: R/W/S=10000/9771/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:42:22,365 INFO streaming.PipeMapRed: R/W/S=100000/99825/0 in:20000=100000/5 [rec/s] out:19965=99825/5 [rec/s]\n","2021-11-09 22:42:27,152 INFO streaming.PipeMapRed: Records R/W=192169/191937\n","2021-11-09 22:42:27,589 INFO streaming.PipeMapRed: R/W/S=200000/199828/0 in:20000=200000/10 [rec/s] out:19982=199828/10 [rec/s]\n","2021-11-09 22:42:28,995 INFO mapred.LocalJobRunner: Records R/W=192169/191937 > map\n","2021-11-09 22:42:29,456 INFO mapreduce.Job:  map 64% reduce 0%\n","2021-11-09 22:42:31,101 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:42:31,101 INFO mapred.MapTask: bufstart = 0; bufend = 79620195; bufvoid = 104857600\n","2021-11-09 22:42:31,101 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25147872(100591488); length = 1066525/6553600\n","2021-11-09 22:42:31,101 INFO mapred.MapTask: (EQUATOR) 80688787 kvi 20172192(80688768)\n","2021-11-09 22:42:32,056 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:42:32,066 INFO mapred.MapTask: (RESET) equator 80688787 kv 20172192(80688768) kvi 20126800(80507200)\n","2021-11-09 22:42:33,262 INFO streaming.PipeMapRed: R/W/S=300000/299812/0 in:18750=300000/16 [rec/s] out:18738=299812/16 [rec/s]\n","2021-11-09 22:42:34,995 INFO mapred.LocalJobRunner: Records R/W=192169/191937 > map\n","2021-11-09 22:42:37,153 INFO streaming.PipeMapRed: Records R/W=363620/363540\n","2021-11-09 22:42:40,997 INFO mapred.LocalJobRunner: Records R/W=363620/363540 > map\n","2021-11-09 22:42:43,734 INFO streaming.PipeMapRed: R/W/S=400000/399859/0 in:15384=400000/26 [rec/s] out:15379=399859/26 [rec/s]\n","2021-11-09 22:42:46,997 INFO mapred.LocalJobRunner: Records R/W=363620/363540 > map\n","2021-11-09 22:42:47,154 INFO streaming.PipeMapRed: Records R/W=441575/441394\n","2021-11-09 22:42:50,552 INFO streaming.PipeMapRed: R/W/S=500000/499875/0 in:15151=500000/33 [rec/s] out:15147=499875/33 [rec/s]\n","2021-11-09 22:42:51,592 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:42:51,592 INFO mapred.MapTask: bufstart = 80688787; bufend = 55682857; bufvoid = 104851916\n","2021-11-09 22:42:51,592 INFO mapred.MapTask: kvstart = 20172192(80688768); kvend = 19163572(76654288); length = 1008621/6553600\n","2021-11-09 22:42:51,592 INFO mapred.MapTask: (EQUATOR) 56721673 kvi 14180412(56721648)\n","2021-11-09 22:42:52,817 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:42:52,821 INFO mapred.MapTask: (RESET) equator 56721673 kv 14180412(56721648) kvi 14129844(56519376)\n","2021-11-09 22:42:52,998 INFO mapred.LocalJobRunner: Records R/W=441575/441394 > map\n","2021-11-09 22:42:56,885 INFO streaming.PipeMapRed: R/W/S=600000/599841/0 in:15384=600000/39 [rec/s] out:15380=599841/39 [rec/s]\n","2021-11-09 22:42:57,156 INFO streaming.PipeMapRed: Records R/W=605515/605282\n","2021-11-09 22:42:58,998 INFO mapred.LocalJobRunner: Records R/W=605515/605282 > map\n","2021-11-09 22:43:02,736 INFO streaming.PipeMapRed: R/W/S=700000/699832/0 in:15555=700000/45 [rec/s] out:15551=699832/45 [rec/s]\n","2021-11-09 22:43:04,999 INFO mapred.LocalJobRunner: Records R/W=605515/605282 > map\n","2021-11-09 22:43:06,500 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:43:06,500 INFO mapred.MapTask: bufstart = 56721673; bufend = 31777601; bufvoid = 104856533\n","2021-11-09 22:43:06,500 INFO mapred.MapTask: kvstart = 14180412(56721648); kvend = 13187256(52749024); length = 993157/6553600\n","2021-11-09 22:43:06,500 INFO mapred.MapTask: (EQUATOR) 32800593 kvi 8200144(32800576)\n","2021-11-09 22:43:07,159 INFO streaming.PipeMapRed: Records R/W=774382/774278\n","2021-11-09 22:43:07,623 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:43:07,624 INFO mapred.MapTask: (RESET) equator 32800593 kv 8200144(32800576) kvi 8151188(32604752)\n","2021-11-09 22:43:08,777 INFO streaming.PipeMapRed: R/W/S=800000/799887/0 in:15686=800000/51 [rec/s] out:15684=799887/51 [rec/s]\n","2021-11-09 22:43:10,999 INFO mapred.LocalJobRunner: Records R/W=774382/774278 > map\n","2021-11-09 22:43:14,709 INFO streaming.PipeMapRed: R/W/S=900000/899902/0 in:15789=900000/57 [rec/s] out:15787=899903/57 [rec/s]\n","2021-11-09 22:43:17,000 INFO mapred.LocalJobRunner: Records R/W=774382/774278 > map\n","2021-11-09 22:43:17,171 INFO streaming.PipeMapRed: Records R/W=941838/941605\n","2021-11-09 22:43:17,475 INFO mapreduce.Job:  map 65% reduce 0%\n","2021-11-09 22:43:20,577 INFO streaming.PipeMapRed: R/W/S=1000000/999809/0 in:15873=1000000/63 [rec/s] out:15869=999809/63 [rec/s]\n","2021-11-09 22:43:21,272 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:43:21,272 INFO mapred.MapTask: bufstart = 32800593; bufend = 7925557; bufvoid = 104857284\n","2021-11-09 22:43:21,272 INFO mapred.MapTask: kvstart = 8200144(32800576); kvend = 7224200(28896800); length = 975945/6553600\n","2021-11-09 22:43:21,272 INFO mapred.MapTask: (EQUATOR) 8939269 kvi 2234812(8939248)\n","2021-11-09 22:43:22,355 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:43:22,356 INFO mapred.MapTask: (RESET) equator 8939269 kv 2234812(8939248) kvi 2186976(8747904)\n","2021-11-09 22:43:23,000 INFO mapred.LocalJobRunner: Records R/W=941838/941605 > map\n","2021-11-09 22:43:26,830 INFO streaming.PipeMapRed: R/W/S=1100000/1099805/0 in:15942=1100000/69 [rec/s] out:15939=1099805/69 [rec/s]\n","2021-11-09 22:43:27,172 INFO streaming.PipeMapRed: Records R/W=1106165/1106048\n","2021-11-09 22:43:29,001 INFO mapred.LocalJobRunner: Records R/W=1106165/1106048 > map\n","2021-11-09 22:43:32,847 INFO streaming.PipeMapRed: R/W/S=1200000/1199767/0 in:16000=1200000/75 [rec/s] out:15996=1199767/75 [rec/s]\n","2021-11-09 22:43:35,002 INFO mapred.LocalJobRunner: Records R/W=1106165/1106048 > map\n","2021-11-09 22:43:36,114 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:43:36,114 INFO mapred.MapTask: bufstart = 8939269; bufend = 88910437; bufvoid = 104857600\n","2021-11-09 22:43:36,114 INFO mapred.MapTask: kvstart = 2234812(8939248); kvend = 1256092(5024368); length = 978721/6553600\n","2021-11-09 22:43:36,114 INFO mapred.MapTask: (EQUATOR) 89915045 kvi 22478756(89915024)\n","2021-11-09 22:43:37,176 INFO streaming.PipeMapRed: Records R/W=1266735/1266470\n","2021-11-09 22:43:37,244 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:43:37,244 INFO mapred.MapTask: (RESET) equator 89915045 kv 22478756(89915024) kvi 22433712(89734848)\n","2021-11-09 22:43:39,216 INFO streaming.PipeMapRed: R/W/S=1300000/1299876/0 in:15853=1300000/82 [rec/s] out:15852=1299876/82 [rec/s]\n","2021-11-09 22:43:39,854 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:43:39,855 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:43:39,855 INFO mapred.LocalJobRunner: Records R/W=1106165/1106048 > map\n","2021-11-09 22:43:39,855 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:43:39,855 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:43:39,855 INFO mapred.MapTask: bufstart = 89915045; bufend = 2977755; bufvoid = 104857138\n","2021-11-09 22:43:39,855 INFO mapred.MapTask: kvstart = 22478756(89915024); kvend = 22258864(89035456); length = 219893/6553600\n","2021-11-09 22:43:39,987 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:43:39,989 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:43:39,990 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 421708728 bytes\n","2021-11-09 22:43:41,003 INFO mapred.LocalJobRunner: Records R/W=1266735/1266470 > sort > \n","2021-11-09 22:43:42,947 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000026_0 is done. And is in the process of committing\n","2021-11-09 22:43:42,959 INFO mapred.LocalJobRunner: Records R/W=1266735/1266470 > sort\n","2021-11-09 22:43:42,960 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000026_0' done.\n","2021-11-09 22:43:42,960 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000026_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=11374168564\n","\t\tFILE: Number of bytes written=22748772834\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3630623744\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=57\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1310720\n","\t\tMap output records=1310720\n","\t\tMap output bytes=417251906\n","\t\tMap output materialized bytes=421709801\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2621440\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=4\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134397952\n","2021-11-09 22:43:42,960 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000026_0\n","2021-11-09 22:43:42,960 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000027_0\n","2021-11-09 22:43:42,962 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:43:42,962 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:43:42,963 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:43:42,963 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:3623878656+134217728\n","2021-11-09 22:43:43,024 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:43:43,056 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:43:43,056 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:43:43,056 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:43:43,056 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:43:43,056 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:43:43,065 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:43:43,070 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:43:43,098 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:43:43,099 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:43:43,102 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:43:43,169 INFO streaming.PipeMapRed: Records R/W=226/1\n","2021-11-09 22:43:43,211 INFO streaming.PipeMapRed: R/W/S=1000/859/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:43:43,483 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:43:43,740 INFO streaming.PipeMapRed: R/W/S=10000/9821/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:43:48,587 INFO streaming.PipeMapRed: R/W/S=100000/99894/0 in:20000=100000/5 [rec/s] out:19978=99894/5 [rec/s]\n","2021-11-09 22:43:53,179 INFO streaming.PipeMapRed: Records R/W=181718/181465\n","2021-11-09 22:43:54,186 INFO streaming.PipeMapRed: R/W/S=200000/199835/0 in:18181=200000/11 [rec/s] out:18166=199835/11 [rec/s]\n","2021-11-09 22:43:54,964 INFO mapred.LocalJobRunner: Records R/W=181718/181465 > map\n","2021-11-09 22:43:55,487 INFO mapreduce.Job:  map 66% reduce 0%\n","2021-11-09 22:43:56,917 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:43:56,917 INFO mapred.MapTask: bufstart = 0; bufend = 79916304; bufvoid = 104857600\n","2021-11-09 22:43:56,917 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25221952(100887808); length = 992445/6553600\n","2021-11-09 22:43:56,917 INFO mapred.MapTask: (EQUATOR) 80909024 kvi 20227252(80909008)\n","2021-11-09 22:43:57,949 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:43:57,972 INFO mapred.MapTask: (RESET) equator 80909024 kv 20227252(80909008) kvi 20185752(80743008)\n","2021-11-09 22:44:00,432 INFO streaming.PipeMapRed: R/W/S=300000/299835/0 in:17647=300000/17 [rec/s] out:17637=299835/17 [rec/s]\n","2021-11-09 22:44:00,964 INFO mapred.LocalJobRunner: Records R/W=181718/181465 > map\n","2021-11-09 22:44:03,189 INFO streaming.PipeMapRed: Records R/W=347325/347090\n","2021-11-09 22:44:05,963 INFO streaming.PipeMapRed: R/W/S=400000/399819/0 in:18181=400000/22 [rec/s] out:18173=399819/22 [rec/s]\n","2021-11-09 22:44:06,965 INFO mapred.LocalJobRunner: Records R/W=347325/347090 > map\n","2021-11-09 22:44:11,197 INFO streaming.PipeMapRed: R/W/S=500000/499805/0 in:17857=500000/28 [rec/s] out:17850=499805/28 [rec/s]\n","2021-11-09 22:44:11,354 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:44:11,354 INFO mapred.MapTask: bufstart = 80909024; bufend = 55866744; bufvoid = 104857568\n","2021-11-09 22:44:11,354 INFO mapred.MapTask: kvstart = 20227252(80909008); kvend = 19209544(76838176); length = 1017709/6553600\n","2021-11-09 22:44:11,354 INFO mapred.MapTask: (EQUATOR) 56874376 kvi 14218588(56874352)\n","2021-11-09 22:44:12,324 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:44:12,324 INFO mapred.MapTask: (RESET) equator 56874376 kv 14218588(56874352) kvi 14172796(56691184)\n","2021-11-09 22:44:12,965 INFO mapred.LocalJobRunner: Records R/W=347325/347090 > map\n","2021-11-09 22:44:13,195 INFO streaming.PipeMapRed: Records R/W=530424/530260\n","2021-11-09 22:44:13,492 INFO mapreduce.Job:  map 67% reduce 0%\n","2021-11-09 22:44:16,891 INFO streaming.PipeMapRed: R/W/S=600000/599904/0 in:18181=600000/33 [rec/s] out:18178=599904/33 [rec/s]\n","2021-11-09 22:44:18,966 INFO mapred.LocalJobRunner: Records R/W=530424/530260 > map\n","2021-11-09 22:44:22,394 INFO streaming.PipeMapRed: R/W/S=700000/699875/0 in:17948=700000/39 [rec/s] out:17945=699875/39 [rec/s]\n","2021-11-09 22:44:23,200 INFO streaming.PipeMapRed: Records R/W=715809/715545\n","2021-11-09 22:44:24,966 INFO mapred.LocalJobRunner: Records R/W=715809/715545 > map\n","2021-11-09 22:44:25,804 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:44:25,804 INFO mapred.MapTask: bufstart = 56874376; bufend = 31692217; bufvoid = 104857329\n","2021-11-09 22:44:25,804 INFO mapred.MapTask: kvstart = 14218588(56874352); kvend = 13165920(52663680); length = 1052669/6553600\n","2021-11-09 22:44:25,804 INFO mapred.MapTask: (EQUATOR) 32715209 kvi 8178796(32715184)\n","2021-11-09 22:44:26,943 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:44:26,948 INFO mapred.MapTask: (RESET) equator 32715209 kv 8178796(32715184) kvi 8126008(32504032)\n","2021-11-09 22:44:28,119 INFO streaming.PipeMapRed: R/W/S=800000/799906/0 in:17777=800000/45 [rec/s] out:17775=799906/45 [rec/s]\n","2021-11-09 22:44:30,967 INFO mapred.LocalJobRunner: Records R/W=715809/715545 > map\n","2021-11-09 22:44:33,201 INFO streaming.PipeMapRed: Records R/W=888944/888710\n","2021-11-09 22:44:33,845 INFO streaming.PipeMapRed: R/W/S=900000/899824/0 in:18000=900000/50 [rec/s] out:17996=899824/50 [rec/s]\n","2021-11-09 22:44:36,967 INFO mapred.LocalJobRunner: Records R/W=888944/888710 > map\n","2021-11-09 22:44:38,939 INFO streaming.PipeMapRed: R/W/S=1000000/999853/0 in:18181=1000000/55 [rec/s] out:18179=999853/55 [rec/s]\n","2021-11-09 22:44:40,089 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:44:40,089 INFO mapred.MapTask: bufstart = 32715209; bufend = 7615055; bufvoid = 104857502\n","2021-11-09 22:44:40,089 INFO mapred.MapTask: kvstart = 8178796(32715184); kvend = 7146560(28586240); length = 1032237/6553600\n","2021-11-09 22:44:40,089 INFO mapred.MapTask: (EQUATOR) 8641167 kvi 2160284(8641136)\n","2021-11-09 22:44:41,194 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:44:41,194 INFO mapred.MapTask: (RESET) equator 8641167 kv 2160284(8641136) kvi 2108748(8434992)\n","2021-11-09 22:44:42,968 INFO mapred.LocalJobRunner: Records R/W=888944/888710 > map\n","2021-11-09 22:44:43,216 INFO streaming.PipeMapRed: Records R/W=1076356/1076099\n","2021-11-09 22:44:44,613 INFO streaming.PipeMapRed: R/W/S=1100000/1099906/0 in:18032=1100000/61 [rec/s] out:18031=1099906/61 [rec/s]\n","2021-11-09 22:44:48,968 INFO mapred.LocalJobRunner: Records R/W=1076356/1076099 > map\n","2021-11-09 22:44:50,157 INFO streaming.PipeMapRed: R/W/S=1200000/1199756/0 in:17910=1200000/67 [rec/s] out:17906=1199756/67 [rec/s]\n","2021-11-09 22:44:53,236 INFO streaming.PipeMapRed: Records R/W=1254334/1254089\n","2021-11-09 22:44:54,147 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:44:54,147 INFO mapred.MapTask: bufstart = 8641167; bufend = 88574549; bufvoid = 104857600\n","2021-11-09 22:44:54,147 INFO mapred.MapTask: kvstart = 2160284(8641136); kvend = 1172100(4688400); length = 988185/6553600\n","2021-11-09 22:44:54,147 INFO mapred.MapTask: (EQUATOR) 89591333 kvi 22397828(89591312)\n","2021-11-09 22:44:54,969 INFO mapred.LocalJobRunner: Records R/W=1254334/1254089 > map\n","2021-11-09 22:44:55,176 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:44:55,178 INFO mapred.MapTask: (RESET) equator 89591333 kv 22397828(89591312) kvi 22347524(89390096)\n","2021-11-09 22:44:56,013 INFO streaming.PipeMapRed: R/W/S=1300000/1299750/0 in:18055=1300000/72 [rec/s] out:18052=1299750/72 [rec/s]\n","2021-11-09 22:44:57,442 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:44:57,442 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:44:57,442 INFO mapred.LocalJobRunner: Records R/W=1254334/1254089 > map\n","2021-11-09 22:44:57,442 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:44:57,443 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:44:57,443 INFO mapred.MapTask: bufstart = 89591333; bufend = 2748511; bufvoid = 104857292\n","2021-11-09 22:44:57,443 INFO mapred.MapTask: kvstart = 22397828(89591312); kvend = 22174592(88698368); length = 223237/6553600\n","2021-11-09 22:44:57,570 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:44:57,571 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:44:57,572 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 421608102 bytes\n","2021-11-09 22:45:00,264 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000027_0 is done. And is in the process of committing\n","2021-11-09 22:45:00,268 INFO mapred.LocalJobRunner: Records R/W=1254334/1254089 > sort\n","2021-11-09 22:45:00,269 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000027_0' done.\n","2021-11-09 22:45:00,269 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000027_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=11795781393\n","\t\tFILE: Number of bytes written=23591994238\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3765140480\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=59\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1326625\n","\t\tMap output records=1326625\n","\t\tMap output bytes=417111962\n","\t\tMap output materialized bytes=421610671\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2653250\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=4\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134516736\n","2021-11-09 22:45:00,269 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000027_0\n","2021-11-09 22:45:00,269 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000028_0\n","2021-11-09 22:45:00,274 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:45:00,274 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:45:00,274 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:45:00,275 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:3758096384+134217728\n","2021-11-09 22:45:00,319 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:45:00,332 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:45:00,332 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:45:00,332 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:45:00,332 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:45:00,332 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:45:00,338 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:45:00,344 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:45:00,375 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:45:00,376 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:45:00,381 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:45:00,436 INFO streaming.PipeMapRed: Records R/W=207/1\n","2021-11-09 22:45:00,474 INFO streaming.PipeMapRed: R/W/S=1000/834/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:45:00,508 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:45:01,000 INFO streaming.PipeMapRed: R/W/S=10000/9874/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:45:05,721 INFO streaming.PipeMapRed: R/W/S=100000/99795/0 in:20000=100000/5 [rec/s] out:19959=99795/5 [rec/s]\n","2021-11-09 22:45:10,443 INFO streaming.PipeMapRed: Records R/W=183127/182883\n","2021-11-09 22:45:11,402 INFO streaming.PipeMapRed: R/W/S=200000/199894/0 in:18181=200000/11 [rec/s] out:18172=199894/11 [rec/s]\n","2021-11-09 22:45:12,283 INFO mapred.LocalJobRunner: Records R/W=183127/182883 > map\n","2021-11-09 22:45:12,514 INFO mapreduce.Job:  map 69% reduce 0%\n","2021-11-09 22:45:13,735 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:45:13,735 INFO mapred.MapTask: bufstart = 0; bufend = 79993546; bufvoid = 104857600\n","2021-11-09 22:45:13,735 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25241192(100964768); length = 973205/6553600\n","2021-11-09 22:45:13,735 INFO mapred.MapTask: (EQUATOR) 80968938 kvi 20242228(80968912)\n","2021-11-09 22:45:14,676 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:45:14,677 INFO mapred.MapTask: (RESET) equator 80968938 kv 20242228(80968912) kvi 20196700(80786800)\n","2021-11-09 22:45:17,143 INFO streaming.PipeMapRed: R/W/S=300000/299763/0 in:18750=300000/16 [rec/s] out:18735=299763/16 [rec/s]\n","2021-11-09 22:45:18,283 INFO mapred.LocalJobRunner: Records R/W=183127/182883 > map\n","2021-11-09 22:45:20,444 INFO streaming.PipeMapRed: Records R/W=359271/359106\n","2021-11-09 22:45:22,606 INFO streaming.PipeMapRed: R/W/S=400000/399790/0 in:18181=400000/22 [rec/s] out:18172=399790/22 [rec/s]\n","2021-11-09 22:45:24,284 INFO mapred.LocalJobRunner: Records R/W=359271/359106 > map\n","2021-11-09 22:45:27,352 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:45:27,352 INFO mapred.MapTask: bufstart = 80968938; bufend = 56079612; bufvoid = 104857494\n","2021-11-09 22:45:27,352 INFO mapred.MapTask: kvstart = 20242228(80968912); kvend = 19262696(77050784); length = 979533/6553600\n","2021-11-09 22:45:27,352 INFO mapred.MapTask: (EQUATOR) 57057852 kvi 14264456(57057824)\n","2021-11-09 22:45:28,319 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:45:28,320 INFO mapred.MapTask: (RESET) equator 57057852 kv 14264456(57057824) kvi 14221320(56885280)\n","2021-11-09 22:45:28,351 INFO streaming.PipeMapRed: R/W/S=500000/499856/0 in:18518=500000/27 [rec/s] out:18513=499856/27 [rec/s]\n","2021-11-09 22:45:30,284 INFO mapred.LocalJobRunner: Records R/W=359271/359106 > map\n","2021-11-09 22:45:30,445 INFO streaming.PipeMapRed: Records R/W=536177/535950\n","2021-11-09 22:45:33,884 INFO streaming.PipeMapRed: R/W/S=600000/599774/0 in:18181=600000/33 [rec/s] out:18174=599774/33 [rec/s]\n","2021-11-09 22:45:36,285 INFO mapred.LocalJobRunner: Records R/W=536177/535950 > map\n","2021-11-09 22:45:39,332 INFO streaming.PipeMapRed: R/W/S=700000/699904/0 in:18421=700000/38 [rec/s] out:18418=699904/38 [rec/s]\n","2021-11-09 22:45:40,457 INFO streaming.PipeMapRed: Records R/W=720892/720665\n","2021-11-09 22:45:41,306 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:45:41,306 INFO mapred.MapTask: bufstart = 57057852; bufend = 32131122; bufvoid = 104857504\n","2021-11-09 22:45:41,306 INFO mapred.MapTask: kvstart = 14264456(57057824); kvend = 13275628(53102512); length = 988829/6553600\n","2021-11-09 22:45:41,306 INFO mapred.MapTask: (EQUATOR) 33112226 kvi 8278052(33112208)\n","2021-11-09 22:45:42,213 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:45:42,223 INFO mapred.MapTask: (RESET) equator 33112226 kv 8278052(33112208) kvi 8233544(32934176)\n","2021-11-09 22:45:42,286 INFO mapred.LocalJobRunner: Records R/W=720892/720665 > map\n","2021-11-09 22:45:45,104 INFO streaming.PipeMapRed: R/W/S=800000/799804/0 in:18181=800000/44 [rec/s] out:18177=799804/44 [rec/s]\n","2021-11-09 22:45:48,286 INFO mapred.LocalJobRunner: Records R/W=720892/720665 > map\n","2021-11-09 22:45:50,374 INFO streaming.PipeMapRed: R/W/S=900000/899875/0 in:18000=900000/50 [rec/s] out:17997=899875/50 [rec/s]\n","2021-11-09 22:45:50,458 INFO streaming.PipeMapRed: Records R/W=901777/901660\n","2021-11-09 22:45:54,288 INFO mapred.LocalJobRunner: Records R/W=901777/901660 > map\n","2021-11-09 22:45:54,527 INFO mapreduce.Job:  map 70% reduce 0%\n","2021-11-09 22:45:54,977 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:45:54,977 INFO mapred.MapTask: bufstart = 33112226; bufend = 8100034; bufvoid = 104857323\n","2021-11-09 22:45:54,977 INFO mapred.MapTask: kvstart = 8278052(33112208); kvend = 7267856(29071424); length = 1010197/6553600\n","2021-11-09 22:45:54,977 INFO mapred.MapTask: (EQUATOR) 9089826 kvi 2272452(9089808)\n","2021-11-09 22:45:55,850 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:45:55,851 INFO mapred.MapTask: (RESET) equator 9089826 kv 2272452(9089808) kvi 2229096(8916384)\n","2021-11-09 22:45:55,886 INFO streaming.PipeMapRed: R/W/S=1000000/999893/0 in:18181=1000000/55 [rec/s] out:18179=999893/55 [rec/s]\n","2021-11-09 22:46:00,289 INFO mapred.LocalJobRunner: Records R/W=901777/901660 > map\n","2021-11-09 22:46:00,460 INFO streaming.PipeMapRed: Records R/W=1090039/1089789\n","2021-11-09 22:46:01,003 INFO streaming.PipeMapRed: R/W/S=1100000/1099876/0 in:18333=1100000/60 [rec/s] out:18331=1099876/60 [rec/s]\n","2021-11-09 22:46:06,226 INFO streaming.PipeMapRed: R/W/S=1200000/1199801/0 in:18461=1200000/65 [rec/s] out:18458=1199801/65 [rec/s]\n","2021-11-09 22:46:06,289 INFO mapred.LocalJobRunner: Records R/W=1090039/1089789 > map\n","2021-11-09 22:46:08,563 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:46:08,564 INFO mapred.MapTask: bufstart = 9089826; bufend = 88802348; bufvoid = 104857600\n","2021-11-09 22:46:08,564 INFO mapred.MapTask: kvstart = 2272452(9089808); kvend = 1229008(4916032); length = 1043445/6553600\n","2021-11-09 22:46:08,564 INFO mapred.MapTask: (EQUATOR) 89803948 kvi 22450980(89803920)\n","2021-11-09 22:46:09,595 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:46:09,597 INFO mapred.MapTask: (RESET) equator 89803948 kv 22450980(89803920) kvi 22400632(89602528)\n","2021-11-09 22:46:10,462 INFO streaming.PipeMapRed: Records R/W=1277569/1277333\n","2021-11-09 22:46:11,564 INFO streaming.PipeMapRed: R/W/S=1300000/1299875/0 in:18309=1300000/71 [rec/s] out:18308=1299875/71 [rec/s]\n","2021-11-09 22:46:11,872 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:46:11,873 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:46:11,873 INFO mapred.LocalJobRunner: Records R/W=1090039/1089789 > map\n","2021-11-09 22:46:11,873 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:46:11,873 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:46:11,873 INFO mapred.MapTask: bufstart = 89803948; bufend = 2100802; bufvoid = 104857536\n","2021-11-09 22:46:11,873 INFO mapred.MapTask: kvstart = 22450980(89803920); kvend = 22223620(88894480); length = 227361/6553600\n","2021-11-09 22:46:11,986 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:46:11,987 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:46:11,988 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 421049330 bytes\n","2021-11-09 22:46:12,291 INFO mapred.LocalJobRunner: Records R/W=1277569/1277333 > sort > \n","2021-11-09 22:46:14,305 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000028_0 is done. And is in the process of committing\n","2021-11-09 22:46:14,314 INFO mapred.LocalJobRunner: Records R/W=1277569/1277333 > sort\n","2021-11-09 22:46:14,314 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000028_0' done.\n","2021-11-09 22:46:14,314 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000028_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=12216834696\n","\t\tFILE: Number of bytes written=24434097614\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3899390976\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=61\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1305647\n","\t\tMap output records=1305647\n","\t\tMap output bytes=416604531\n","\t\tMap output materialized bytes=421051657\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2611294\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=4\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134250496\n","2021-11-09 22:46:14,314 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000028_0\n","2021-11-09 22:46:14,314 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000029_0\n","2021-11-09 22:46:14,315 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:46:14,315 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:46:14,316 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:46:14,316 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:3892314112+134217728\n","2021-11-09 22:46:14,337 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:46:14,353 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:46:14,354 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:46:14,354 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:46:14,354 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:46:14,354 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:46:14,354 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:46:14,361 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:46:14,372 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:46:14,413 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:46:14,418 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:46:14,471 INFO streaming.PipeMapRed: Records R/W=222/1\n","2021-11-09 22:46:14,506 INFO streaming.PipeMapRed: R/W/S=1000/871/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:46:14,538 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:46:15,007 INFO streaming.PipeMapRed: R/W/S=10000/9766/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:46:19,651 INFO streaming.PipeMapRed: R/W/S=100000/99891/0 in:20000=100000/5 [rec/s] out:19978=99891/5 [rec/s]\n","2021-11-09 22:46:24,489 INFO streaming.PipeMapRed: Records R/W=194349/194114\n","2021-11-09 22:46:24,779 INFO streaming.PipeMapRed: R/W/S=200000/199918/0 in:20000=200000/10 [rec/s] out:19991=199918/10 [rec/s]\n","2021-11-09 22:46:26,319 INFO mapred.LocalJobRunner: Records R/W=194349/194114 > map\n","2021-11-09 22:46:26,541 INFO mapreduce.Job:  map 71% reduce 0%\n","2021-11-09 22:46:27,703 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:46:27,703 INFO mapred.MapTask: bufstart = 0; bufend = 79792970; bufvoid = 104857600\n","2021-11-09 22:46:27,703 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25191040(100764160); length = 1023357/6553600\n","2021-11-09 22:46:27,703 INFO mapred.MapTask: (EQUATOR) 80819082 kvi 20204764(80819056)\n","2021-11-09 22:46:28,690 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:46:28,726 INFO mapred.MapTask: (RESET) equator 80819082 kv 20204764(80819056) kvi 20157972(80631888)\n","2021-11-09 22:46:30,463 INFO streaming.PipeMapRed: R/W/S=300000/299834/0 in:18750=300000/16 [rec/s] out:18739=299834/16 [rec/s]\n","2021-11-09 22:46:32,320 INFO mapred.LocalJobRunner: Records R/W=194349/194114 > map\n","2021-11-09 22:46:34,490 INFO streaming.PipeMapRed: Records R/W=377219/377089\n","2021-11-09 22:46:35,725 INFO streaming.PipeMapRed: R/W/S=400000/399865/0 in:19047=400000/21 [rec/s] out:19041=399865/21 [rec/s]\n","2021-11-09 22:46:38,320 INFO mapred.LocalJobRunner: Records R/W=377219/377089 > map\n","2021-11-09 22:46:40,505 INFO streaming.PipeMapRed: R/W/S=500000/499882/0 in:19230=500000/26 [rec/s] out:19226=499882/26 [rec/s]\n","2021-11-09 22:46:40,809 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:46:40,809 INFO mapred.MapTask: bufstart = 80819082; bufend = 55843813; bufvoid = 104857556\n","2021-11-09 22:46:40,809 INFO mapred.MapTask: kvstart = 20204764(80819056); kvend = 19203588(76814352); length = 1001177/6553600\n","2021-11-09 22:46:40,809 INFO mapred.MapTask: (EQUATOR) 56857493 kvi 14214368(56857472)\n","2021-11-09 22:46:41,732 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:46:41,732 INFO mapred.MapTask: (RESET) equator 56857493 kv 14214368(56857472) kvi 14169436(56677744)\n","2021-11-09 22:46:44,321 INFO mapred.LocalJobRunner: Records R/W=377219/377089 > map\n","2021-11-09 22:46:44,501 INFO streaming.PipeMapRed: Records R/W=572855/572613\n","2021-11-09 22:46:46,003 INFO streaming.PipeMapRed: R/W/S=600000/599851/0 in:19354=600000/31 [rec/s] out:19350=599851/31 [rec/s]\n","2021-11-09 22:46:50,321 INFO mapred.LocalJobRunner: Records R/W=572855/572613 > map\n","2021-11-09 22:46:50,550 INFO mapreduce.Job:  map 72% reduce 0%\n","2021-11-09 22:46:51,559 INFO streaming.PipeMapRed: R/W/S=700000/699918/0 in:18918=700000/37 [rec/s] out:18916=699918/37 [rec/s]\n","2021-11-09 22:46:54,364 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:46:54,364 INFO mapred.MapTask: bufstart = 56857493; bufend = 31958308; bufvoid = 104857533\n","2021-11-09 22:46:54,364 INFO mapred.MapTask: kvstart = 14214368(56857472); kvend = 13232420(52929680); length = 981949/6553600\n","2021-11-09 22:46:54,364 INFO mapred.MapTask: (EQUATOR) 32962916 kvi 8240724(32962896)\n","2021-11-09 22:46:54,502 INFO streaming.PipeMapRed: Records R/W=753552/753378\n","2021-11-09 22:46:55,339 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:46:55,339 INFO mapred.MapTask: (RESET) equator 32962916 kv 8240724(32962896) kvi 8196696(32786784)\n","2021-11-09 22:46:56,322 INFO mapred.LocalJobRunner: Records R/W=753552/753378 > map\n","2021-11-09 22:46:57,463 INFO streaming.PipeMapRed: R/W/S=800000/799877/0 in:18604=800000/43 [rec/s] out:18601=799877/43 [rec/s]\n","2021-11-09 22:47:02,322 INFO mapred.LocalJobRunner: Records R/W=753552/753378 > map\n","2021-11-09 22:47:02,763 INFO streaming.PipeMapRed: R/W/S=900000/899824/0 in:18750=900000/48 [rec/s] out:18746=899824/48 [rec/s]\n","2021-11-09 22:47:04,506 INFO streaming.PipeMapRed: Records R/W=935735/935491\n","2021-11-09 22:47:07,729 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:47:07,729 INFO mapred.MapTask: bufstart = 32962916; bufend = 8027930; bufvoid = 104857590\n","2021-11-09 22:47:07,729 INFO mapred.MapTask: kvstart = 8240724(32962896); kvend = 7249852(28999408); length = 990873/6553600\n","2021-11-09 22:47:07,729 INFO mapred.MapTask: (EQUATOR) 9029546 kvi 2257380(9029520)\n","2021-11-09 22:47:07,783 INFO streaming.PipeMapRed: R/W/S=1000000/999901/0 in:18867=1000000/53 [rec/s] out:18866=999901/53 [rec/s]\n","2021-11-09 22:47:08,323 INFO mapred.LocalJobRunner: Records R/W=935735/935491 > map\n","2021-11-09 22:47:08,728 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:47:08,753 INFO mapred.MapTask: (RESET) equator 9029546 kv 2257380(9029520) kvi 2210156(8840624)\n","2021-11-09 22:47:13,345 INFO streaming.PipeMapRed: R/W/S=1100000/1099790/0 in:18965=1100000/58 [rec/s] out:18961=1099790/58 [rec/s]\n","2021-11-09 22:47:14,323 INFO mapred.LocalJobRunner: Records R/W=935735/935491 > map\n","2021-11-09 22:47:14,507 INFO streaming.PipeMapRed: Records R/W=1123494/1123269\n","2021-11-09 22:47:18,601 INFO streaming.PipeMapRed: R/W/S=1200000/1199837/0 in:18750=1200000/64 [rec/s] out:18747=1199837/64 [rec/s]\n","2021-11-09 22:47:20,324 INFO mapred.LocalJobRunner: Records R/W=1123494/1123269 > map\n","2021-11-09 22:47:21,251 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:47:21,251 INFO mapred.MapTask: bufstart = 9029546; bufend = 88904979; bufvoid = 104857600\n","2021-11-09 22:47:21,251 INFO mapred.MapTask: kvstart = 2257380(9029520); kvend = 1254680(5018720); length = 1002701/6553600\n","2021-11-09 22:47:21,251 INFO mapred.MapTask: (EQUATOR) 89906579 kvi 22476640(89906560)\n","2021-11-09 22:47:22,224 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:47:22,234 INFO mapred.MapTask: (RESET) equator 89906579 kv 22476640(89906560) kvi 22428524(89714096)\n","2021-11-09 22:47:24,232 INFO streaming.PipeMapRed: R/W/S=1300000/1299828/0 in:18840=1300000/69 [rec/s] out:18838=1299828/69 [rec/s]\n","2021-11-09 22:47:24,471 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:47:24,471 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:47:24,472 INFO mapred.LocalJobRunner: Records R/W=1123494/1123269 > map\n","2021-11-09 22:47:24,472 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:47:24,472 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:47:24,472 INFO mapred.MapTask: bufstart = 89906579; bufend = 1887699; bufvoid = 104857432\n","2021-11-09 22:47:24,472 INFO mapred.MapTask: kvstart = 22476640(89906560); kvend = 22261752(89047008); length = 214889/6553600\n","2021-11-09 22:47:24,578 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:47:24,580 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:47:24,580 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 420707356 bytes\n","2021-11-09 22:47:26,325 INFO mapred.LocalJobRunner: Records R/W=1123494/1123269 > sort > \n","2021-11-09 22:47:26,561 INFO mapreduce.Job:  map 73% reduce 0%\n","2021-11-09 22:47:26,781 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000029_0 is done. And is in the process of committing\n","2021-11-09 22:47:26,800 INFO mapred.LocalJobRunner: Records R/W=1123494/1123269 > sort\n","2021-11-09 22:47:26,800 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000029_0' done.\n","2021-11-09 22:47:26,800 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000029_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=12637545882\n","\t\tFILE: Number of bytes written=25275516756\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4033838080\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=63\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1303741\n","\t\tMap output records=1303741\n","\t\tMap output bytes=416270194\n","\t\tMap output materialized bytes=420709540\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2607482\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134447104\n","2021-11-09 22:47:26,800 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000029_0\n","2021-11-09 22:47:26,800 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000030_0\n","2021-11-09 22:47:26,801 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:47:26,801 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:47:26,801 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:47:26,802 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:4026531840+134217728\n","2021-11-09 22:47:26,851 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:47:26,880 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:47:26,881 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:47:26,881 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:47:26,881 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:47:26,881 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:47:26,892 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:47:26,901 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:47:26,936 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:47:26,937 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:47:26,941 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:47:27,006 INFO streaming.PipeMapRed: Records R/W=228/1\n","2021-11-09 22:47:27,052 INFO streaming.PipeMapRed: R/W/S=1000/877/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:47:27,561 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:47:27,565 INFO streaming.PipeMapRed: R/W/S=10000/9800/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:47:32,327 INFO streaming.PipeMapRed: R/W/S=100000/99840/0 in:20000=100000/5 [rec/s] out:19968=99840/5 [rec/s]\n","2021-11-09 22:47:32,807 INFO mapred.LocalJobRunner: Records R/W=228/1 > map\n","2021-11-09 22:47:33,563 INFO mapreduce.Job:  map 73% reduce 0%\n","2021-11-09 22:47:37,007 INFO streaming.PipeMapRed: Records R/W=191897/191664\n","2021-11-09 22:47:37,442 INFO streaming.PipeMapRed: R/W/S=200000/199798/0 in:20000=200000/10 [rec/s] out:19979=199798/10 [rec/s]\n","2021-11-09 22:47:38,808 INFO mapred.LocalJobRunner: Records R/W=191897/191664 > map\n","2021-11-09 22:47:40,770 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:47:40,770 INFO mapred.MapTask: bufstart = 0; bufend = 79755896; bufvoid = 104857600\n","2021-11-09 22:47:40,771 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25181820(100727280); length = 1032577/6553600\n","2021-11-09 22:47:40,771 INFO mapred.MapTask: (EQUATOR) 80791512 kvi 20197872(80791488)\n","2021-11-09 22:47:41,726 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:47:41,731 INFO mapred.MapTask: (RESET) equator 80791512 kv 20197872(80791488) kvi 20152472(80609888)\n","2021-11-09 22:47:43,341 INFO streaming.PipeMapRed: R/W/S=300000/299802/0 in:18750=300000/16 [rec/s] out:18737=299802/16 [rec/s]\n","2021-11-09 22:47:44,808 INFO mapred.LocalJobRunner: Records R/W=191897/191664 > map\n","2021-11-09 22:47:45,567 INFO mapreduce.Job:  map 74% reduce 0%\n","2021-11-09 22:47:47,013 INFO streaming.PipeMapRed: Records R/W=369444/369198\n","2021-11-09 22:47:48,462 INFO streaming.PipeMapRed: R/W/S=400000/399823/0 in:19047=400000/21 [rec/s] out:19039=399823/21 [rec/s]\n","2021-11-09 22:47:50,809 INFO mapred.LocalJobRunner: Records R/W=369444/369198 > map\n","2021-11-09 22:47:53,618 INFO streaming.PipeMapRed: R/W/S=500000/499860/0 in:19230=500000/26 [rec/s] out:19225=499860/26 [rec/s]\n","2021-11-09 22:47:54,940 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:47:54,940 INFO mapred.MapTask: bufstart = 80791512; bufend = 55533413; bufvoid = 104857287\n","2021-11-09 22:47:54,940 INFO mapred.MapTask: kvstart = 20197872(80791488); kvend = 19126200(76504800); length = 1071673/6553600\n","2021-11-09 22:47:54,940 INFO mapred.MapTask: (EQUATOR) 56588565 kvi 14147136(56588544)\n","2021-11-09 22:47:55,925 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:47:55,930 INFO mapred.MapTask: (RESET) equator 56588565 kv 14147136(56588544) kvi 14096564(56386256)\n","2021-11-09 22:47:56,809 INFO mapred.LocalJobRunner: Records R/W=369444/369198 > map\n","2021-11-09 22:47:57,016 INFO streaming.PipeMapRed: Records R/W=559903/559667\n","2021-11-09 22:47:59,282 INFO streaming.PipeMapRed: R/W/S=600000/599818/0 in:18750=600000/32 [rec/s] out:18744=599818/32 [rec/s]\n","2021-11-09 22:48:02,810 INFO mapred.LocalJobRunner: Records R/W=559903/559667 > map\n","2021-11-09 22:48:04,572 INFO streaming.PipeMapRed: R/W/S=700000/699836/0 in:18918=700000/37 [rec/s] out:18914=699836/37 [rec/s]\n","2021-11-09 22:48:07,021 INFO streaming.PipeMapRed: Records R/W=745325/745065\n","2021-11-09 22:48:08,810 INFO mapred.LocalJobRunner: Records R/W=745325/745065 > map\n","2021-11-09 22:48:08,988 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:48:08,988 INFO mapred.MapTask: bufstart = 56588565; bufend = 31493199; bufvoid = 104857523\n","2021-11-09 22:48:08,988 INFO mapred.MapTask: kvstart = 14147136(56588544); kvend = 13116152(52464608); length = 1030985/6553600\n","2021-11-09 22:48:08,988 INFO mapred.MapTask: (EQUATOR) 32538495 kvi 8134616(32538464)\n","2021-11-09 22:48:09,952 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:48:09,953 INFO mapred.MapTask: (RESET) equator 32538495 kv 8134616(32538464) kvi 8089716(32358864)\n","2021-11-09 22:48:10,203 INFO streaming.PipeMapRed: R/W/S=800000/799792/0 in:18604=800000/43 [rec/s] out:18599=799792/43 [rec/s]\n","2021-11-09 22:48:14,811 INFO mapred.LocalJobRunner: Records R/W=745325/745065 > map\n","2021-11-09 22:48:15,500 INFO streaming.PipeMapRed: R/W/S=900000/899749/0 in:18750=900000/48 [rec/s] out:18744=899749/48 [rec/s]\n","2021-11-09 22:48:17,046 INFO streaming.PipeMapRed: Records R/W=926142/925911\n","2021-11-09 22:48:20,812 INFO mapred.LocalJobRunner: Records R/W=926142/925911 > map\n","2021-11-09 22:48:21,081 INFO streaming.PipeMapRed: R/W/S=1000000/999853/0 in:18518=1000000/54 [rec/s] out:18515=999853/54 [rec/s]\n","2021-11-09 22:48:23,138 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:48:23,138 INFO mapred.MapTask: bufstart = 32538495; bufend = 7507423; bufvoid = 104857235\n","2021-11-09 22:48:23,138 INFO mapred.MapTask: kvstart = 8134616(32538464); kvend = 7119736(28478944); length = 1014881/6553600\n","2021-11-09 22:48:23,138 INFO mapred.MapTask: (EQUATOR) 8546255 kvi 2136556(8546224)\n","2021-11-09 22:48:24,148 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:48:24,148 INFO mapred.MapTask: (RESET) equator 8546255 kv 2136556(8546224) kvi 2089196(8356784)\n","2021-11-09 22:48:26,812 INFO mapred.LocalJobRunner: Records R/W=926142/925911 > map\n","2021-11-09 22:48:26,831 INFO streaming.PipeMapRed: R/W/S=1100000/1099830/0 in:18644=1100000/59 [rec/s] out:18641=1099830/59 [rec/s]\n","2021-11-09 22:48:27,054 INFO streaming.PipeMapRed: Records R/W=1104507/1104251\n","2021-11-09 22:48:32,127 INFO streaming.PipeMapRed: R/W/S=1200000/1199893/0 in:18461=1200000/65 [rec/s] out:18459=1199893/65 [rec/s]\n","2021-11-09 22:48:32,813 INFO mapred.LocalJobRunner: Records R/W=1104507/1104251 > map\n","2021-11-09 22:48:33,582 INFO mapreduce.Job:  map 75% reduce 0%\n","2021-11-09 22:48:37,076 INFO streaming.PipeMapRed: Records R/W=1291903/1291649\n","2021-11-09 22:48:37,255 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:48:37,255 INFO mapred.MapTask: bufstart = 8546255; bufend = 88311995; bufvoid = 104857600\n","2021-11-09 22:48:37,255 INFO mapred.MapTask: kvstart = 2136556(8546224); kvend = 1106432(4425728); length = 1030125/6553600\n","2021-11-09 22:48:37,255 INFO mapred.MapTask: (EQUATOR) 89350811 kvi 22337696(89350784)\n","2021-11-09 22:48:37,675 INFO streaming.PipeMapRed: R/W/S=1300000/1299928/0 in:18571=1300000/70 [rec/s] out:18570=1299928/70 [rec/s]\n","2021-11-09 22:48:38,255 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:48:38,261 INFO mapred.MapTask: (RESET) equator 89350811 kv 22337696(89350784) kvi 22292592(89170368)\n","2021-11-09 22:48:38,813 INFO mapred.LocalJobRunner: Records R/W=1291903/1291649 > map\n","2021-11-09 22:48:40,595 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:48:40,596 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:48:40,596 INFO mapred.LocalJobRunner: Records R/W=1291903/1291649 > map\n","2021-11-09 22:48:40,597 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:48:40,597 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:48:40,597 INFO mapred.MapTask: bufstart = 89350811; bufend = 1713018; bufvoid = 104857596\n","2021-11-09 22:48:40,597 INFO mapred.MapTask: kvstart = 22337696(89350784); kvend = 22118904(88475616); length = 218793/6553600\n","2021-11-09 22:48:40,714 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:48:40,716 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:48:40,717 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 420481376 bytes\n","2021-11-09 22:48:42,980 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000030_0 is done. And is in the process of committing\n","2021-11-09 22:48:42,986 INFO mapred.LocalJobRunner: Records R/W=1291903/1291649 > sort\n","2021-11-09 22:48:42,986 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000030_0' done.\n","2021-11-09 22:48:42,986 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000030_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=13058031907\n","\t\tFILE: Number of bytes written=26116485576\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4168268800\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=65\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1349763\n","\t\tMap output records=1349763\n","\t\tMap output bytes=415928947\n","\t\tMap output materialized bytes=420484379\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2699526\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=4\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134430720\n","2021-11-09 22:48:42,986 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000030_0\n","2021-11-09 22:48:42,986 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000031_0\n","2021-11-09 22:48:42,988 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:48:42,988 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:48:42,988 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:48:42,988 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:4160749568+134217728\n","2021-11-09 22:48:43,026 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:48:43,040 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:48:43,040 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:48:43,040 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:48:43,040 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:48:43,040 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:48:43,053 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:48:43,059 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:48:43,095 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:48:43,096 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:48:43,103 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:48:43,167 INFO streaming.PipeMapRed: Records R/W=233/1\n","2021-11-09 22:48:43,211 INFO streaming.PipeMapRed: R/W/S=1000/883/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:48:43,600 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:48:43,722 INFO streaming.PipeMapRed: R/W/S=10000/9813/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:48:48,474 INFO streaming.PipeMapRed: R/W/S=100000/99860/0 in:20000=100000/5 [rec/s] out:19972=99860/5 [rec/s]\n","2021-11-09 22:48:53,166 INFO streaming.PipeMapRed: Records R/W=192446/192311\n","2021-11-09 22:48:53,627 INFO streaming.PipeMapRed: R/W/S=200000/199842/0 in:20000=200000/10 [rec/s] out:19984=199842/10 [rec/s]\n","2021-11-09 22:48:54,989 INFO mapred.LocalJobRunner: Records R/W=192446/192311 > map\n","2021-11-09 22:48:55,604 INFO mapreduce.Job:  map 76% reduce 0%\n","2021-11-09 22:48:56,815 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:48:56,815 INFO mapred.MapTask: bufstart = 0; bufend = 79724893; bufvoid = 104857600\n","2021-11-09 22:48:56,815 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25174052(100696208); length = 1040345/6553600\n","2021-11-09 22:48:56,815 INFO mapred.MapTask: (EQUATOR) 80766941 kvi 20191728(80766912)\n","2021-11-09 22:48:57,801 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:48:57,804 INFO mapred.MapTask: (RESET) equator 80766941 kv 20191728(80766912) kvi 20145508(80582032)\n","2021-11-09 22:48:59,347 INFO streaming.PipeMapRed: R/W/S=300000/299853/0 in:18750=300000/16 [rec/s] out:18740=299853/16 [rec/s]\n","2021-11-09 22:49:00,989 INFO mapred.LocalJobRunner: Records R/W=192446/192311 > map\n","2021-11-09 22:49:03,167 INFO streaming.PipeMapRed: Records R/W=373792/373542\n","2021-11-09 22:49:04,577 INFO streaming.PipeMapRed: R/W/S=400000/399777/0 in:19047=400000/21 [rec/s] out:19037=399777/21 [rec/s]\n","2021-11-09 22:49:06,990 INFO mapred.LocalJobRunner: Records R/W=373792/373542 > map\n","2021-11-09 22:49:09,597 INFO streaming.PipeMapRed: R/W/S=500000/499875/0 in:19230=500000/26 [rec/s] out:19225=499875/26 [rec/s]\n","2021-11-09 22:49:10,774 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:49:10,774 INFO mapred.MapTask: bufstart = 80766941; bufend = 55611764; bufvoid = 104857046\n","2021-11-09 22:49:10,774 INFO mapred.MapTask: kvstart = 20191728(80766912); kvend = 19145808(76583232); length = 1045921/6553600\n","2021-11-09 22:49:10,774 INFO mapred.MapTask: (EQUATOR) 56657060 kvi 14164260(56657040)\n","2021-11-09 22:49:11,737 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:49:11,744 INFO mapred.MapTask: (RESET) equator 56657060 kv 14164260(56657040) kvi 14115884(56463536)\n","2021-11-09 22:49:12,991 INFO mapred.LocalJobRunner: Records R/W=373792/373542 > map\n","2021-11-09 22:49:13,168 INFO streaming.PipeMapRed: Records R/W=561755/561517\n","2021-11-09 22:49:15,303 INFO streaming.PipeMapRed: R/W/S=600000/599830/0 in:18750=600000/32 [rec/s] out:18744=599830/32 [rec/s]\n","2021-11-09 22:49:18,991 INFO mapred.LocalJobRunner: Records R/W=561755/561517 > map\n","2021-11-09 22:49:20,899 INFO streaming.PipeMapRed: R/W/S=700000/699862/0 in:18918=700000/37 [rec/s] out:18915=699862/37 [rec/s]\n","2021-11-09 22:49:23,169 INFO streaming.PipeMapRed: Records R/W=743143/742885\n","2021-11-09 22:49:24,992 INFO mapred.LocalJobRunner: Records R/W=743143/742885 > map\n","2021-11-09 22:49:25,409 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:49:25,409 INFO mapred.MapTask: bufstart = 56657060; bufend = 31435980; bufvoid = 104857441\n","2021-11-09 22:49:25,409 INFO mapred.MapTask: kvstart = 14164260(56657040); kvend = 13101612(52406448); length = 1062649/6553600\n","2021-11-09 22:49:25,409 INFO mapred.MapTask: (EQUATOR) 32487788 kvi 8121940(32487760)\n","2021-11-09 22:49:25,613 INFO mapreduce.Job:  map 77% reduce 0%\n","2021-11-09 22:49:26,459 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:49:26,472 INFO mapred.MapTask: (RESET) equator 32487788 kv 8121940(32487760) kvi 8073528(32294112)\n","2021-11-09 22:49:26,486 INFO streaming.PipeMapRed: R/W/S=800000/799780/0 in:18604=800000/43 [rec/s] out:18599=799780/43 [rec/s]\n","2021-11-09 22:49:30,992 INFO mapred.LocalJobRunner: Records R/W=743143/742885 > map\n","2021-11-09 22:49:31,805 INFO streaming.PipeMapRed: R/W/S=900000/899886/0 in:18750=900000/48 [rec/s] out:18747=899886/48 [rec/s]\n","2021-11-09 22:49:33,177 INFO streaming.PipeMapRed: Records R/W=924994/924758\n","2021-11-09 22:49:36,993 INFO mapred.LocalJobRunner: Records R/W=924994/924758 > map\n","2021-11-09 22:49:37,236 INFO streaming.PipeMapRed: R/W/S=1000000/999807/0 in:18518=1000000/54 [rec/s] out:18514=999807/54 [rec/s]\n","2021-11-09 22:49:39,391 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:49:39,391 INFO mapred.MapTask: bufstart = 32487788; bufend = 7486467; bufvoid = 104857594\n","2021-11-09 22:49:39,391 INFO mapred.MapTask: kvstart = 8121940(32487760); kvend = 7114472(28457888); length = 1007469/6553600\n","2021-11-09 22:49:39,391 INFO mapred.MapTask: (EQUATOR) 8528515 kvi 2132124(8528496)\n","2021-11-09 22:49:40,340 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:49:40,341 INFO mapred.MapTask: (RESET) equator 8528515 kv 2132124(8528496) kvi 2084648(8338592)\n","2021-11-09 22:49:42,665 INFO streaming.PipeMapRed: R/W/S=1100000/1099846/0 in:18644=1100000/59 [rec/s] out:18641=1099852/59 [rec/s]\n","2021-11-09 22:49:42,993 INFO mapred.LocalJobRunner: Records R/W=924994/924758 > map\n","2021-11-09 22:49:43,178 INFO streaming.PipeMapRed: Records R/W=1109626/1109400\n","2021-11-09 22:49:47,734 INFO streaming.PipeMapRed: R/W/S=1200000/1199852/0 in:18750=1200000/64 [rec/s] out:18747=1199852/64 [rec/s]\n","2021-11-09 22:49:48,994 INFO mapred.LocalJobRunner: Records R/W=1109626/1109400 > map\n","2021-11-09 22:49:52,636 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:49:52,636 INFO mapred.MapTask: bufstart = 8528515; bufend = 88346190; bufvoid = 104857600\n","2021-11-09 22:49:52,636 INFO mapred.MapTask: kvstart = 2132124(8528496); kvend = 1115020(4460080); length = 1017105/6553600\n","2021-11-09 22:49:52,636 INFO mapred.MapTask: (EQUATOR) 89381806 kvi 22345444(89381776)\n","2021-11-09 22:49:53,179 INFO streaming.PipeMapRed: Records R/W=1299811/1299683\n","2021-11-09 22:49:53,189 INFO streaming.PipeMapRed: R/W/S=1300000/1299796/0 in:18571=1300000/70 [rec/s] out:18568=1299796/70 [rec/s]\n","2021-11-09 22:49:53,603 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:49:53,607 INFO mapred.MapTask: (RESET) equator 89381806 kv 22345444(89381776) kvi 22299940(89199760)\n","2021-11-09 22:49:54,995 INFO mapred.LocalJobRunner: Records R/W=1299811/1299683 > map\n","2021-11-09 22:49:56,353 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:49:56,354 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:49:56,355 INFO mapred.LocalJobRunner: Records R/W=1299811/1299683 > map\n","2021-11-09 22:49:56,355 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:49:56,355 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:49:56,355 INFO mapred.MapTask: bufstart = 89381806; bufend = 4154413; bufvoid = 104857550\n","2021-11-09 22:49:56,355 INFO mapred.MapTask: kvstart = 22345444(89381776); kvend = 22094028(88376112); length = 251417/6553600\n","2021-11-09 22:49:56,481 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:49:56,483 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:49:56,484 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 422940938 bytes\n","2021-11-09 22:49:58,750 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000031_0 is done. And is in the process of committing\n","2021-11-09 22:49:58,758 INFO mapred.LocalJobRunner: Records R/W=1299811/1299683 > sort\n","2021-11-09 22:49:58,758 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000031_0' done.\n","2021-11-09 22:49:58,759 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000031_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=13480976643\n","\t\tFILE: Number of bytes written=26962371818\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4302658560\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=67\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1356231\n","\t\tMap output records=1356231\n","\t\tMap output bytes=418367228\n","\t\tMap output materialized bytes=422943090\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2712462\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=4\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134389760\n","2021-11-09 22:49:58,759 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000031_0\n","2021-11-09 22:49:58,759 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000032_0\n","2021-11-09 22:49:58,760 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:49:58,760 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:49:58,760 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:49:58,761 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:4294967296+134217728\n","2021-11-09 22:49:58,793 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:49:58,806 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:49:58,806 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:49:58,806 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:49:58,806 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:49:58,806 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:49:58,807 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:49:58,814 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:49:58,870 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:49:58,871 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:49:58,879 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:49:58,938 INFO streaming.PipeMapRed: Records R/W=219/1\n","2021-11-09 22:49:58,982 INFO streaming.PipeMapRed: R/W/S=1000/865/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:49:59,460 INFO streaming.PipeMapRed: R/W/S=10000/9860/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:49:59,624 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:50:04,241 INFO streaming.PipeMapRed: R/W/S=100000/99752/0 in:20000=100000/5 [rec/s] out:19950=99752/5 [rec/s]\n","2021-11-09 22:50:08,942 INFO streaming.PipeMapRed: Records R/W=191614/191364\n","2021-11-09 22:50:09,389 INFO streaming.PipeMapRed: R/W/S=200000/199890/0 in:20000=200000/10 [rec/s] out:19989=199890/10 [rec/s]\n","2021-11-09 22:50:10,764 INFO mapred.LocalJobRunner: Records R/W=191614/191364 > map\n","2021-11-09 22:50:11,627 INFO mapreduce.Job:  map 78% reduce 0%\n","2021-11-09 22:50:12,247 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:50:12,247 INFO mapred.MapTask: bufstart = 0; bufend = 79797877; bufvoid = 104857600\n","2021-11-09 22:50:12,247 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25192328(100769312); length = 1022069/6553600\n","2021-11-09 22:50:12,247 INFO mapred.MapTask: (EQUATOR) 80820869 kvi 20205212(80820848)\n","2021-11-09 22:50:13,218 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:50:13,219 INFO mapred.MapTask: (RESET) equator 80820869 kv 20205212(80820848) kvi 20157020(80628080)\n","2021-11-09 22:50:14,890 INFO streaming.PipeMapRed: R/W/S=300000/299775/0 in:18750=300000/16 [rec/s] out:18735=299775/16 [rec/s]\n","2021-11-09 22:50:16,765 INFO mapred.LocalJobRunner: Records R/W=191614/191364 > map\n","2021-11-09 22:50:18,944 INFO streaming.PipeMapRed: Records R/W=380309/380072\n","2021-11-09 22:50:19,957 INFO streaming.PipeMapRed: R/W/S=400000/399798/0 in:19047=400000/21 [rec/s] out:19038=399798/21 [rec/s]\n","2021-11-09 22:50:22,765 INFO mapred.LocalJobRunner: Records R/W=380309/380072 > map\n","2021-11-09 22:50:23,631 INFO mapreduce.Job:  map 79% reduce 0%\n","2021-11-09 22:50:25,142 INFO streaming.PipeMapRed: R/W/S=500000/499918/0 in:19230=500000/26 [rec/s] out:19227=499918/26 [rec/s]\n","2021-11-09 22:50:25,945 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:50:25,945 INFO mapred.MapTask: bufstart = 80820869; bufend = 55683078; bufvoid = 104857467\n","2021-11-09 22:50:25,945 INFO mapred.MapTask: kvstart = 20205212(80820848); kvend = 19163588(76654352); length = 1041625/6553600\n","2021-11-09 22:50:25,945 INFO mapred.MapTask: (EQUATOR) 56715494 kvi 14178868(56715472)\n","2021-11-09 22:50:26,959 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:50:26,967 INFO mapred.MapTask: (RESET) equator 56715494 kv 14178868(56715472) kvi 14130364(56521456)\n","2021-11-09 22:50:28,766 INFO mapred.LocalJobRunner: Records R/W=380309/380072 > map\n","2021-11-09 22:50:28,948 INFO streaming.PipeMapRed: Records R/W=568329/568111\n","2021-11-09 22:50:30,732 INFO streaming.PipeMapRed: R/W/S=600000/599834/0 in:19354=600000/31 [rec/s] out:19349=599834/31 [rec/s]\n","2021-11-09 22:50:34,767 INFO mapred.LocalJobRunner: Records R/W=568329/568111 > map\n","2021-11-09 22:50:36,059 INFO streaming.PipeMapRed: R/W/S=700000/699777/0 in:18918=700000/37 [rec/s] out:18912=699777/37 [rec/s]\n","2021-11-09 22:50:38,949 INFO streaming.PipeMapRed: Records R/W=757566/757313\n","2021-11-09 22:50:40,226 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:50:40,226 INFO mapred.MapTask: bufstart = 56715494; bufend = 31438678; bufvoid = 104857456\n","2021-11-09 22:50:40,226 INFO mapred.MapTask: kvstart = 14178868(56715472); kvend = 13102380(52409520); length = 1076489/6553600\n","2021-11-09 22:50:40,226 INFO mapred.MapTask: (EQUATOR) 32487206 kvi 8121796(32487184)\n","2021-11-09 22:50:40,767 INFO mapred.LocalJobRunner: Records R/W=757566/757313 > map\n","2021-11-09 22:50:41,303 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:50:41,306 INFO mapred.MapTask: (RESET) equator 32487206 kv 8121796(32487184) kvi 8073464(32293856)\n","2021-11-09 22:50:41,453 INFO streaming.PipeMapRed: R/W/S=800000/799875/0 in:19047=800000/42 [rec/s] out:19044=799875/42 [rec/s]\n","2021-11-09 22:50:46,401 INFO streaming.PipeMapRed: R/W/S=900000/899813/0 in:19148=900000/47 [rec/s] out:19144=899813/47 [rec/s]\n","2021-11-09 22:50:46,768 INFO mapred.LocalJobRunner: Records R/W=757566/757313 > map\n","2021-11-09 22:50:48,958 INFO streaming.PipeMapRed: Records R/W=945611/945360\n","2021-11-09 22:50:51,719 INFO streaming.PipeMapRed: R/W/S=1000000/999910/0 in:19230=1000000/52 [rec/s] out:19229=999910/52 [rec/s]\n","2021-11-09 22:50:52,768 INFO mapred.LocalJobRunner: Records R/W=945611/945360 > map\n","2021-11-09 22:50:54,582 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:50:54,582 INFO mapred.MapTask: bufstart = 32487206; bufend = 7216778; bufvoid = 104857517\n","2021-11-09 22:50:54,582 INFO mapred.MapTask: kvstart = 8121796(32487184); kvend = 7047076(28188304); length = 1074721/6553600\n","2021-11-09 22:50:54,582 INFO mapred.MapTask: (EQUATOR) 8271946 kvi 2067980(8271920)\n","2021-11-09 22:50:55,571 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:50:55,575 INFO mapred.MapTask: (RESET) equator 8271946 kv 2067980(8271920) kvi 2023192(8092768)\n","2021-11-09 22:50:57,392 INFO streaming.PipeMapRed: R/W/S=1100000/1099840/0 in:18965=1100000/58 [rec/s] out:18962=1099840/58 [rec/s]\n","2021-11-09 22:50:58,770 INFO mapred.LocalJobRunner: Records R/W=945611/945360 > map\n","2021-11-09 22:50:58,961 INFO streaming.PipeMapRed: Records R/W=1129768/1129510\n","2021-11-09 22:51:02,727 INFO streaming.PipeMapRed: R/W/S=1200000/1199983/0 in:19047=1200000/63 [rec/s] out:19047=1199983/63 [rec/s]\n","2021-11-09 22:51:04,770 INFO mapred.LocalJobRunner: Records R/W=1129768/1129510 > map\n","2021-11-09 22:51:05,643 INFO mapreduce.Job:  map 80% reduce 0%\n","2021-11-09 22:51:08,357 INFO streaming.PipeMapRed: R/W/S=1300000/1299824/0 in:18840=1300000/69 [rec/s] out:18838=1299824/69 [rec/s]\n","2021-11-09 22:51:08,429 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:51:08,429 INFO mapred.MapTask: bufstart = 8271946; bufend = 88195482; bufvoid = 104857600\n","2021-11-09 22:51:08,429 INFO mapred.MapTask: kvstart = 2067980(8271920); kvend = 1077332(4309328); length = 990649/6553600\n","2021-11-09 22:51:08,429 INFO mapred.MapTask: (EQUATOR) 89237530 kvi 22309376(89237504)\n","2021-11-09 22:51:08,995 INFO streaming.PipeMapRed: Records R/W=1308199/1307970\n","2021-11-09 22:51:09,402 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:51:09,421 INFO mapred.MapTask: (RESET) equator 89237530 kv 22309376(89237504) kvi 22264756(89059024)\n","2021-11-09 22:51:10,771 INFO mapred.LocalJobRunner: Records R/W=1308199/1307970 > map\n","2021-11-09 22:51:11,869 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:51:11,869 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:51:11,870 INFO mapred.LocalJobRunner: Records R/W=1308199/1307970 > map\n","2021-11-09 22:51:11,870 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:51:11,870 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:51:11,870 INFO mapred.MapTask: bufstart = 89237530; bufend = 3688949; bufvoid = 104857462\n","2021-11-09 22:51:11,870 INFO mapred.MapTask: kvstart = 22309376(89237504); kvend = 22071028(88284112); length = 238349/6553600\n","2021-11-09 22:51:11,991 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:51:11,993 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:51:11,993 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 422507560 bytes\n","2021-11-09 22:51:14,258 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000032_0 is done. And is in the process of committing\n","2021-11-09 22:51:14,264 INFO mapred.LocalJobRunner: Records R/W=1308199/1307970 > sort\n","2021-11-09 22:51:14,264 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000032_0' done.\n","2021-11-09 22:51:14,264 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000032_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=13903486970\n","\t\tFILE: Number of bytes written=27807390266\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4437156864\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=69\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1360980\n","\t\tMap output records=1360980\n","\t\tMap output bytes=417917699\n","\t\tMap output materialized bytes=422509193\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2721960\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134498304\n","2021-11-09 22:51:14,265 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000032_0\n","2021-11-09 22:51:14,265 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000033_0\n","2021-11-09 22:51:14,265 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:51:14,265 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:51:14,266 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:51:14,266 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:4429185024+134217728\n","2021-11-09 22:51:14,314 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:51:14,328 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:51:14,328 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:51:14,328 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:51:14,328 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:51:14,328 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:51:14,329 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:51:14,522 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:51:14,528 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:51:14,529 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:51:14,532 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:51:14,592 INFO streaming.PipeMapRed: Records R/W=211/1\n","2021-11-09 22:51:14,630 INFO streaming.PipeMapRed: R/W/S=1000/845/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:51:14,646 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:51:15,118 INFO streaming.PipeMapRed: R/W/S=10000/9873/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:51:19,836 INFO streaming.PipeMapRed: R/W/S=100000/99780/0 in:20000=100000/5 [rec/s] out:19956=99780/5 [rec/s]\n","2021-11-09 22:51:20,269 INFO mapred.LocalJobRunner: Records R/W=211/1 > map\n","2021-11-09 22:51:20,648 INFO mapreduce.Job:  map 81% reduce 0%\n","2021-11-09 22:51:24,593 INFO streaming.PipeMapRed: Records R/W=186712/186506\n","2021-11-09 22:51:25,418 INFO streaming.PipeMapRed: R/W/S=200000/199901/0 in:20000=200000/10 [rec/s] out:19990=199901/10 [rec/s]\n","2021-11-09 22:51:26,269 INFO mapred.LocalJobRunner: Records R/W=186712/186506 > map\n","2021-11-09 22:51:28,120 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:51:28,120 INFO mapred.MapTask: bufstart = 0; bufend = 79964946; bufvoid = 104857600\n","2021-11-09 22:51:28,120 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25234072(100936288); length = 980325/6553600\n","2021-11-09 22:51:28,120 INFO mapred.MapTask: (EQUATOR) 80946050 kvi 20236508(80946032)\n","2021-11-09 22:51:29,152 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:51:29,157 INFO mapred.MapTask: (RESET) equator 80946050 kv 20236508(80946032) kvi 20193976(80775904)\n","2021-11-09 22:51:31,625 INFO streaming.PipeMapRed: R/W/S=300000/299842/0 in:17647=300000/17 [rec/s] out:17637=299842/17 [rec/s]\n","2021-11-09 22:51:32,270 INFO mapred.LocalJobRunner: Records R/W=186712/186506 > map\n","2021-11-09 22:51:34,594 INFO streaming.PipeMapRed: Records R/W=359884/359716\n","2021-11-09 22:51:36,543 INFO streaming.PipeMapRed: R/W/S=400000/399829/0 in:18181=400000/22 [rec/s] out:18174=399829/22 [rec/s]\n","2021-11-09 22:51:38,271 INFO mapred.LocalJobRunner: Records R/W=359884/359716 > map\n","2021-11-09 22:51:41,406 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:51:41,406 INFO mapred.MapTask: bufstart = 80946050; bufend = 56075627; bufvoid = 104857177\n","2021-11-09 22:51:41,406 INFO mapred.MapTask: kvstart = 20236508(80946032); kvend = 19261408(77045632); length = 975101/6553600\n","2021-11-09 22:51:41,406 INFO mapred.MapTask: (EQUATOR) 57053803 kvi 14263444(57053776)\n","2021-11-09 22:51:42,364 INFO streaming.PipeMapRed: R/W/S=500000/499912/0 in:18518=500000/27 [rec/s] out:18515=499912/27 [rec/s]\n","2021-11-09 22:51:42,374 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:51:42,374 INFO mapred.MapTask: (RESET) equator 57053803 kv 14263444(57053776) kvi 14218604(56874416)\n","2021-11-09 22:51:44,271 INFO mapred.LocalJobRunner: Records R/W=359884/359716 > map\n","2021-11-09 22:51:44,595 INFO streaming.PipeMapRed: Records R/W=543141/542911\n","2021-11-09 22:51:47,779 INFO streaming.PipeMapRed: R/W/S=600000/599852/0 in:18181=600000/33 [rec/s] out:18177=599852/33 [rec/s]\n","2021-11-09 22:51:50,272 INFO mapred.LocalJobRunner: Records R/W=543141/542911 > map\n","2021-11-09 22:51:53,479 INFO streaming.PipeMapRed: R/W/S=700000/699831/0 in:18421=700000/38 [rec/s] out:18416=699831/38 [rec/s]\n","2021-11-09 22:51:54,604 INFO streaming.PipeMapRed: Records R/W=720446/720208\n","2021-11-09 22:51:55,456 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:51:55,456 INFO mapred.MapTask: bufstart = 57053803; bufend = 32150089; bufvoid = 104856489\n","2021-11-09 22:51:55,456 INFO mapred.MapTask: kvstart = 14263444(57053776); kvend = 13280204(53120816); length = 983241/6553600\n","2021-11-09 22:51:55,456 INFO mapred.MapTask: (EQUATOR) 33131161 kvi 8282784(33131136)\n","2021-11-09 22:51:56,272 INFO mapred.LocalJobRunner: Records R/W=720446/720208 > map\n","2021-11-09 22:51:56,420 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:51:56,422 INFO mapred.MapTask: (RESET) equator 33131161 kv 8282784(33131136) kvi 8239792(32959168)\n","2021-11-09 22:51:59,358 INFO streaming.PipeMapRed: R/W/S=800000/799784/0 in:18181=800000/44 [rec/s] out:18176=799784/44 [rec/s]\n","2021-11-09 22:52:02,273 INFO mapred.LocalJobRunner: Records R/W=720446/720208 > map\n","2021-11-09 22:52:02,667 INFO mapreduce.Job:  map 82% reduce 0%\n","2021-11-09 22:52:04,605 INFO streaming.PipeMapRed: Records R/W=896144/896024\n","2021-11-09 22:52:04,818 INFO streaming.PipeMapRed: R/W/S=900000/899908/0 in:18000=900000/50 [rec/s] out:17998=899908/50 [rec/s]\n","2021-11-09 22:52:08,273 INFO mapred.LocalJobRunner: Records R/W=896144/896024 > map\n","2021-11-09 22:52:09,310 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:52:09,310 INFO mapred.MapTask: bufstart = 33131161; bufend = 8157194; bufvoid = 104856191\n","2021-11-09 22:52:09,310 INFO mapred.MapTask: kvstart = 8282784(33131136); kvend = 7281984(29127936); length = 1000801/6553600\n","2021-11-09 22:52:09,310 INFO mapred.MapTask: (EQUATOR) 9144042 kvi 2286004(9144016)\n","2021-11-09 22:52:10,315 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:52:10,316 INFO mapred.MapTask: (RESET) equator 9144042 kv 2286004(9144016) kvi 2241932(8967728)\n","2021-11-09 22:52:10,576 INFO streaming.PipeMapRed: R/W/S=1000000/999829/0 in:17857=1000000/56 [rec/s] out:17854=999829/56 [rec/s]\n","2021-11-09 22:52:14,274 INFO mapred.LocalJobRunner: Records R/W=896144/896024 > map\n","2021-11-09 22:52:14,607 INFO streaming.PipeMapRed: Records R/W=1075620/1075432\n","2021-11-09 22:52:15,945 INFO streaming.PipeMapRed: R/W/S=1100000/1099792/0 in:18032=1100000/61 [rec/s] out:18029=1099792/61 [rec/s]\n","2021-11-09 22:52:20,275 INFO mapred.LocalJobRunner: Records R/W=1075620/1075432 > map\n","2021-11-09 22:52:21,243 INFO streaming.PipeMapRed: R/W/S=1200000/1199813/0 in:18181=1200000/66 [rec/s] out:18178=1199813/66 [rec/s]\n","2021-11-09 22:52:23,408 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:52:23,408 INFO mapred.MapTask: bufstart = 9144042; bufend = 88935466; bufvoid = 104857600\n","2021-11-09 22:52:23,408 INFO mapred.MapTask: kvstart = 2286004(9144016); kvend = 1262304(5049216); length = 1023701/6553600\n","2021-11-09 22:52:23,408 INFO mapred.MapTask: (EQUATOR) 89928186 kvi 22482040(89928160)\n","2021-11-09 22:52:24,392 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:52:24,408 INFO mapred.MapTask: (RESET) equator 89928186 kv 22482040(89928160) kvi 22438364(89753456)\n","2021-11-09 22:52:24,622 INFO streaming.PipeMapRed: Records R/W=1256720/1256477\n","2021-11-09 22:52:26,275 INFO mapred.LocalJobRunner: Records R/W=1256720/1256477 > map\n","2021-11-09 22:52:26,378 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:52:26,378 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:52:26,378 INFO mapred.LocalJobRunner: Records R/W=1256720/1256477 > map\n","2021-11-09 22:52:26,379 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:52:26,379 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:52:26,379 INFO mapred.MapTask: bufstart = 89928186; bufend = 104734626; bufvoid = 104857600\n","2021-11-09 22:52:26,379 INFO mapred.MapTask: kvstart = 22482040(89928160); kvend = 22288016(89152064); length = 194025/6553600\n","2021-11-09 22:52:26,472 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:52:26,474 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:52:26,475 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 418783109 bytes\n","2021-11-09 22:52:28,688 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000033_0 is done. And is in the process of committing\n","2021-11-09 22:52:28,695 INFO mapred.LocalJobRunner: Records R/W=1256720/1256477 > sort\n","2021-11-09 22:52:28,695 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000033_0' done.\n","2021-11-09 22:52:28,695 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000033_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=14322275514\n","\t\tFILE: Number of bytes written=28644965148\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4571587584\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=71\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1289303\n","\t\tMap output records=1289303\n","\t\tMap output bytes=414384563\n","\t\tMap output materialized bytes=418787410\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2578606\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134430720\n","2021-11-09 22:52:28,695 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000033_0\n","2021-11-09 22:52:28,696 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000034_0\n","2021-11-09 22:52:28,697 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:52:28,697 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:52:28,698 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:52:28,698 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:4563402752+134217728\n","2021-11-09 22:52:28,752 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:52:28,767 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:52:28,767 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:52:28,767 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:52:28,767 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:52:28,767 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:52:28,779 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:52:28,786 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:52:28,821 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:52:28,821 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:52:28,825 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:52:28,885 INFO streaming.PipeMapRed: Records R/W=225/1\n","2021-11-09 22:52:28,923 INFO streaming.PipeMapRed: R/W/S=1000/893/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:52:29,461 INFO streaming.PipeMapRed: R/W/S=10000/9901/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:52:29,675 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:52:34,241 INFO streaming.PipeMapRed: R/W/S=100000/99901/0 in:20000=100000/5 [rec/s] out:19980=99901/5 [rec/s]\n","2021-11-09 22:52:34,702 INFO mapred.LocalJobRunner: Records R/W=225/1 > map\n","2021-11-09 22:52:35,677 INFO mapreduce.Job:  map 83% reduce 0%\n","2021-11-09 22:52:38,886 INFO streaming.PipeMapRed: Records R/W=184352/184118\n","2021-11-09 22:52:39,761 INFO streaming.PipeMapRed: R/W/S=200000/199799/0 in:20000=200000/10 [rec/s] out:19979=199799/10 [rec/s]\n","2021-11-09 22:52:40,703 INFO mapred.LocalJobRunner: Records R/W=184352/184118 > map\n","2021-11-09 22:52:42,816 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:52:42,816 INFO mapred.MapTask: bufstart = 0; bufend = 79757423; bufvoid = 104857600\n","2021-11-09 22:52:42,816 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25182068(100728272); length = 1032329/6553600\n","2021-11-09 22:52:42,816 INFO mapred.MapTask: (EQUATOR) 80789823 kvi 20197448(80789792)\n","2021-11-09 22:52:43,827 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:52:43,834 INFO mapred.MapTask: (RESET) equator 80789823 kv 20197448(80789792) kvi 20151068(80604272)\n","2021-11-09 22:52:45,439 INFO streaming.PipeMapRed: R/W/S=300000/299821/0 in:18750=300000/16 [rec/s] out:18738=299821/16 [rec/s]\n","2021-11-09 22:52:46,704 INFO mapred.LocalJobRunner: Records R/W=184352/184118 > map\n","2021-11-09 22:52:48,892 INFO streaming.PipeMapRed: Records R/W=335131/334898\n","2021-11-09 22:52:52,705 INFO mapred.LocalJobRunner: Records R/W=335131/334898 > map\n","2021-11-09 22:52:56,116 INFO streaming.PipeMapRed: R/W/S=400000/399773/0 in:14814=400000/27 [rec/s] out:14806=399773/27 [rec/s]\n","2021-11-09 22:52:58,705 INFO mapred.LocalJobRunner: Records R/W=335131/334898 > map\n","2021-11-09 22:52:58,893 INFO streaming.PipeMapRed: Records R/W=452225/452105\n","2021-11-09 22:53:01,531 INFO streaming.PipeMapRed: R/W/S=500000/499834/0 in:15625=500000/32 [rec/s] out:15619=499834/32 [rec/s]\n","2021-11-09 22:53:01,912 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:53:01,912 INFO mapred.MapTask: bufstart = 80789823; bufend = 55850011; bufvoid = 104857255\n","2021-11-09 22:53:01,912 INFO mapred.MapTask: kvstart = 20197448(80789792); kvend = 19205220(76820880); length = 992229/6553600\n","2021-11-09 22:53:01,912 INFO mapred.MapTask: (EQUATOR) 56863707 kvi 14215920(56863680)\n","2021-11-09 22:53:03,123 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:53:03,126 INFO mapred.MapTask: (RESET) equator 56863707 kv 14215920(56863680) kvi 14159956(56639824)\n","2021-11-09 22:53:04,706 INFO mapred.LocalJobRunner: Records R/W=452225/452105 > map\n","2021-11-09 22:53:05,688 INFO mapreduce.Job:  map 84% reduce 0%\n","2021-11-09 22:53:07,370 INFO streaming.PipeMapRed: R/W/S=600000/599785/0 in:15789=600000/38 [rec/s] out:15783=599785/38 [rec/s]\n","2021-11-09 22:53:08,896 INFO streaming.PipeMapRed: Records R/W=628824/628593\n","2021-11-09 22:53:10,707 INFO mapred.LocalJobRunner: Records R/W=628824/628593 > map\n","2021-11-09 22:53:12,805 INFO streaming.PipeMapRed: R/W/S=700000/699791/0 in:16279=700000/43 [rec/s] out:16274=699791/43 [rec/s]\n","2021-11-09 22:53:15,693 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:53:15,693 INFO mapred.MapTask: bufstart = 56863707; bufend = 31923830; bufvoid = 104857502\n","2021-11-09 22:53:15,693 INFO mapred.MapTask: kvstart = 14215920(56863680); kvend = 13223804(52895216); length = 992117/6553600\n","2021-11-09 22:53:15,693 INFO mapred.MapTask: (EQUATOR) 32931462 kvi 8232860(32931440)\n","2021-11-09 22:53:16,707 INFO mapred.LocalJobRunner: Records R/W=628824/628593 > map\n","2021-11-09 22:53:16,784 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:53:16,785 INFO mapred.MapTask: (RESET) equator 32931462 kv 8232860(32931440) kvi 8183532(32734128)\n","2021-11-09 22:53:18,637 INFO streaming.PipeMapRed: R/W/S=800000/799858/0 in:16326=800000/49 [rec/s] out:16323=799858/49 [rec/s]\n","2021-11-09 22:53:18,907 INFO streaming.PipeMapRed: Records R/W=804286/804021\n","2021-11-09 22:53:22,708 INFO mapred.LocalJobRunner: Records R/W=804286/804021 > map\n","2021-11-09 22:53:24,208 INFO streaming.PipeMapRed: R/W/S=900000/899717/0 in:16363=900000/55 [rec/s] out:16358=899717/55 [rec/s]\n","2021-11-09 22:53:28,708 INFO mapred.LocalJobRunner: Records R/W=804286/804021 > map\n","2021-11-09 22:53:28,911 INFO streaming.PipeMapRed: Records R/W=986605/986359\n","2021-11-09 22:53:29,478 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:53:29,478 INFO mapred.MapTask: bufstart = 32931462; bufend = 8057692; bufvoid = 104857573\n","2021-11-09 22:53:29,478 INFO mapred.MapTask: kvstart = 8232860(32931440); kvend = 7257296(29029184); length = 975565/6553600\n","2021-11-09 22:53:29,478 INFO mapred.MapTask: (EQUATOR) 9056332 kvi 2264076(9056304)\n","2021-11-09 22:53:29,620 INFO streaming.PipeMapRed: R/W/S=1000000/999864/0 in:16666=1000000/60 [rec/s] out:16664=999864/60 [rec/s]\n","2021-11-09 22:53:30,580 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:53:30,583 INFO mapred.MapTask: (RESET) equator 9056332 kv 2264076(9056304) kvi 2209452(8837808)\n","2021-11-09 22:53:34,709 INFO mapred.LocalJobRunner: Records R/W=986605/986359 > map\n","2021-11-09 22:53:35,364 INFO streaming.PipeMapRed: R/W/S=1100000/1099775/0 in:16666=1100000/66 [rec/s] out:16663=1099775/66 [rec/s]\n","2021-11-09 22:53:38,926 INFO streaming.PipeMapRed: Records R/W=1165990/1165771\n","2021-11-09 22:53:40,712 INFO mapred.LocalJobRunner: Records R/W=1165990/1165771 > map\n","2021-11-09 22:53:40,741 INFO streaming.PipeMapRed: R/W/S=1200000/1199857/0 in:16901=1200000/71 [rec/s] out:16899=1199857/71 [rec/s]\n","2021-11-09 22:53:43,242 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:53:43,242 INFO mapred.MapTask: bufstart = 9056332; bufend = 88975723; bufvoid = 104857600\n","2021-11-09 22:53:43,242 INFO mapred.MapTask: kvstart = 2264076(9056304); kvend = 1272412(5089648); length = 991665/6553600\n","2021-11-09 22:53:43,242 INFO mapred.MapTask: (EQUATOR) 89974363 kvi 22493584(89974336)\n","2021-11-09 22:53:44,338 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:53:44,343 INFO mapred.MapTask: (RESET) equator 89974363 kv 22493584(89974336) kvi 22440756(89763024)\n","2021-11-09 22:53:46,420 INFO streaming.PipeMapRed: R/W/S=1300000/1299852/0 in:16883=1300000/77 [rec/s] out:16881=1299852/77 [rec/s]\n","2021-11-09 22:53:46,496 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:53:46,497 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:53:46,497 INFO mapred.LocalJobRunner: Records R/W=1165990/1165771 > map\n","2021-11-09 22:53:46,498 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:53:46,498 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:53:46,498 INFO mapred.MapTask: bufstart = 89974363; bufend = 2636865; bufvoid = 104857495\n","2021-11-09 22:53:46,498 INFO mapred.MapTask: kvstart = 22493584(89974336); kvend = 22273112(89092448); length = 220473/6553600\n","2021-11-09 22:53:46,620 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:53:46,621 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:53:46,622 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 421445327 bytes\n","2021-11-09 22:53:46,721 INFO mapred.LocalJobRunner: Records R/W=1165990/1165771 > sort > \n","2021-11-09 22:53:47,721 INFO mapreduce.Job:  map 85% reduce 0%\n","2021-11-09 22:53:49,567 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000034_0 is done. And is in the process of committing\n","2021-11-09 22:53:49,572 INFO mapred.LocalJobRunner: Records R/W=1165990/1165771 > sort\n","2021-11-09 22:53:49,572 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000034_0' done.\n","2021-11-09 22:53:49,572 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000034_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=14743723759\n","\t\tFILE: Number of bytes written=29487859432\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4706059264\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=73\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1301099\n","\t\tMap output records=1301099\n","\t\tMap output bytes=417015682\n","\t\tMap output materialized bytes=421447111\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2602198\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134471680\n","2021-11-09 22:53:49,572 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000034_0\n","2021-11-09 22:53:49,572 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000035_0\n","2021-11-09 22:53:49,576 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:53:49,576 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:53:49,576 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:53:49,577 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:4697620480+134217728\n","2021-11-09 22:53:49,614 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:53:49,631 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:53:49,632 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:53:49,632 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:53:49,632 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:53:49,632 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:53:49,632 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:53:49,638 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:53:49,693 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:53:49,693 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:53:49,704 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:53:49,721 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:53:49,766 INFO streaming.PipeMapRed: Records R/W=216/1\n","2021-11-09 22:53:49,799 INFO streaming.PipeMapRed: R/W/S=1000/858/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:53:50,276 INFO streaming.PipeMapRed: R/W/S=10000/9849/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:53:54,994 INFO streaming.PipeMapRed: R/W/S=100000/99819/0 in:20000=100000/5 [rec/s] out:19963=99819/5 [rec/s]\n","2021-11-09 22:53:55,585 INFO mapred.LocalJobRunner: Records R/W=216/1 > map\n","2021-11-09 22:53:55,723 INFO mapreduce.Job:  map 86% reduce 0%\n","2021-11-09 22:53:59,768 INFO streaming.PipeMapRed: Records R/W=191839/191612\n","2021-11-09 22:54:00,212 INFO streaming.PipeMapRed: R/W/S=200000/199876/0 in:20000=200000/10 [rec/s] out:19987=199876/10 [rec/s]\n","2021-11-09 22:54:01,586 INFO mapred.LocalJobRunner: Records R/W=191839/191612 > map\n","2021-11-09 22:54:03,106 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:54:03,106 INFO mapred.MapTask: bufstart = 0; bufend = 79831959; bufvoid = 104857600\n","2021-11-09 22:54:03,106 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25200872(100803488); length = 1013525/6553600\n","2021-11-09 22:54:03,106 INFO mapred.MapTask: (EQUATOR) 80845687 kvi 20211416(80845664)\n","2021-11-09 22:54:04,046 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:54:04,050 INFO mapred.MapTask: (RESET) equator 80845687 kv 20211416(80845664) kvi 20164388(80657552)\n","2021-11-09 22:54:05,700 INFO streaming.PipeMapRed: R/W/S=300000/299854/0 in:18750=300000/16 [rec/s] out:18740=299854/16 [rec/s]\n","2021-11-09 22:54:07,586 INFO mapred.LocalJobRunner: Records R/W=191839/191612 > map\n","2021-11-09 22:54:09,769 INFO streaming.PipeMapRed: Records R/W=378424/378172\n","2021-11-09 22:54:10,823 INFO streaming.PipeMapRed: R/W/S=400000/399825/0 in:19047=400000/21 [rec/s] out:19039=399825/21 [rec/s]\n","2021-11-09 22:54:13,587 INFO mapred.LocalJobRunner: Records R/W=378424/378172 > map\n","2021-11-09 22:54:15,715 INFO streaming.PipeMapRed: R/W/S=500000/499891/0 in:19230=500000/26 [rec/s] out:19226=499891/26 [rec/s]\n","2021-11-09 22:54:16,501 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:54:16,501 INFO mapred.MapTask: bufstart = 80845687; bufend = 55701656; bufvoid = 104857550\n","2021-11-09 22:54:16,501 INFO mapred.MapTask: kvstart = 20211416(80845664); kvend = 19168252(76673008); length = 1043165/6553600\n","2021-11-09 22:54:16,501 INFO mapred.MapTask: (EQUATOR) 56730920 kvi 14182724(56730896)\n","2021-11-09 22:54:17,473 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:54:17,474 INFO mapred.MapTask: (RESET) equator 56730920 kv 14182724(56730896) kvi 14135632(56542528)\n","2021-11-09 22:54:19,588 INFO mapred.LocalJobRunner: Records R/W=378424/378172 > map\n","2021-11-09 22:54:19,770 INFO streaming.PipeMapRed: Records R/W=569089/568866\n","2021-11-09 22:54:21,424 INFO streaming.PipeMapRed: R/W/S=600000/599827/0 in:19354=600000/31 [rec/s] out:19349=599827/31 [rec/s]\n","2021-11-09 22:54:25,588 INFO mapred.LocalJobRunner: Records R/W=569089/568866 > map\n","2021-11-09 22:54:26,520 INFO streaming.PipeMapRed: R/W/S=700000/699773/0 in:19444=700000/36 [rec/s] out:19438=699773/36 [rec/s]\n","2021-11-09 22:54:29,771 INFO streaming.PipeMapRed: Records R/W=761160/760969\n","2021-11-09 22:54:30,516 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:54:30,516 INFO mapred.MapTask: bufstart = 56730920; bufend = 31581074; bufvoid = 104857524\n","2021-11-09 22:54:30,516 INFO mapred.MapTask: kvstart = 14182724(56730896); kvend = 13138140(52552560); length = 1044585/6553600\n","2021-11-09 22:54:30,516 INFO mapred.MapTask: (EQUATOR) 32616690 kvi 8154168(32616672)\n","2021-11-09 22:54:31,589 INFO mapred.LocalJobRunner: Records R/W=761160/760969 > map\n","2021-11-09 22:54:31,680 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:54:31,700 INFO mapred.MapTask: (RESET) equator 32616690 kv 8154168(32616672) kvi 8095420(32381680)\n","2021-11-09 22:54:32,154 INFO streaming.PipeMapRed: R/W/S=800000/799863/0 in:19047=800000/42 [rec/s] out:19044=799863/42 [rec/s]\n","2021-11-09 22:54:37,301 INFO streaming.PipeMapRed: R/W/S=900000/899894/0 in:19148=900000/47 [rec/s] out:19146=899894/47 [rec/s]\n","2021-11-09 22:54:37,589 INFO mapred.LocalJobRunner: Records R/W=761160/760969 > map\n","2021-11-09 22:54:39,772 INFO streaming.PipeMapRed: Records R/W=947001/946842\n","2021-11-09 22:54:42,663 INFO streaming.PipeMapRed: R/W/S=1000000/999792/0 in:19230=1000000/52 [rec/s] out:19226=999792/52 [rec/s]\n","2021-11-09 22:54:43,590 INFO mapred.LocalJobRunner: Records R/W=947001/946842 > map\n","2021-11-09 22:54:43,737 INFO mapreduce.Job:  map 87% reduce 0%\n","2021-11-09 22:54:43,882 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:54:43,882 INFO mapred.MapTask: bufstart = 32616690; bufend = 7690010; bufvoid = 104857571\n","2021-11-09 22:54:43,882 INFO mapred.MapTask: kvstart = 8154168(32616672); kvend = 7165372(28661488); length = 988797/6553600\n","2021-11-09 22:54:43,882 INFO mapred.MapTask: (EQUATOR) 8713002 kvi 2178244(8712976)\n","2021-11-09 22:54:44,969 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:54:44,970 INFO mapred.MapTask: (RESET) equator 8713002 kv 2178244(8712976) kvi 2129324(8517296)\n","2021-11-09 22:54:48,450 INFO streaming.PipeMapRed: R/W/S=1100000/1099865/0 in:18965=1100000/58 [rec/s] out:18963=1099865/58 [rec/s]\n","2021-11-09 22:54:49,590 INFO mapred.LocalJobRunner: Records R/W=947001/946842 > map\n","2021-11-09 22:54:49,777 INFO streaming.PipeMapRed: Records R/W=1124442/1124197\n","2021-11-09 22:54:53,996 INFO streaming.PipeMapRed: R/W/S=1200000/1199888/0 in:18750=1200000/64 [rec/s] out:18748=1199888/64 [rec/s]\n","2021-11-09 22:54:55,591 INFO mapred.LocalJobRunner: Records R/W=1124442/1124197 > map\n","2021-11-09 22:54:57,602 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:54:57,603 INFO mapred.MapTask: bufstart = 8713002; bufend = 88724007; bufvoid = 104857600\n","2021-11-09 22:54:57,603 INFO mapred.MapTask: kvstart = 2178244(8712976); kvend = 1209456(4837824); length = 968789/6553600\n","2021-11-09 22:54:57,603 INFO mapred.MapTask: (EQUATOR) 89737719 kvi 22434424(89737696)\n","2021-11-09 22:54:58,701 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:54:58,706 INFO mapred.MapTask: (RESET) equator 89737719 kv 22434424(89737696) kvi 22386536(89546144)\n","2021-11-09 22:54:59,778 INFO streaming.PipeMapRed: Records R/W=1296272/1296104\n","2021-11-09 22:54:59,989 INFO streaming.PipeMapRed: R/W/S=1300000/1299876/0 in:18571=1300000/70 [rec/s] out:18569=1299876/70 [rec/s]\n","2021-11-09 22:55:00,628 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:55:00,634 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:55:00,634 INFO mapred.LocalJobRunner: Records R/W=1124442/1124197 > map\n","2021-11-09 22:55:00,635 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:55:00,635 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:55:00,635 INFO mapred.MapTask: bufstart = 89737719; bufend = 30046; bufvoid = 104857245\n","2021-11-09 22:55:00,635 INFO mapred.MapTask: kvstart = 22434424(89737696); kvend = 22253012(89012048); length = 181413/6553600\n","2021-11-09 22:55:00,738 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:55:00,739 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:55:00,739 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 418791309 bytes\n","2021-11-09 22:55:01,594 INFO mapred.LocalJobRunner: Records R/W=1296272/1296104 > sort > \n","2021-11-09 22:55:03,499 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000035_0 is done. And is in the process of committing\n","2021-11-09 22:55:03,503 INFO mapred.LocalJobRunner: Records R/W=1296272/1296104 > sort\n","2021-11-09 22:55:03,503 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000035_0' done.\n","2021-11-09 22:55:03,504 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000035_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=15162517550\n","\t\tFILE: Number of bytes written=30325444808\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4840457216\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=75\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1310073\n","\t\tMap output records=1310073\n","\t\tMap output bytes=414344624\n","\t\tMap output materialized bytes=418792657\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2620146\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=4\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134397952\n","2021-11-09 22:55:03,504 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000035_0\n","2021-11-09 22:55:03,504 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000036_0\n","2021-11-09 22:55:03,508 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:55:03,508 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:55:03,508 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:55:03,509 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:4831838208+134217728\n","2021-11-09 22:55:03,548 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:55:03,562 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:55:03,562 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:55:03,562 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:55:03,562 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:55:03,562 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:55:03,564 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:55:03,579 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:55:03,611 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:55:03,612 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:55:03,616 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:55:03,680 INFO streaming.PipeMapRed: Records R/W=213/1\n","2021-11-09 22:55:03,713 INFO streaming.PipeMapRed: R/W/S=1000/845/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:55:03,743 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:55:04,216 INFO streaming.PipeMapRed: R/W/S=10000/9823/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:55:09,360 INFO streaming.PipeMapRed: R/W/S=100000/99776/0 in:20000=100000/5 [rec/s] out:19955=99776/5 [rec/s]\n","2021-11-09 22:55:13,690 INFO streaming.PipeMapRed: Records R/W=177085/176846\n","2021-11-09 22:55:14,969 INFO streaming.PipeMapRed: R/W/S=200000/199812/0 in:18181=200000/11 [rec/s] out:18164=199812/11 [rec/s]\n","2021-11-09 22:55:15,509 INFO mapred.LocalJobRunner: Records R/W=177085/176846 > map\n","2021-11-09 22:55:15,746 INFO mapreduce.Job:  map 88% reduce 0%\n","2021-11-09 22:55:17,328 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:55:17,328 INFO mapred.MapTask: bufstart = 0; bufend = 80018894; bufvoid = 104857600\n","2021-11-09 22:55:17,328 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25247568(100990272); length = 966829/6553600\n","2021-11-09 22:55:17,328 INFO mapred.MapTask: (EQUATOR) 80985870 kvi 20246460(80985840)\n","2021-11-09 22:55:18,244 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:55:18,245 INFO mapred.MapTask: (RESET) equator 80985870 kv 20246460(80985840) kvi 20204968(80819872)\n","2021-11-09 22:55:20,794 INFO streaming.PipeMapRed: R/W/S=300000/299887/0 in:17647=300000/17 [rec/s] out:17640=299887/17 [rec/s]\n","2021-11-09 22:55:21,510 INFO mapred.LocalJobRunner: Records R/W=177085/176846 > map\n","2021-11-09 22:55:23,690 INFO streaming.PipeMapRed: Records R/W=351821/351589\n","2021-11-09 22:55:26,349 INFO streaming.PipeMapRed: R/W/S=400000/399855/0 in:18181=400000/22 [rec/s] out:18175=399855/22 [rec/s]\n","2021-11-09 22:55:27,510 INFO mapred.LocalJobRunner: Records R/W=351821/351589 > map\n","2021-11-09 22:55:31,035 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:55:31,036 INFO mapred.MapTask: bufstart = 80985870; bufend = 56105572; bufvoid = 104857595\n","2021-11-09 22:55:31,036 INFO mapred.MapTask: kvstart = 20246460(80985840); kvend = 19269252(77077008); length = 977209/6553600\n","2021-11-09 22:55:31,036 INFO mapred.MapTask: (EQUATOR) 57078148 kvi 14269532(57078128)\n","2021-11-09 22:55:32,040 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:55:32,040 INFO mapred.MapTask: (RESET) equator 57078148 kv 14269532(57078128) kvi 14224704(56898816)\n","2021-11-09 22:55:32,191 INFO streaming.PipeMapRed: R/W/S=500000/499908/0 in:17857=500000/28 [rec/s] out:17853=499908/28 [rec/s]\n","2021-11-09 22:55:33,511 INFO mapred.LocalJobRunner: Records R/W=351821/351589 > map\n","2021-11-09 22:55:33,691 INFO streaming.PipeMapRed: Records R/W=525977/525882\n","2021-11-09 22:55:37,915 INFO streaming.PipeMapRed: R/W/S=600000/599865/0 in:17647=600000/34 [rec/s] out:17643=599865/34 [rec/s]\n","2021-11-09 22:55:39,511 INFO mapred.LocalJobRunner: Records R/W=525977/525882 > map\n","2021-11-09 22:55:39,755 INFO mapreduce.Job:  map 89% reduce 0%\n","2021-11-09 22:55:43,496 INFO streaming.PipeMapRed: R/W/S=700000/699859/0 in:17948=700000/39 [rec/s] out:17945=699859/39 [rec/s]\n","2021-11-09 22:55:43,710 INFO streaming.PipeMapRed: Records R/W=704371/704130\n","2021-11-09 22:55:45,495 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:55:45,495 INFO mapred.MapTask: bufstart = 57078148; bufend = 32111483; bufvoid = 104857239\n","2021-11-09 22:55:45,495 INFO mapred.MapTask: kvstart = 14269532(57078128); kvend = 13270664(53082656); length = 998869/6553600\n","2021-11-09 22:55:45,495 INFO mapred.MapTask: (EQUATOR) 33092587 kvi 8273140(33092560)\n","2021-11-09 22:55:45,512 INFO mapred.LocalJobRunner: Records R/W=704371/704130 > map\n","2021-11-09 22:55:46,492 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:55:46,494 INFO mapred.MapTask: (RESET) equator 33092587 kv 8273140(33092560) kvi 8226748(32906992)\n","2021-11-09 22:55:49,386 INFO streaming.PipeMapRed: R/W/S=800000/799897/0 in:17777=800000/45 [rec/s] out:17775=799897/45 [rec/s]\n","2021-11-09 22:55:51,512 INFO mapred.LocalJobRunner: Records R/W=704371/704130 > map\n","2021-11-09 22:55:53,711 INFO streaming.PipeMapRed: Records R/W=876588/876476\n","2021-11-09 22:55:55,069 INFO streaming.PipeMapRed: R/W/S=900000/899842/0 in:17647=900000/51 [rec/s] out:17643=899842/51 [rec/s]\n","2021-11-09 22:55:57,513 INFO mapred.LocalJobRunner: Records R/W=876588/876476 > map\n","2021-11-09 22:55:59,784 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:55:59,784 INFO mapred.MapTask: bufstart = 33092587; bufend = 8061575; bufvoid = 104857353\n","2021-11-09 22:55:59,784 INFO mapred.MapTask: kvstart = 8273140(33092560); kvend = 7258252(29033008); length = 1014889/6553600\n","2021-11-09 22:55:59,784 INFO mapred.MapTask: (EQUATOR) 9051367 kvi 2262836(9051344)\n","2021-11-09 22:56:00,689 INFO streaming.PipeMapRed: R/W/S=1000000/999810/0 in:17543=1000000/57 [rec/s] out:17540=999810/57 [rec/s]\n","2021-11-09 22:56:00,754 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:56:00,763 INFO mapred.MapTask: (RESET) equator 9051367 kv 2262836(9051344) kvi 2217768(8871072)\n","2021-11-09 22:56:03,513 INFO mapred.LocalJobRunner: Records R/W=876588/876476 > map\n","2021-11-09 22:56:03,712 INFO streaming.PipeMapRed: Records R/W=1053014/1052882\n","2021-11-09 22:56:06,287 INFO streaming.PipeMapRed: R/W/S=1100000/1099903/0 in:17741=1100000/62 [rec/s] out:17740=1099903/62 [rec/s]\n","2021-11-09 22:56:09,514 INFO mapred.LocalJobRunner: Records R/W=1053014/1052882 > map\n","2021-11-09 22:56:11,726 INFO streaming.PipeMapRed: R/W/S=1200000/1199941/0 in:17647=1200000/68 [rec/s] out:17646=1199941/68 [rec/s]\n","2021-11-09 22:56:13,732 INFO streaming.PipeMapRed: Records R/W=1236424/1236197\n","2021-11-09 22:56:14,559 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:56:14,559 INFO mapred.MapTask: bufstart = 9051367; bufend = 88755082; bufvoid = 104857600\n","2021-11-09 22:56:14,559 INFO mapred.MapTask: kvstart = 2262836(9051344); kvend = 1217200(4868800); length = 1045637/6553600\n","2021-11-09 22:56:14,559 INFO mapred.MapTask: (EQUATOR) 89756682 kvi 22439164(89756656)\n","2021-11-09 22:56:15,514 INFO mapred.LocalJobRunner: Records R/W=1236424/1236197 > map\n","2021-11-09 22:56:15,555 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:56:15,562 INFO mapred.MapTask: (RESET) equator 89756682 kv 22439164(89756656) kvi 22392368(89569472)\n","2021-11-09 22:56:17,543 INFO streaming.PipeMapRed: R/W/S=1300000/1299790/0 in:17808=1300000/73 [rec/s] out:17805=1299790/73 [rec/s]\n","2021-11-09 22:56:17,758 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:56:17,758 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:56:17,759 INFO mapred.LocalJobRunner: Records R/W=1236424/1236197 > map\n","2021-11-09 22:56:17,759 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:56:17,759 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:56:17,759 INFO mapred.MapTask: bufstart = 89756682; bufend = 757811; bufvoid = 104857488\n","2021-11-09 22:56:17,759 INFO mapred.MapTask: kvstart = 22439164(89756656); kvend = 22229812(88919248); length = 209353/6553600\n","2021-11-09 22:56:17,871 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:56:17,873 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:56:17,873 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 419711894 bytes\n","2021-11-09 22:56:20,143 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000036_0 is done. And is in the process of committing\n","2021-11-09 22:56:20,149 INFO mapred.LocalJobRunner: Records R/W=1236424/1236197 > sort\n","2021-11-09 22:56:20,150 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000036_0' done.\n","2021-11-09 22:56:20,150 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000036_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=15582231804\n","\t\tFILE: Number of bytes written=31164871110\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4974863360\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=77\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1303201\n","\t\tMap output records=1303201\n","\t\tMap output bytes=415275438\n","\t\tMap output materialized bytes=419713120\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2606402\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134406144\n","2021-11-09 22:56:20,150 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000036_0\n","2021-11-09 22:56:20,150 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000037_0\n","2021-11-09 22:56:20,151 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:56:20,151 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:56:20,151 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:56:20,152 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:4966055936+134217728\n","2021-11-09 22:56:20,183 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:56:20,199 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:56:20,199 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:56:20,199 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:56:20,199 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:56:20,199 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:56:20,200 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:56:20,206 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:56:20,237 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:56:20,238 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:56:20,242 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:56:20,303 INFO streaming.PipeMapRed: Records R/W=213/1\n","2021-11-09 22:56:20,335 INFO streaming.PipeMapRed: R/W/S=1000/823/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:56:20,768 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:56:20,831 INFO streaming.PipeMapRed: R/W/S=10000/9858/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:56:25,528 INFO streaming.PipeMapRed: R/W/S=100000/99848/0 in:20000=100000/5 [rec/s] out:19969=99848/5 [rec/s]\n","2021-11-09 22:56:30,311 INFO streaming.PipeMapRed: Records R/W=194299/194049\n","2021-11-09 22:56:30,574 INFO streaming.PipeMapRed: R/W/S=200000/199813/0 in:20000=200000/10 [rec/s] out:19981=199813/10 [rec/s]\n","2021-11-09 22:56:32,164 INFO mapred.LocalJobRunner: Records R/W=194299/194049 > map\n","2021-11-09 22:56:32,771 INFO mapreduce.Job:  map 91% reduce 0%\n","2021-11-09 22:56:33,343 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:56:33,343 INFO mapred.MapTask: bufstart = 0; bufend = 79869319; bufvoid = 104857600\n","2021-11-09 22:56:33,343 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25210212(100840848); length = 1004185/6553600\n","2021-11-09 22:56:33,343 INFO mapred.MapTask: (EQUATOR) 80873927 kvi 20218476(80873904)\n","2021-11-09 22:56:34,364 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:56:34,364 INFO mapred.MapTask: (RESET) equator 80873927 kv 20218476(80873904) kvi 20170796(80683184)\n","2021-11-09 22:56:36,555 INFO streaming.PipeMapRed: R/W/S=300000/299895/0 in:18750=300000/16 [rec/s] out:18743=299895/16 [rec/s]\n","2021-11-09 22:56:38,165 INFO mapred.LocalJobRunner: Records R/W=194299/194049 > map\n","2021-11-09 22:56:40,326 INFO streaming.PipeMapRed: Records R/W=369837/369591\n","2021-11-09 22:56:41,865 INFO streaming.PipeMapRed: R/W/S=400000/399825/0 in:19047=400000/21 [rec/s] out:19039=399825/21 [rec/s]\n","2021-11-09 22:56:44,165 INFO mapred.LocalJobRunner: Records R/W=369837/369591 > map\n","2021-11-09 22:56:46,810 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:56:46,810 INFO mapred.MapTask: bufstart = 80873927; bufend = 55989533; bufvoid = 104857340\n","2021-11-09 22:56:46,810 INFO mapred.MapTask: kvstart = 20218476(80873904); kvend = 19240228(76960912); length = 978249/6553600\n","2021-11-09 22:56:46,811 INFO mapred.MapTask: (EQUATOR) 56982253 kvi 14245556(56982224)\n","2021-11-09 22:56:47,155 INFO streaming.PipeMapRed: R/W/S=500000/499863/0 in:19230=500000/26 [rec/s] out:19225=499863/26 [rec/s]\n","2021-11-09 22:56:47,854 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:56:47,872 INFO mapred.MapTask: (RESET) equator 56982253 kv 14245556(56982224) kvi 14197384(56789536)\n","2021-11-09 22:56:50,166 INFO mapred.LocalJobRunner: Records R/W=369837/369591 > map\n","2021-11-09 22:56:50,348 INFO streaming.PipeMapRed: Records R/W=550915/550685\n","2021-11-09 22:56:53,210 INFO streaming.PipeMapRed: R/W/S=600000/599873/0 in:18750=600000/32 [rec/s] out:18746=599873/32 [rec/s]\n","2021-11-09 22:56:56,166 INFO mapred.LocalJobRunner: Records R/W=550915/550685 > map\n","2021-11-09 22:56:58,604 INFO streaming.PipeMapRed: R/W/S=700000/699895/0 in:18421=700000/38 [rec/s] out:18418=699895/38 [rec/s]\n","2021-11-09 22:57:00,349 INFO streaming.PipeMapRed: Records R/W=729227/729102\n","2021-11-09 22:57:00,826 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:57:00,826 INFO mapred.MapTask: bufstart = 56982253; bufend = 32158632; bufvoid = 104856175\n","2021-11-09 22:57:00,826 INFO mapred.MapTask: kvstart = 14245556(56982224); kvend = 13282504(53130016); length = 963053/6553600\n","2021-11-09 22:57:00,826 INFO mapred.MapTask: (EQUATOR) 33142616 kvi 8285648(33142592)\n","2021-11-09 22:57:01,806 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:57:01,808 INFO mapred.MapTask: (RESET) equator 33142616 kv 8285648(33142592) kvi 8242448(32969792)\n","2021-11-09 22:57:02,167 INFO mapred.LocalJobRunner: Records R/W=729227/729102 > map\n","2021-11-09 22:57:04,911 INFO streaming.PipeMapRed: R/W/S=800000/799796/0 in:18181=800000/44 [rec/s] out:18177=799796/44 [rec/s]\n","2021-11-09 22:57:08,167 INFO mapred.LocalJobRunner: Records R/W=729227/729102 > map\n","2021-11-09 22:57:10,351 INFO streaming.PipeMapRed: Records R/W=897559/897314\n","2021-11-09 22:57:10,463 INFO streaming.PipeMapRed: R/W/S=900000/899833/0 in:18000=900000/50 [rec/s] out:17996=899833/50 [rec/s]\n","2021-11-09 22:57:14,168 INFO mapred.LocalJobRunner: Records R/W=897559/897314 > map\n","2021-11-09 22:57:15,053 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:57:15,053 INFO mapred.MapTask: bufstart = 33142616; bufend = 8303785; bufvoid = 104857526\n","2021-11-09 22:57:15,053 INFO mapred.MapTask: kvstart = 8285648(33142592); kvend = 7318828(29275312); length = 966821/6553600\n","2021-11-09 22:57:15,053 INFO mapred.MapTask: (EQUATOR) 9282041 kvi 2320504(9282016)\n","2021-11-09 22:57:15,989 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:57:15,989 INFO mapred.MapTask: (RESET) equator 9282041 kv 2320504(9282016) kvi 2276828(9107312)\n","2021-11-09 22:57:16,618 INFO streaming.PipeMapRed: R/W/S=1000000/999856/0 in:17857=1000000/56 [rec/s] out:17854=999856/56 [rec/s]\n","2021-11-09 22:57:20,168 INFO mapred.LocalJobRunner: Records R/W=897559/897314 > map\n","2021-11-09 22:57:20,352 INFO streaming.PipeMapRed: Records R/W=1068222/1068127\n","2021-11-09 22:57:20,784 INFO mapreduce.Job:  map 92% reduce 0%\n","2021-11-09 22:57:22,192 INFO streaming.PipeMapRed: R/W/S=1100000/1099863/0 in:18032=1100000/61 [rec/s] out:18030=1099863/61 [rec/s]\n","2021-11-09 22:57:26,169 INFO mapred.LocalJobRunner: Records R/W=1068222/1068127 > map\n","2021-11-09 22:57:27,763 INFO streaming.PipeMapRed: R/W/S=1200000/1199811/0 in:17910=1200000/67 [rec/s] out:17907=1199811/67 [rec/s]\n","2021-11-09 22:57:28,930 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:57:28,931 INFO mapred.MapTask: bufstart = 9282041; bufend = 89258778; bufvoid = 104857600\n","2021-11-09 22:57:28,931 INFO mapred.MapTask: kvstart = 2320504(9282016); kvend = 1343108(5372432); length = 977397/6553600\n","2021-11-09 22:57:28,931 INFO mapred.MapTask: (EQUATOR) 90237018 kvi 22559248(90236992)\n","2021-11-09 22:57:29,876 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:57:29,884 INFO mapred.MapTask: (RESET) equator 90237018 kv 22559248(90236992) kvi 22516304(90065216)\n","2021-11-09 22:57:30,353 INFO streaming.PipeMapRed: Records R/W=1240510/1240315\n","2021-11-09 22:57:32,168 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:57:32,169 INFO mapred.LocalJobRunner: Records R/W=1240510/1240315 > map\n","2021-11-09 22:57:32,170 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:57:32,171 INFO mapred.LocalJobRunner: Records R/W=1240510/1240315 > map\n","2021-11-09 22:57:32,171 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:57:32,171 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:57:32,171 INFO mapred.MapTask: bufstart = 90237018; bufend = 1692462; bufvoid = 104857593\n","2021-11-09 22:57:32,171 INFO mapred.MapTask: kvstart = 22559248(90236992); kvend = 22353716(89414864); length = 205533/6553600\n","2021-11-09 22:57:32,280 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:57:32,282 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:57:32,286 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 420536021 bytes\n","2021-11-09 22:57:34,509 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000037_0 is done. And is in the process of committing\n","2021-11-09 22:57:34,514 INFO mapred.LocalJobRunner: Records R/W=1240510/1240315 > sort\n","2021-11-09 22:57:34,514 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000037_0' done.\n","2021-11-09 22:57:34,514 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000037_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=16002770283\n","\t\tFILE: Number of bytes written=32005946886\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5109352448\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=79\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1273814\n","\t\tMap output records=1273814\n","\t\tMap output bytes=416183288\n","\t\tMap output materialized bytes=420537857\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2547628\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134489088\n","2021-11-09 22:57:34,514 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000037_0\n","2021-11-09 22:57:34,514 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000038_0\n","2021-11-09 22:57:34,515 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:57:34,515 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:57:34,515 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:57:34,516 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:5100273664+134217728\n","2021-11-09 22:57:34,561 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:57:34,572 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:57:34,572 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:57:34,572 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:57:34,572 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:57:34,572 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:57:34,573 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:57:34,578 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:57:34,611 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:57:34,611 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:57:34,615 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:57:34,673 INFO streaming.PipeMapRed: Records R/W=220/1\n","2021-11-09 22:57:34,707 INFO streaming.PipeMapRed: R/W/S=1000/831/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:57:34,788 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:57:35,298 INFO streaming.PipeMapRed: R/W/S=10000/9883/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:57:39,812 INFO streaming.PipeMapRed: R/W/S=100000/99815/0 in:20000=100000/5 [rec/s] out:19963=99815/5 [rec/s]\n","2021-11-09 22:57:44,685 INFO streaming.PipeMapRed: Records R/W=193297/193065\n","2021-11-09 22:57:45,050 INFO streaming.PipeMapRed: R/W/S=200000/199848/0 in:20000=200000/10 [rec/s] out:19984=199848/10 [rec/s]\n","2021-11-09 22:57:46,518 INFO mapred.LocalJobRunner: Records R/W=193297/193065 > map\n","2021-11-09 22:57:46,791 INFO mapreduce.Job:  map 93% reduce 0%\n","2021-11-09 22:57:48,057 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:57:48,057 INFO mapred.MapTask: bufstart = 0; bufend = 79790516; bufvoid = 104857600\n","2021-11-09 22:57:48,057 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25190464(100761856); length = 1023933/6553600\n","2021-11-09 22:57:48,057 INFO mapred.MapTask: (EQUATOR) 80816628 kvi 20204152(80816608)\n","2021-11-09 22:57:49,046 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:57:49,051 INFO mapred.MapTask: (RESET) equator 80816628 kv 20204152(80816608) kvi 20155800(80623200)\n","2021-11-09 22:57:50,674 INFO streaming.PipeMapRed: R/W/S=300000/299879/0 in:18750=300000/16 [rec/s] out:18742=299879/16 [rec/s]\n","2021-11-09 22:57:52,519 INFO mapred.LocalJobRunner: Records R/W=193297/193065 > map\n","2021-11-09 22:57:54,694 INFO streaming.PipeMapRed: Records R/W=373997/373746\n","2021-11-09 22:57:55,983 INFO streaming.PipeMapRed: R/W/S=400000/399852/0 in:19047=400000/21 [rec/s] out:19040=399852/21 [rec/s]\n","2021-11-09 22:57:58,519 INFO mapred.LocalJobRunner: Records R/W=373997/373746 > map\n","2021-11-09 22:58:01,159 INFO streaming.PipeMapRed: R/W/S=500000/499805/0 in:19230=500000/26 [rec/s] out:19223=499805/26 [rec/s]\n","2021-11-09 22:58:02,072 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:58:02,072 INFO mapred.MapTask: bufstart = 80816628; bufend = 55666737; bufvoid = 104857494\n","2021-11-09 22:58:02,072 INFO mapred.MapTask: kvstart = 20204152(80816608); kvend = 19159544(76638176); length = 1044609/6553600\n","2021-11-09 22:58:02,072 INFO mapred.MapTask: (EQUATOR) 56702353 kvi 14175584(56702336)\n","2021-11-09 22:58:03,082 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:58:03,086 INFO mapred.MapTask: (RESET) equator 56702353 kv 14175584(56702336) kvi 14129984(56519936)\n","2021-11-09 22:58:04,520 INFO mapred.LocalJobRunner: Records R/W=373997/373746 > map\n","2021-11-09 22:58:04,695 INFO streaming.PipeMapRed: Records R/W=560520/560425\n","2021-11-09 22:58:06,899 INFO streaming.PipeMapRed: R/W/S=600000/599780/0 in:18750=600000/32 [rec/s] out:18743=599780/32 [rec/s]\n","2021-11-09 22:58:10,520 INFO mapred.LocalJobRunner: Records R/W=560520/560425 > map\n","2021-11-09 22:58:12,166 INFO streaming.PipeMapRed: R/W/S=700000/699865/0 in:18918=700000/37 [rec/s] out:18915=699865/37 [rec/s]\n","2021-11-09 22:58:14,713 INFO streaming.PipeMapRed: Records R/W=747477/747255\n","2021-11-09 22:58:15,783 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:58:15,783 INFO mapred.MapTask: bufstart = 56702353; bufend = 31688391; bufvoid = 104857484\n","2021-11-09 22:58:15,783 INFO mapred.MapTask: kvstart = 14175584(56702336); kvend = 13164948(52659792); length = 1010637/6553600\n","2021-11-09 22:58:15,783 INFO mapred.MapTask: (EQUATOR) 32717655 kvi 8179408(32717632)\n","2021-11-09 22:58:16,524 INFO mapred.LocalJobRunner: Records R/W=747477/747255 > map\n","2021-11-09 22:58:16,669 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:58:16,673 INFO mapred.MapTask: (RESET) equator 32717655 kv 8179408(32717632) kvi 8135852(32543408)\n","2021-11-09 22:58:16,798 INFO mapreduce.Job:  map 94% reduce 0%\n","2021-11-09 22:58:17,732 INFO streaming.PipeMapRed: R/W/S=800000/799859/0 in:18604=800000/43 [rec/s] out:18601=799859/43 [rec/s]\n","2021-11-09 22:58:22,524 INFO mapred.LocalJobRunner: Records R/W=747477/747255 > map\n","2021-11-09 22:58:23,012 INFO streaming.PipeMapRed: R/W/S=900000/899869/0 in:18750=900000/48 [rec/s] out:18747=899869/48 [rec/s]\n","2021-11-09 22:58:24,714 INFO streaming.PipeMapRed: Records R/W=931000/930760\n","2021-11-09 22:58:28,525 INFO mapred.LocalJobRunner: Records R/W=931000/930760 > map\n","2021-11-09 22:58:28,598 INFO streaming.PipeMapRed: R/W/S=1000000/999892/0 in:18518=1000000/54 [rec/s] out:18516=999892/54 [rec/s]\n","2021-11-09 22:58:29,625 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:58:29,625 INFO mapred.MapTask: bufstart = 32717655; bufend = 7776583; bufvoid = 104856703\n","2021-11-09 22:58:29,625 INFO mapred.MapTask: kvstart = 8179408(32717632); kvend = 7187012(28748048); length = 992397/6553600\n","2021-11-09 22:58:29,625 INFO mapred.MapTask: (EQUATOR) 8796471 kvi 2199112(8796448)\n","2021-11-09 22:58:30,640 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:58:30,640 INFO mapred.MapTask: (RESET) equator 8796471 kv 2199112(8796448) kvi 2151328(8605312)\n","2021-11-09 22:58:34,466 INFO streaming.PipeMapRed: R/W/S=1100000/1099884/0 in:18644=1100000/59 [rec/s] out:18642=1099884/59 [rec/s]\n","2021-11-09 22:58:34,525 INFO mapred.LocalJobRunner: Records R/W=931000/930760 > map\n","2021-11-09 22:58:34,715 INFO streaming.PipeMapRed: Records R/W=1105473/1105308\n","2021-11-09 22:58:39,737 INFO streaming.PipeMapRed: R/W/S=1200000/1199895/0 in:18461=1200000/65 [rec/s] out:18459=1199895/65 [rec/s]\n","2021-11-09 22:58:40,526 INFO mapred.LocalJobRunner: Records R/W=1105473/1105308 > map\n","2021-11-09 22:58:43,263 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:58:43,263 INFO mapred.MapTask: bufstart = 8796471; bufend = 88728550; bufvoid = 104857600\n","2021-11-09 22:58:43,263 INFO mapred.MapTask: kvstart = 2199112(8796448); kvend = 1210608(4842432); length = 988505/6553600\n","2021-11-09 22:58:43,264 INFO mapred.MapTask: (EQUATOR) 89742262 kvi 22435560(89742240)\n","2021-11-09 22:58:44,223 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 22:58:44,236 INFO mapred.MapTask: (RESET) equator 89742262 kv 22435560(89742240) kvi 22392260(89569040)\n","2021-11-09 22:58:44,719 INFO streaming.PipeMapRed: Records R/W=1284732/1284501\n","2021-11-09 22:58:45,521 INFO streaming.PipeMapRed: R/W/S=1300000/1299785/0 in:18571=1300000/70 [rec/s] out:18568=1299785/70 [rec/s]\n","2021-11-09 22:58:46,521 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 22:58:46,522 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 22:58:46,523 INFO mapred.LocalJobRunner: Records R/W=1105473/1105308 > map\n","2021-11-09 22:58:46,523 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 22:58:46,523 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:58:46,523 INFO mapred.MapTask: bufstart = 89742262; bufend = 1638185; bufvoid = 104857211\n","2021-11-09 22:58:46,523 INFO mapred.MapTask: kvstart = 22435560(89742240); kvend = 22229572(88918288); length = 205989/6553600\n","2021-11-09 22:58:46,526 INFO mapred.LocalJobRunner: Records R/W=1284732/1284501 > sort\n","2021-11-09 22:58:46,630 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 22:58:46,632 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 22:58:46,632 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 420411119 bytes\n","2021-11-09 22:58:48,905 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000038_0 is done. And is in the process of committing\n","2021-11-09 22:58:48,916 INFO mapred.LocalJobRunner: Records R/W=1284732/1284501 > sort\n","2021-11-09 22:58:48,917 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000038_0' done.\n","2021-11-09 22:58:48,917 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000038_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=16423183812\n","\t\tFILE: Number of bytes written=32846772762\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5243859968\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=81\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1316522\n","\t\tMap output records=1316522\n","\t\tMap output bytes=415942485\n","\t\tMap output materialized bytes=420412907\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2633044\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134507520\n","2021-11-09 22:58:48,917 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000038_0\n","2021-11-09 22:58:48,917 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000039_0\n","2021-11-09 22:58:48,918 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 22:58:48,918 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 22:58:48,918 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 22:58:48,919 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:5234491392+134217728\n","2021-11-09 22:58:48,967 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 22:58:48,980 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 22:58:48,980 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 22:58:48,980 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 22:58:48,980 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 22:58:48,980 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 22:58:48,983 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 22:58:48,993 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 22:58:49,014 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:58:49,015 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:58:49,031 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:58:49,092 INFO streaming.PipeMapRed: Records R/W=218/1\n","2021-11-09 22:58:49,129 INFO streaming.PipeMapRed: R/W/S=1000/848/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:58:49,687 INFO streaming.PipeMapRed: R/W/S=10000/9848/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 22:58:49,809 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 22:58:54,734 INFO streaming.PipeMapRed: R/W/S=100000/99898/0 in:20000=100000/5 [rec/s] out:19979=99898/5 [rec/s]\n","2021-11-09 22:58:59,094 INFO streaming.PipeMapRed: Records R/W=176503/176293\n","2021-11-09 22:59:00,394 INFO streaming.PipeMapRed: R/W/S=200000/199857/0 in:18181=200000/11 [rec/s] out:18168=199857/11 [rec/s]\n","2021-11-09 22:59:00,921 INFO mapred.LocalJobRunner: Records R/W=176503/176293 > map\n","2021-11-09 22:59:01,812 INFO mapreduce.Job:  map 95% reduce 0%\n","2021-11-09 22:59:03,006 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:59:03,006 INFO mapred.MapTask: bufstart = 0; bufend = 79918196; bufvoid = 104857600\n","2021-11-09 22:59:03,006 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25222412(100889648); length = 991985/6553600\n","2021-11-09 22:59:03,006 INFO mapred.MapTask: (EQUATOR) 80910916 kvi 20227724(80910896)\n","2021-11-09 22:59:03,996 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 22:59:04,038 INFO mapred.MapTask: (RESET) equator 80910916 kv 20227724(80910896) kvi 20186120(80744480)\n","2021-11-09 22:59:06,208 INFO streaming.PipeMapRed: R/W/S=300000/299893/0 in:17647=300000/17 [rec/s] out:17640=299893/17 [rec/s]\n","2021-11-09 22:59:06,922 INFO mapred.LocalJobRunner: Records R/W=176503/176293 > map\n","2021-11-09 22:59:07,814 INFO mapreduce.Job:  map 96% reduce 0%\n","2021-11-09 22:59:09,096 INFO streaming.PipeMapRed: Records R/W=352334/352091\n","2021-11-09 22:59:11,815 INFO streaming.PipeMapRed: R/W/S=400000/399839/0 in:18181=400000/22 [rec/s] out:18174=399839/22 [rec/s]\n","2021-11-09 22:59:12,922 INFO mapred.LocalJobRunner: Records R/W=352334/352091 > map\n","2021-11-09 22:59:17,309 INFO streaming.PipeMapRed: R/W/S=500000/499849/0 in:17857=500000/28 [rec/s] out:17851=499849/28 [rec/s]\n","2021-11-09 22:59:17,382 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:59:17,382 INFO mapred.MapTask: bufstart = 80910916; bufend = 55892103; bufvoid = 104857463\n","2021-11-09 22:59:17,382 INFO mapred.MapTask: kvstart = 20227724(80910896); kvend = 19215800(76863200); length = 1011925/6553600\n","2021-11-09 22:59:17,382 INFO mapred.MapTask: (EQUATOR) 56896695 kvi 14224168(56896672)\n","2021-11-09 22:59:18,319 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 22:59:18,323 INFO mapred.MapTask: (RESET) equator 56896695 kv 14224168(56896672) kvi 14179620(56718480)\n","2021-11-09 22:59:18,923 INFO mapred.LocalJobRunner: Records R/W=352334/352091 > map\n","2021-11-09 22:59:19,099 INFO streaming.PipeMapRed: Records R/W=525328/525092\n","2021-11-09 22:59:23,184 INFO streaming.PipeMapRed: R/W/S=600000/599862/0 in:17647=600000/34 [rec/s] out:17643=599862/34 [rec/s]\n","2021-11-09 22:59:24,923 INFO mapred.LocalJobRunner: Records R/W=525328/525092 > map\n","2021-11-09 22:59:28,512 INFO streaming.PipeMapRed: R/W/S=700000/699720/0 in:17948=700000/39 [rec/s] out:17941=699720/39 [rec/s]\n","2021-11-09 22:59:29,103 INFO streaming.PipeMapRed: Records R/W=710583/710329\n","2021-11-09 22:59:30,924 INFO mapred.LocalJobRunner: Records R/W=710583/710329 > map\n","2021-11-09 22:59:31,756 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:59:31,756 INFO mapred.MapTask: bufstart = 56896695; bufend = 31806966; bufvoid = 104857587\n","2021-11-09 22:59:31,756 INFO mapred.MapTask: kvstart = 14224168(56896672); kvend = 13194584(52778336); length = 1029585/6553600\n","2021-11-09 22:59:31,756 INFO mapred.MapTask: (EQUATOR) 32820678 kvi 8205164(32820656)\n","2021-11-09 22:59:32,678 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 22:59:32,679 INFO mapred.MapTask: (RESET) equator 32820678 kv 8205164(32820656) kvi 8162500(32650000)\n","2021-11-09 22:59:34,387 INFO streaming.PipeMapRed: R/W/S=800000/799890/0 in:17777=800000/45 [rec/s] out:17775=799890/45 [rec/s]\n","2021-11-09 22:59:36,924 INFO mapred.LocalJobRunner: Records R/W=710583/710329 > map\n","2021-11-09 22:59:39,106 INFO streaming.PipeMapRed: Records R/W=883906/883674\n","2021-11-09 22:59:39,941 INFO streaming.PipeMapRed: R/W/S=900000/899779/0 in:18000=900000/50 [rec/s] out:17995=899779/50 [rec/s]\n","2021-11-09 22:59:42,925 INFO mapred.LocalJobRunner: Records R/W=883906/883674 > map\n","2021-11-09 22:59:45,436 INFO streaming.PipeMapRed: R/W/S=1000000/999784/0 in:17857=1000000/56 [rec/s] out:17853=999784/56 [rec/s]\n","2021-11-09 22:59:46,853 INFO mapred.MapTask: Spilling map output\n","2021-11-09 22:59:46,853 INFO mapred.MapTask: bufstart = 32820678; bufend = 7595910; bufvoid = 104857431\n","2021-11-09 22:59:46,853 INFO mapred.MapTask: kvstart = 8205164(32820656); kvend = 7141860(28567440); length = 1063305/6553600\n","2021-11-09 22:59:46,853 INFO mapred.MapTask: (EQUATOR) 8622038 kvi 2155504(8622016)\n","2021-11-09 22:59:47,927 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 22:59:47,936 INFO mapred.MapTask: (RESET) equator 8622038 kv 2155504(8622016) kvi 2108972(8435888)\n","2021-11-09 22:59:48,925 INFO mapred.LocalJobRunner: Records R/W=883906/883674 > map\n","2021-11-09 22:59:49,107 INFO streaming.PipeMapRed: Records R/W=1055680/1055549\n","2021-11-09 22:59:51,443 INFO streaming.PipeMapRed: R/W/S=1100000/1099805/0 in:17741=1100000/62 [rec/s] out:17738=1099805/62 [rec/s]\n","2021-11-09 22:59:54,926 INFO mapred.LocalJobRunner: Records R/W=1055680/1055549 > map\n","2021-11-09 22:59:55,827 INFO mapreduce.Job:  map 97% reduce 0%\n","2021-11-09 22:59:57,088 INFO streaming.PipeMapRed: R/W/S=1200000/1199839/0 in:17647=1200000/68 [rec/s] out:17644=1199839/68 [rec/s]\n","2021-11-09 22:59:59,108 INFO streaming.PipeMapRed: Records R/W=1237029/1236819\n","2021-11-09 23:00:00,926 INFO mapred.LocalJobRunner: Records R/W=1237029/1236819 > map\n","2021-11-09 23:00:01,402 INFO mapred.MapTask: Spilling map output\n","2021-11-09 23:00:01,402 INFO mapred.MapTask: bufstart = 8622038; bufend = 88451263; bufvoid = 104857600\n","2021-11-09 23:00:01,402 INFO mapred.MapTask: kvstart = 2155504(8622016); kvend = 1141284(4565136); length = 1014221/6553600\n","2021-11-09 23:00:01,402 INFO mapred.MapTask: (EQUATOR) 89474255 kvi 22368556(89474224)\n","2021-11-09 23:00:02,422 INFO mapred.MapTask: Finished spill 4\n","2021-11-09 23:00:02,429 INFO mapred.MapTask: (RESET) equator 89474255 kv 22368556(89474224) kvi 22324308(89297232)\n","2021-11-09 23:00:03,034 INFO streaming.PipeMapRed: R/W/S=1300000/1299843/0 in:17567=1300000/74 [rec/s] out:17565=1299843/74 [rec/s]\n","2021-11-09 23:00:04,609 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 23:00:04,609 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 23:00:04,610 INFO mapred.LocalJobRunner: Records R/W=1237029/1236819 > map\n","2021-11-09 23:00:04,610 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 23:00:04,610 INFO mapred.MapTask: Spilling map output\n","2021-11-09 23:00:04,610 INFO mapred.MapTask: bufstart = 89474255; bufend = 224969; bufvoid = 104857010\n","2021-11-09 23:00:04,610 INFO mapred.MapTask: kvstart = 22368556(89474224); kvend = 22168528(88674112); length = 200029/6553600\n","2021-11-09 23:00:04,713 INFO mapred.MapTask: Finished spill 5\n","2021-11-09 23:00:04,714 INFO mapred.Merger: Merging 6 sorted segments\n","2021-11-09 23:00:04,715 INFO mapred.Merger: Down to the last merge-pass, with 6 segments left of total size: 419089051 bytes\n","2021-11-09 23:00:06,927 INFO mapred.LocalJobRunner: Records R/W=1237029/1236819 > sort > \n","2021-11-09 23:00:06,995 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000039_0 is done. And is in the process of committing\n","2021-11-09 23:00:07,003 INFO mapred.LocalJobRunner: Records R/W=1237029/1236819 > sort\n","2021-11-09 23:00:07,003 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000039_0' done.\n","2021-11-09 23:00:07,003 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000039_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=16842275091\n","\t\tFILE: Number of bytes written=33684954138\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5378290688\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=83\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1327767\n","\t\tMap output records=1327767\n","\t\tMap output bytes=414594316\n","\t\tMap output materialized bytes=419090657\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=2655534\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=134430720\n","2021-11-09 23:00:07,003 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000039_0\n","2021-11-09 23:00:07,003 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_m_000040_0\n","2021-11-09 23:00:07,004 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 23:00:07,004 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 23:00:07,004 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 23:00:07,005 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/compressed_data/RC_2015-01.bz2:5368709120+83704440\n","2021-11-09 23:00:07,046 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-09 23:00:07,071 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-09 23:00:07,071 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-09 23:00:07,071 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-09 23:00:07,071 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-09 23:00:07,071 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-09 23:00:07,072 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-09 23:00:07,081 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-09 23:00:07,106 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 23:00:07,106 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 23:00:07,110 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 23:00:07,169 INFO streaming.PipeMapRed: Records R/W=214/1\n","2021-11-09 23:00:07,197 INFO streaming.PipeMapRed: R/W/S=1000/822/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 23:00:07,698 INFO streaming.PipeMapRed: R/W/S=10000/9856/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 23:00:07,831 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 23:00:12,678 INFO streaming.PipeMapRed: R/W/S=100000/99878/0 in:20000=100000/5 [rec/s] out:19975=99878/5 [rec/s]\n","2021-11-09 23:00:13,007 INFO mapred.LocalJobRunner: Records R/W=214/1 > map\n","2021-11-09 23:00:13,832 INFO mapreduce.Job:  map 98% reduce 0%\n","2021-11-09 23:00:17,169 INFO streaming.PipeMapRed: Records R/W=181617/181351\n","2021-11-09 23:00:18,090 INFO streaming.PipeMapRed: R/W/S=200000/199860/0 in:18181=200000/11 [rec/s] out:18169=199860/11 [rec/s]\n","2021-11-09 23:00:19,007 INFO mapred.LocalJobRunner: Records R/W=181617/181351 > map\n","2021-11-09 23:00:21,167 INFO mapred.MapTask: Spilling map output\n","2021-11-09 23:00:21,167 INFO mapred.MapTask: bufstart = 0; bufend = 79797598; bufvoid = 104857600\n","2021-11-09 23:00:21,167 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25192272(100769088); length = 1022125/6553600\n","2021-11-09 23:00:21,167 INFO mapred.MapTask: (EQUATOR) 80820590 kvi 20205140(80820560)\n","2021-11-09 23:00:22,184 INFO mapred.MapTask: Finished spill 0\n","2021-11-09 23:00:22,189 INFO mapred.MapTask: (RESET) equator 80820590 kv 20205140(80820560) kvi 20162112(80648448)\n","2021-11-09 23:00:23,946 INFO streaming.PipeMapRed: R/W/S=300000/299877/0 in:18750=300000/16 [rec/s] out:18742=299877/16 [rec/s]\n","2021-11-09 23:00:25,008 INFO mapred.LocalJobRunner: Records R/W=181617/181351 > map\n","2021-11-09 23:00:27,189 INFO streaming.PipeMapRed: Records R/W=358493/358270\n","2021-11-09 23:00:29,463 INFO streaming.PipeMapRed: R/W/S=400000/399764/0 in:18181=400000/22 [rec/s] out:18171=399764/22 [rec/s]\n","2021-11-09 23:00:31,008 INFO mapred.LocalJobRunner: Records R/W=358493/358270 > map\n","2021-11-09 23:00:34,615 INFO streaming.PipeMapRed: R/W/S=500000/499794/0 in:18518=500000/27 [rec/s] out:18510=499794/27 [rec/s]\n","2021-11-09 23:00:35,306 INFO mapred.MapTask: Spilling map output\n","2021-11-09 23:00:35,306 INFO mapred.MapTask: bufstart = 80820590; bufend = 55737989; bufvoid = 104856893\n","2021-11-09 23:00:35,306 INFO mapred.MapTask: kvstart = 20205140(80820560); kvend = 19177360(76709440); length = 1027781/6553600\n","2021-11-09 23:00:35,306 INFO mapred.MapTask: (EQUATOR) 56764101 kvi 14191020(56764080)\n","2021-11-09 23:00:36,282 INFO mapred.MapTask: Finished spill 1\n","2021-11-09 23:00:36,285 INFO mapred.MapTask: (RESET) equator 56764101 kv 14191020(56764080) kvi 14145656(56582624)\n","2021-11-09 23:00:37,009 INFO mapred.LocalJobRunner: Records R/W=358493/358270 > map\n","2021-11-09 23:00:37,190 INFO streaming.PipeMapRed: Records R/W=540727/540591\n","2021-11-09 23:00:37,839 INFO mapreduce.Job:  map 99% reduce 0%\n","2021-11-09 23:00:40,276 INFO streaming.PipeMapRed: R/W/S=600000/599873/0 in:18181=600000/33 [rec/s] out:18177=599873/33 [rec/s]\n","2021-11-09 23:00:43,010 INFO mapred.LocalJobRunner: Records R/W=540727/540591 > map\n","2021-11-09 23:00:45,826 INFO streaming.PipeMapRed: R/W/S=700000/699778/0 in:18421=700000/38 [rec/s] out:18415=699778/38 [rec/s]\n","2021-11-09 23:00:47,191 INFO streaming.PipeMapRed: Records R/W=727267/727090\n","2021-11-09 23:00:49,010 INFO mapred.LocalJobRunner: Records R/W=727267/727090 > map\n","2021-11-09 23:00:49,584 INFO mapred.MapTask: Spilling map output\n","2021-11-09 23:00:49,584 INFO mapred.MapTask: bufstart = 56764101; bufend = 31646854; bufvoid = 104857500\n","2021-11-09 23:00:49,584 INFO mapred.MapTask: kvstart = 14191020(56764080); kvend = 13154480(52617920); length = 1036541/6553600\n","2021-11-09 23:00:49,584 INFO mapred.MapTask: (EQUATOR) 32676102 kvi 8169020(32676080)\n","2021-11-09 23:00:50,610 INFO mapred.MapTask: Finished spill 2\n","2021-11-09 23:00:50,612 INFO mapred.MapTask: (RESET) equator 32676102 kv 8169020(32676080) kvi 8121612(32486448)\n","2021-11-09 23:00:51,470 INFO streaming.PipeMapRed: R/W/S=800000/799862/0 in:18181=800000/44 [rec/s] out:18178=799862/44 [rec/s]\n","2021-11-09 23:00:53,656 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-09 23:00:53,656 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-09 23:00:53,657 INFO mapred.LocalJobRunner: Records R/W=727267/727090 > map\n","2021-11-09 23:00:53,657 INFO mapred.MapTask: Starting flush of map output\n","2021-11-09 23:00:53,657 INFO mapred.MapTask: Spilling map output\n","2021-11-09 23:00:53,657 INFO mapred.MapTask: bufstart = 32676102; bufend = 52948163; bufvoid = 104857600\n","2021-11-09 23:00:53,657 INFO mapred.MapTask: kvstart = 8169020(32676080); kvend = 7902992(31611968); length = 266029/6553600\n","2021-11-09 23:00:53,806 INFO mapred.MapTask: Finished spill 3\n","2021-11-09 23:00:53,807 INFO mapred.Merger: Merging 4 sorted segments\n","2021-11-09 23:00:53,808 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 262419272 bytes\n","2021-11-09 23:00:55,011 INFO mapred.LocalJobRunner: Records R/W=727267/727090 > sort > \n","2021-11-09 23:00:55,226 INFO mapred.Task: Task:attempt_local1236956871_0001_m_000040_0 is done. And is in the process of committing\n","2021-11-09 23:00:55,234 INFO mapred.LocalJobRunner: Records R/W=727267/727090 > sort\n","2021-11-09 23:00:55,234 INFO mapred.Task: Task 'attempt_local1236956871_0001_m_000040_0' done.\n","2021-11-09 23:00:55,235 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_m_000040_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=17104696592\n","\t\tFILE: Number of bytes written=34209795970\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5461995128\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=85\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=838122\n","\t\tMap output records=838122\n","\t\tMap output bytes=259584204\n","\t\tMap output materialized bytes=262420891\n","\t\tInput split bytes=113\n","\t\tCombine input records=0\n","\t\tSpilled Records=1676244\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=4\n","\t\tTotal committed heap usage (bytes)=421527552\n","\tFile Input Format Counters \n","\t\tBytes Read=83704440\n","2021-11-09 23:00:55,235 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_m_000040_0\n","2021-11-09 23:00:55,235 INFO mapred.LocalJobRunner: map task executor complete.\n","2021-11-09 23:00:55,240 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2021-11-09 23:00:55,242 INFO mapred.LocalJobRunner: Starting task: attempt_local1236956871_0001_r_000000_0\n","2021-11-09 23:00:55,254 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-09 23:00:55,254 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-09 23:00:55,254 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-09 23:00:55,257 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@117c41cb\n","2021-11-09 23:00:55,260 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2021-11-09 23:00:55,299 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2021-11-09 23:00:55,313 INFO reduce.EventFetcher: attempt_local1236956871_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2021-11-09 23:00:55,531 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000026_0 decomp: 421709797 len: 421709801 to MEMORY\n","2021-11-09 23:00:55,846 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-09 23:01:03,367 INFO reduce.InMemoryMapOutput: Read 421709797 bytes from map-output for attempt_local1236956871_0001_m_000026_0\n","2021-11-09 23:01:03,368 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 421709797, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->421709797\n","2021-11-09 23:01:03,589 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000013_0 decomp: 423738867 len: 423738871 to MEMORY\n","2021-11-09 23:01:07,262 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000026_0 succeeded at 402173.81 MB/s) Aggregated copy rate(1 of 41 at 402173.81 MB/s)\n","2021-11-09 23:01:07,852 INFO mapreduce.Job:  map 100% reduce 1%\n","2021-11-09 23:01:11,396 INFO reduce.InMemoryMapOutput: Read 423738867 bytes from map-output for attempt_local1236956871_0001_m_000013_0\n","2021-11-09 23:01:11,396 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 423738867, inMemoryMapOutputs.size() -> 2, commitMemory -> 421709797, usedMemory ->845448664\n","2021-11-09 23:01:11,612 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000000_0 decomp: 423194213 len: 423194217 to MEMORY\n","2021-11-09 23:01:13,262 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000013_0 succeeded at 404108.88 MB/s) Aggregated copy rate(2 of 41 at 806282.69 MB/s)\n","2021-11-09 23:01:13,855 INFO mapreduce.Job:  map 100% reduce 2%\n","2021-11-09 23:01:19,263 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000013_0 succeeded at 404108.88 MB/s) Aggregated copy rate(2 of 41 at 806282.69 MB/s)\n","2021-11-09 23:01:19,444 INFO reduce.InMemoryMapOutput: Read 423194213 bytes from map-output for attempt_local1236956871_0001_m_000000_0\n","2021-11-09 23:01:19,444 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 423194213, inMemoryMapOutputs.size() -> 3, commitMemory -> 845448664, usedMemory ->1268642877\n","2021-11-09 23:01:19,669 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000038_0 decomp: 420412903 len: 420412907 to MEMORY\n","2021-11-09 23:01:20,062 INFO reduce.InMemoryMapOutput: Read 420412903 bytes from map-output for attempt_local1236956871_0001_m_000038_0\n","2021-11-09 23:01:20,062 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420412903, inMemoryMapOutputs.size() -> 4, commitMemory -> 1268642877, usedMemory ->1689055780\n","2021-11-09 23:01:20,062 INFO reduce.MergeManagerImpl: Starting inMemoryMerger's merge since commitMemory=1689055780 > mergeThreshold=1573467904. Current usedMemory=1689055780\n","2021-11-09 23:01:20,062 INFO reduce.MergeThread: InMemoryMerger - Thread to merge in-memory shuffled map-outputs: Starting merge with 4 segments, while ignoring 0 segments\n","2021-11-09 23:01:20,065 INFO reduce.MergeManagerImpl: Initiating in-memory merge with 4 segments...\n","2021-11-09 23:01:20,065 INFO mapred.Merger: Merging 4 sorted segments\n","2021-11-09 23:01:20,069 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 1689055041 bytes\n","2021-11-09 23:01:20,316 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000025_0 decomp: 418036330 len: 418036334 to MEMORY\n","2021-11-09 23:01:25,274 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000038_0 succeeded at 400936.97 MB/s) Aggregated copy rate(4 of 41 at 1610809.12 MB/s)\n","2021-11-09 23:01:25,867 INFO mapreduce.Job:  map 100% reduce 3%\n","2021-11-09 23:01:27,540 INFO reduce.MergeManagerImpl: attempt_local1236956871_0001_r_000000_0 Merge of the 4 files in-memory complete. Local file is /tmp/hadoop-root/mapred/local/localRunner/root/jobcache/job_local1236956871_0001/attempt_local1236956871_0001_r_000000_0/output/map_38.out.merged of size 1689055778\n","2021-11-09 23:01:31,274 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000038_0 succeeded at 400936.97 MB/s) Aggregated copy rate(4 of 41 at 1610809.12 MB/s)\n","2021-11-09 23:01:31,937 INFO reduce.InMemoryMapOutput: Read 418036330 bytes from map-output for attempt_local1236956871_0001_m_000025_0\n","2021-11-09 23:01:31,937 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 418036330, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->418036330\n","2021-11-09 23:01:32,000 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000012_0 decomp: 422749728 len: 422749732 to MEMORY\n","2021-11-09 23:01:37,275 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000025_0 succeeded at 398670.50 MB/s) Aggregated copy rate(5 of 41 at 2009479.62 MB/s)\n","2021-11-09 23:01:37,872 INFO mapreduce.Job:  map 100% reduce 4%\n","2021-11-09 23:01:39,849 INFO reduce.InMemoryMapOutput: Read 422749728 bytes from map-output for attempt_local1236956871_0001_m_000012_0\n","2021-11-09 23:01:39,850 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 422749728, inMemoryMapOutputs.size() -> 2, commitMemory -> 418036330, usedMemory ->840786058\n","2021-11-09 23:01:39,914 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000037_0 decomp: 420537853 len: 420537857 to MEMORY\n","2021-11-09 23:01:43,276 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000012_0 succeeded at 403165.56 MB/s) Aggregated copy rate(6 of 41 at 2412645.25 MB/s)\n","2021-11-09 23:01:43,875 INFO mapreduce.Job:  map 100% reduce 5%\n","2021-11-09 23:01:47,651 INFO reduce.InMemoryMapOutput: Read 420537853 bytes from map-output for attempt_local1236956871_0001_m_000037_0\n","2021-11-09 23:01:47,651 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420537853, inMemoryMapOutputs.size() -> 3, commitMemory -> 840786058, usedMemory ->1261323911\n","2021-11-09 23:01:47,713 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000024_0 decomp: 423287512 len: 423287516 to MEMORY\n","2021-11-09 23:01:49,276 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000037_0 succeeded at 401056.16 MB/s) Aggregated copy rate(7 of 41 at 2813701.50 MB/s)\n","2021-11-09 23:01:49,877 INFO mapreduce.Job:  map 100% reduce 6%\n","2021-11-09 23:01:55,277 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000037_0 succeeded at 401056.16 MB/s) Aggregated copy rate(7 of 41 at 2813701.50 MB/s)\n","2021-11-09 23:01:55,549 INFO reduce.InMemoryMapOutput: Read 423287512 bytes from map-output for attempt_local1236956871_0001_m_000024_0\n","2021-11-09 23:01:55,550 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 423287512, inMemoryMapOutputs.size() -> 4, commitMemory -> 1261323911, usedMemory ->1684611423\n","2021-11-09 23:01:55,550 INFO reduce.MergeManagerImpl: Starting inMemoryMerger's merge since commitMemory=1684611423 > mergeThreshold=1573467904. Current usedMemory=1684611423\n","2021-11-09 23:01:55,550 INFO reduce.MergeThread: InMemoryMerger - Thread to merge in-memory shuffled map-outputs: Starting merge with 4 segments, while ignoring 0 segments\n","2021-11-09 23:01:55,557 INFO reduce.MergeManagerImpl: Initiating in-memory merge with 4 segments...\n","2021-11-09 23:01:55,557 INFO mapred.Merger: Merging 4 sorted segments\n","2021-11-09 23:01:55,557 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 1684608135 bytes\n","2021-11-09 23:01:55,641 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000011_0 decomp: 425658065 len: 425658069 to MEMORY\n","2021-11-09 23:02:01,277 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000024_0 succeeded at 403678.44 MB/s) Aggregated copy rate(8 of 41 at 3217380.00 MB/s)\n","2021-11-09 23:02:01,888 INFO mapreduce.Job:  map 100% reduce 7%\n","2021-11-09 23:02:02,851 INFO reduce.MergeManagerImpl: attempt_local1236956871_0001_r_000000_0 Merge of the 4 files in-memory complete. Local file is /tmp/hadoop-root/mapred/local/localRunner/root/jobcache/job_local1236956871_0001/attempt_local1236956871_0001_r_000000_0/output/map_25.out.merged of size 1684611421\n","2021-11-09 23:02:07,196 INFO reduce.InMemoryMapOutput: Read 425658065 bytes from map-output for attempt_local1236956871_0001_m_000011_0\n","2021-11-09 23:02:07,196 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 425658065, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->425658065\n","2021-11-09 23:02:07,256 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000036_0 decomp: 419713116 len: 419713120 to MEMORY\n","2021-11-09 23:02:07,278 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000011_0 succeeded at 405939.19 MB/s) Aggregated copy rate(9 of 41 at 3623318.75 MB/s)\n","2021-11-09 23:02:13,279 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000011_0 succeeded at 405939.19 MB/s) Aggregated copy rate(9 of 41 at 3623318.75 MB/s)\n","2021-11-09 23:02:15,014 INFO reduce.InMemoryMapOutput: Read 419713116 bytes from map-output for attempt_local1236956871_0001_m_000036_0\n","2021-11-09 23:02:15,014 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 419713116, inMemoryMapOutputs.size() -> 2, commitMemory -> 425658065, usedMemory ->845371181\n","2021-11-09 23:02:15,077 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000023_0 decomp: 424381006 len: 424381010 to MEMORY\n","2021-11-09 23:02:19,279 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000036_0 succeeded at 400269.62 MB/s) Aggregated copy rate(10 of 41 at 4023588.50 MB/s)\n","2021-11-09 23:02:19,895 INFO mapreduce.Job:  map 100% reduce 8%\n","2021-11-09 23:02:22,997 INFO reduce.InMemoryMapOutput: Read 424381006 bytes from map-output for attempt_local1236956871_0001_m_000023_0\n","2021-11-09 23:02:22,997 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 424381006, inMemoryMapOutputs.size() -> 3, commitMemory -> 845371181, usedMemory ->1269752187\n","2021-11-09 23:02:23,060 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000010_0 decomp: 419604382 len: 419604386 to MEMORY\n","2021-11-09 23:02:25,280 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000023_0 succeeded at 404721.28 MB/s) Aggregated copy rate(11 of 41 at 4428310.00 MB/s)\n","2021-11-09 23:02:25,897 INFO mapreduce.Job:  map 100% reduce 9%\n","2021-11-09 23:02:30,809 INFO reduce.InMemoryMapOutput: Read 419604382 bytes from map-output for attempt_local1236956871_0001_m_000010_0\n","2021-11-09 23:02:30,810 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 419604382, inMemoryMapOutputs.size() -> 4, commitMemory -> 1269752187, usedMemory ->1689356569\n","2021-11-09 23:02:30,810 INFO reduce.MergeManagerImpl: Starting inMemoryMerger's merge since commitMemory=1689356569 > mergeThreshold=1573467904. Current usedMemory=1689356569\n","2021-11-09 23:02:30,810 INFO reduce.MergeThread: InMemoryMerger - Thread to merge in-memory shuffled map-outputs: Starting merge with 4 segments, while ignoring 0 segments\n","2021-11-09 23:02:30,811 INFO reduce.MergeManagerImpl: Initiating in-memory merge with 4 segments...\n","2021-11-09 23:02:30,812 INFO mapred.Merger: Merging 4 sorted segments\n","2021-11-09 23:02:30,812 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 1689355560 bytes\n","2021-11-09 23:02:30,945 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000035_0 decomp: 418792653 len: 418792657 to MEMORY\n","2021-11-09 23:02:31,283 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000010_0 succeeded at 400165.94 MB/s) Aggregated copy rate(12 of 41 at 4828475.50 MB/s)\n","2021-11-09 23:02:31,945 INFO mapreduce.Job:  map 100% reduce 10%\n","2021-11-09 23:02:37,283 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000010_0 succeeded at 400165.94 MB/s) Aggregated copy rate(12 of 41 at 4828475.50 MB/s)\n","2021-11-09 23:02:38,255 INFO reduce.MergeManagerImpl: attempt_local1236956871_0001_r_000000_0 Merge of the 4 files in-memory complete. Local file is /tmp/hadoop-root/mapred/local/localRunner/root/jobcache/job_local1236956871_0001/attempt_local1236956871_0001_r_000000_0/output/map_10.out.merged of size 1689356567\n","2021-11-09 23:02:42,771 INFO reduce.InMemoryMapOutput: Read 418792653 bytes from map-output for attempt_local1236956871_0001_m_000035_0\n","2021-11-09 23:02:42,771 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 418792653, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->418792653\n","2021-11-09 23:02:42,835 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000022_0 decomp: 423098857 len: 423098861 to MEMORY\n","2021-11-09 23:02:43,284 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000035_0 succeeded at 399391.81 MB/s) Aggregated copy rate(13 of 41 at 5227867.50 MB/s)\n","2021-11-09 23:02:43,951 INFO mapreduce.Job:  map 100% reduce 11%\n","2021-11-09 23:02:49,285 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000035_0 succeeded at 399391.81 MB/s) Aggregated copy rate(13 of 41 at 5227867.50 MB/s)\n","2021-11-09 23:02:50,569 INFO reduce.InMemoryMapOutput: Read 423098857 bytes from map-output for attempt_local1236956871_0001_m_000022_0\n","2021-11-09 23:02:50,569 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 423098857, inMemoryMapOutputs.size() -> 2, commitMemory -> 418792653, usedMemory ->841891510\n","2021-11-09 23:02:50,619 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000009_0 decomp: 420750204 len: 420750208 to MEMORY\n","2021-11-09 23:02:55,285 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000022_0 succeeded at 403498.50 MB/s) Aggregated copy rate(14 of 41 at 5631366.00 MB/s)\n","2021-11-09 23:02:58,482 INFO reduce.InMemoryMapOutput: Read 420750204 bytes from map-output for attempt_local1236956871_0001_m_000009_0\n","2021-11-09 23:02:58,482 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420750204, inMemoryMapOutputs.size() -> 3, commitMemory -> 841891510, usedMemory ->1262641714\n","2021-11-09 23:02:58,599 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000034_0 decomp: 421447107 len: 421447111 to MEMORY\n","2021-11-09 23:03:01,287 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000009_0 succeeded at 401258.66 MB/s) Aggregated copy rate(15 of 41 at 6032625.00 MB/s)\n","2021-11-09 23:03:01,958 INFO mapreduce.Job:  map 100% reduce 12%\n","2021-11-09 23:03:06,417 INFO reduce.InMemoryMapOutput: Read 421447107 bytes from map-output for attempt_local1236956871_0001_m_000034_0\n","2021-11-09 23:03:06,417 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 421447107, inMemoryMapOutputs.size() -> 4, commitMemory -> 1262641714, usedMemory ->1684088821\n","2021-11-09 23:03:06,417 INFO reduce.MergeManagerImpl: Starting inMemoryMerger's merge since commitMemory=1684088821 > mergeThreshold=1573467904. Current usedMemory=1684088821\n","2021-11-09 23:03:06,417 INFO reduce.MergeThread: InMemoryMerger - Thread to merge in-memory shuffled map-outputs: Starting merge with 4 segments, while ignoring 0 segments\n","2021-11-09 23:03:06,419 INFO reduce.MergeManagerImpl: Initiating in-memory merge with 4 segments...\n","2021-11-09 23:03:06,419 INFO mapred.Merger: Merging 4 sorted segments\n","2021-11-09 23:03:06,419 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 1684087642 bytes\n","2021-11-09 23:03:06,476 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000021_0 decomp: 420329180 len: 420329184 to MEMORY\n","2021-11-09 23:03:07,288 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000034_0 succeeded at 401923.28 MB/s) Aggregated copy rate(16 of 41 at 6434548.00 MB/s)\n","2021-11-09 23:03:07,985 INFO mapreduce.Job:  map 100% reduce 13%\n","2021-11-09 23:03:13,290 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000034_0 succeeded at 401923.28 MB/s) Aggregated copy rate(16 of 41 at 6434548.00 MB/s)\n","2021-11-09 23:03:14,002 INFO reduce.MergeManagerImpl: attempt_local1236956871_0001_r_000000_0 Merge of the 4 files in-memory complete. Local file is /tmp/hadoop-root/mapred/local/localRunner/root/jobcache/job_local1236956871_0001/attempt_local1236956871_0001_r_000000_0/output/map_35.out.merged of size 1684088819\n","2021-11-09 23:03:15,868 INFO reduce.InMemoryMapOutput: Read 420329180 bytes from map-output for attempt_local1236956871_0001_m_000021_0\n","2021-11-09 23:03:15,868 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420329180, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->420329180\n","2021-11-09 23:03:15,927 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000008_0 decomp: 419462940 len: 419462944 to MEMORY\n","2021-11-09 23:03:19,291 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000021_0 succeeded at 400857.16 MB/s) Aggregated copy rate(17 of 41 at 6835405.50 MB/s)\n","2021-11-09 23:03:20,001 INFO mapreduce.Job:  map 100% reduce 14%\n","2021-11-09 23:03:23,679 INFO reduce.InMemoryMapOutput: Read 419462940 bytes from map-output for attempt_local1236956871_0001_m_000008_0\n","2021-11-09 23:03:23,679 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 419462940, inMemoryMapOutputs.size() -> 2, commitMemory -> 420329180, usedMemory ->839792120\n","2021-11-09 23:03:23,719 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000033_0 decomp: 418787406 len: 418787410 to MEMORY\n","2021-11-09 23:03:25,291 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000008_0 succeeded at 400031.03 MB/s) Aggregated copy rate(18 of 41 at 7235436.00 MB/s)\n","2021-11-09 23:03:26,003 INFO mapreduce.Job:  map 100% reduce 15%\n","2021-11-09 23:03:31,292 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000008_0 succeeded at 400031.03 MB/s) Aggregated copy rate(18 of 41 at 7235436.00 MB/s)\n","2021-11-09 23:03:31,458 INFO reduce.InMemoryMapOutput: Read 418787406 bytes from map-output for attempt_local1236956871_0001_m_000033_0\n","2021-11-09 23:03:31,458 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 418787406, inMemoryMapOutputs.size() -> 3, commitMemory -> 839792120, usedMemory ->1258579526\n","2021-11-09 23:03:31,521 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000020_0 decomp: 418943620 len: 418943624 to MEMORY\n","2021-11-09 23:03:37,293 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000033_0 succeeded at 399386.81 MB/s) Aggregated copy rate(19 of 41 at 7634823.00 MB/s)\n","2021-11-09 23:03:39,266 INFO reduce.InMemoryMapOutput: Read 418943620 bytes from map-output for attempt_local1236956871_0001_m_000020_0\n","2021-11-09 23:03:39,266 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 418943620, inMemoryMapOutputs.size() -> 4, commitMemory -> 1258579526, usedMemory ->1677523146\n","2021-11-09 23:03:39,266 INFO reduce.MergeManagerImpl: Starting inMemoryMerger's merge since commitMemory=1677523146 > mergeThreshold=1573467904. Current usedMemory=1677523146\n","2021-11-09 23:03:39,266 INFO reduce.MergeThread: InMemoryMerger - Thread to merge in-memory shuffled map-outputs: Starting merge with 4 segments, while ignoring 0 segments\n","2021-11-09 23:03:39,268 INFO reduce.MergeManagerImpl: Initiating in-memory merge with 4 segments...\n","2021-11-09 23:03:39,268 INFO mapred.Merger: Merging 4 sorted segments\n","2021-11-09 23:03:39,268 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 1677522380 bytes\n","2021-11-09 23:03:39,315 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000007_0 decomp: 420029954 len: 420029958 to MEMORY\n","2021-11-09 23:03:43,295 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000020_0 succeeded at 399535.78 MB/s) Aggregated copy rate(20 of 41 at 8034359.00 MB/s)\n","2021-11-09 23:03:44,010 INFO mapreduce.Job:  map 100% reduce 16%\n","2021-11-09 23:03:46,427 INFO reduce.MergeManagerImpl: attempt_local1236956871_0001_r_000000_0 Merge of the 4 files in-memory complete. Local file is /tmp/hadoop-root/mapred/local/localRunner/root/jobcache/job_local1236956871_0001/attempt_local1236956871_0001_r_000000_0/output/map_33.out.merged of size 1677523144\n","2021-11-09 23:03:49,296 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000020_0 succeeded at 399535.78 MB/s) Aggregated copy rate(20 of 41 at 8034359.00 MB/s)\n","2021-11-09 23:03:50,866 INFO reduce.InMemoryMapOutput: Read 420029954 bytes from map-output for attempt_local1236956871_0001_m_000007_0\n","2021-11-09 23:03:50,866 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420029954, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->420029954\n","2021-11-09 23:03:50,924 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000032_0 decomp: 422509189 len: 422509193 to MEMORY\n","2021-11-09 23:03:55,296 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000007_0 succeeded at 400571.78 MB/s) Aggregated copy rate(21 of 41 at 8434931.00 MB/s)\n","2021-11-09 23:03:56,021 INFO mapreduce.Job:  map 100% reduce 17%\n","2021-11-09 23:03:58,687 INFO reduce.InMemoryMapOutput: Read 422509189 bytes from map-output for attempt_local1236956871_0001_m_000032_0\n","2021-11-09 23:03:58,687 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 422509189, inMemoryMapOutputs.size() -> 2, commitMemory -> 420029954, usedMemory ->842539143\n","2021-11-09 23:03:58,729 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000019_0 decomp: 419875631 len: 419875635 to MEMORY\n","2021-11-09 23:04:01,297 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000032_0 succeeded at 402936.16 MB/s) Aggregated copy rate(22 of 41 at 8837867.00 MB/s)\n","2021-11-09 23:04:02,023 INFO mapreduce.Job:  map 100% reduce 18%\n","2021-11-09 23:04:06,517 INFO reduce.InMemoryMapOutput: Read 419875631 bytes from map-output for attempt_local1236956871_0001_m_000019_0\n","2021-11-09 23:04:06,517 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 419875631, inMemoryMapOutputs.size() -> 3, commitMemory -> 842539143, usedMemory ->1262414774\n","2021-11-09 23:04:06,572 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000006_0 decomp: 420871449 len: 420871453 to MEMORY\n","2021-11-09 23:04:07,297 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000019_0 succeeded at 400424.62 MB/s) Aggregated copy rate(23 of 41 at 9238291.00 MB/s)\n","2021-11-09 23:04:08,025 INFO mapreduce.Job:  map 100% reduce 19%\n","2021-11-09 23:04:13,299 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000019_0 succeeded at 400424.62 MB/s) Aggregated copy rate(23 of 41 at 9238291.00 MB/s)\n","2021-11-09 23:04:14,319 INFO reduce.InMemoryMapOutput: Read 420871449 bytes from map-output for attempt_local1236956871_0001_m_000006_0\n","2021-11-09 23:04:14,320 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420871449, inMemoryMapOutputs.size() -> 4, commitMemory -> 1262414774, usedMemory ->1683286223\n","2021-11-09 23:04:14,320 INFO reduce.MergeManagerImpl: Starting inMemoryMerger's merge since commitMemory=1683286223 > mergeThreshold=1573467904. Current usedMemory=1683286223\n","2021-11-09 23:04:14,320 INFO reduce.MergeThread: InMemoryMerger - Thread to merge in-memory shuffled map-outputs: Starting merge with 4 segments, while ignoring 0 segments\n","2021-11-09 23:04:14,321 INFO reduce.MergeManagerImpl: Initiating in-memory merge with 4 segments...\n","2021-11-09 23:04:14,321 INFO mapred.Merger: Merging 4 sorted segments\n","2021-11-09 23:04:14,321 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 1683285240 bytes\n","2021-11-09 23:04:14,378 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000031_0 decomp: 422943086 len: 422943090 to MEMORY\n","2021-11-09 23:04:19,300 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000006_0 succeeded at 401374.31 MB/s) Aggregated copy rate(24 of 41 at 9639666.00 MB/s)\n","2021-11-09 23:04:20,029 INFO mapreduce.Job:  map 100% reduce 20%\n","2021-11-09 23:04:21,827 INFO reduce.MergeManagerImpl: attempt_local1236956871_0001_r_000000_0 Merge of the 4 files in-memory complete. Local file is /tmp/hadoop-root/mapred/local/localRunner/root/jobcache/job_local1236956871_0001/attempt_local1236956871_0001_r_000000_0/output/map_19.out.merged of size 1683286221\n","2021-11-09 23:04:25,300 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000006_0 succeeded at 401374.31 MB/s) Aggregated copy rate(24 of 41 at 9639666.00 MB/s)\n","2021-11-09 23:04:26,136 INFO reduce.InMemoryMapOutput: Read 422943086 bytes from map-output for attempt_local1236956871_0001_m_000031_0\n","2021-11-09 23:04:26,137 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 422943086, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->422943086\n","2021-11-09 23:04:26,198 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000018_0 decomp: 419465910 len: 419465914 to MEMORY\n","2021-11-09 23:04:31,301 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000031_0 succeeded at 403349.97 MB/s) Aggregated copy rate(25 of 41 at 10043016.00 MB/s)\n","2021-11-09 23:04:33,940 INFO reduce.InMemoryMapOutput: Read 419465910 bytes from map-output for attempt_local1236956871_0001_m_000018_0\n","2021-11-09 23:04:33,940 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 419465910, inMemoryMapOutputs.size() -> 2, commitMemory -> 422943086, usedMemory ->842408996\n","2021-11-09 23:04:33,981 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000005_0 decomp: 420665963 len: 420665967 to MEMORY\n","2021-11-09 23:04:37,302 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000018_0 succeeded at 400033.88 MB/s) Aggregated copy rate(26 of 41 at 10443050.00 MB/s)\n","2021-11-09 23:04:38,036 INFO mapreduce.Job:  map 100% reduce 21%\n","2021-11-09 23:04:41,761 INFO reduce.InMemoryMapOutput: Read 420665963 bytes from map-output for attempt_local1236956871_0001_m_000005_0\n","2021-11-09 23:04:41,761 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420665963, inMemoryMapOutputs.size() -> 3, commitMemory -> 842408996, usedMemory ->1263074959\n","2021-11-09 23:04:41,819 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000030_0 decomp: 420484375 len: 420484379 to MEMORY\n","2021-11-09 23:04:43,302 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000005_0 succeeded at 401178.31 MB/s) Aggregated copy rate(27 of 41 at 10844228.00 MB/s)\n","2021-11-09 23:04:44,039 INFO mapreduce.Job:  map 100% reduce 22%\n","2021-11-09 23:04:49,303 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000005_0 succeeded at 401178.31 MB/s) Aggregated copy rate(27 of 41 at 10844228.00 MB/s)\n","2021-11-09 23:04:49,639 INFO reduce.InMemoryMapOutput: Read 420484375 bytes from map-output for attempt_local1236956871_0001_m_000030_0\n","2021-11-09 23:04:49,639 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420484375, inMemoryMapOutputs.size() -> 4, commitMemory -> 1263074959, usedMemory ->1683559334\n","2021-11-09 23:04:49,639 INFO reduce.MergeManagerImpl: Starting inMemoryMerger's merge since commitMemory=1683559334 > mergeThreshold=1573467904. Current usedMemory=1683559334\n","2021-11-09 23:04:49,639 INFO reduce.MergeThread: InMemoryMerger - Thread to merge in-memory shuffled map-outputs: Starting merge with 4 segments, while ignoring 0 segments\n","2021-11-09 23:04:49,640 INFO reduce.MergeManagerImpl: Initiating in-memory merge with 4 segments...\n","2021-11-09 23:04:49,640 INFO mapred.Merger: Merging 4 sorted segments\n","2021-11-09 23:04:49,640 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 1683557796 bytes\n","2021-11-09 23:04:49,736 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000017_0 decomp: 418955133 len: 418955137 to MEMORY\n","2021-11-09 23:04:55,304 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000030_0 succeeded at 401005.16 MB/s) Aggregated copy rate(28 of 41 at 11245233.00 MB/s)\n","2021-11-09 23:04:56,056 INFO mapreduce.Job:  map 100% reduce 23%\n","2021-11-09 23:04:56,877 INFO reduce.MergeManagerImpl: attempt_local1236956871_0001_r_000000_0 Merge of the 4 files in-memory complete. Local file is /tmp/hadoop-root/mapred/local/localRunner/root/jobcache/job_local1236956871_0001/attempt_local1236956871_0001_r_000000_0/output/map_18.out.merged of size 1683559332\n","2021-11-09 23:05:01,304 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000030_0 succeeded at 401005.16 MB/s) Aggregated copy rate(28 of 41 at 11245233.00 MB/s)\n","2021-11-09 23:05:02,112 INFO reduce.InMemoryMapOutput: Read 418955133 bytes from map-output for attempt_local1236956871_0001_m_000017_0\n","2021-11-09 23:05:02,112 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 418955133, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->418955133\n","2021-11-09 23:05:02,171 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000004_0 decomp: 423854396 len: 423854400 to MEMORY\n","2021-11-09 23:05:07,305 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000017_0 succeeded at 399546.75 MB/s) Aggregated copy rate(29 of 41 at 11644779.00 MB/s)\n","2021-11-09 23:05:08,060 INFO mapreduce.Job:  map 100% reduce 24%\n","2021-11-09 23:05:10,000 INFO reduce.InMemoryMapOutput: Read 423854396 bytes from map-output for attempt_local1236956871_0001_m_000004_0\n","2021-11-09 23:05:10,000 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 423854396, inMemoryMapOutputs.size() -> 2, commitMemory -> 418955133, usedMemory ->842809529\n","2021-11-09 23:05:10,044 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000029_0 decomp: 420709536 len: 420709540 to MEMORY\n","2021-11-09 23:05:13,306 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000004_0 succeeded at 404219.06 MB/s) Aggregated copy rate(30 of 41 at 12048999.00 MB/s)\n","2021-11-09 23:05:17,824 INFO reduce.InMemoryMapOutput: Read 420709536 bytes from map-output for attempt_local1236956871_0001_m_000029_0\n","2021-11-09 23:05:17,824 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420709536, inMemoryMapOutputs.size() -> 3, commitMemory -> 842809529, usedMemory ->1263519065\n","2021-11-09 23:05:17,878 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000016_0 decomp: 422073010 len: 422073014 to MEMORY\n","2021-11-09 23:05:19,306 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000029_0 succeeded at 401219.88 MB/s) Aggregated copy rate(31 of 41 at 12450219.00 MB/s)\n","2021-11-09 23:05:20,064 INFO mapreduce.Job:  map 100% reduce 25%\n","2021-11-09 23:05:25,307 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000029_0 succeeded at 401219.88 MB/s) Aggregated copy rate(31 of 41 at 12450219.00 MB/s)\n","2021-11-09 23:05:25,818 INFO reduce.InMemoryMapOutput: Read 422073010 bytes from map-output for attempt_local1236956871_0001_m_000016_0\n","2021-11-09 23:05:25,818 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 422073010, inMemoryMapOutputs.size() -> 4, commitMemory -> 1263519065, usedMemory ->1685592075\n","2021-11-09 23:05:25,818 INFO reduce.MergeManagerImpl: Starting inMemoryMerger's merge since commitMemory=1685592075 > mergeThreshold=1573467904. Current usedMemory=1685592075\n","2021-11-09 23:05:25,818 INFO reduce.MergeThread: InMemoryMerger - Thread to merge in-memory shuffled map-outputs: Starting merge with 4 segments, while ignoring 0 segments\n","2021-11-09 23:05:25,820 INFO reduce.MergeManagerImpl: Initiating in-memory merge with 4 segments...\n","2021-11-09 23:05:25,820 INFO mapred.Merger: Merging 4 sorted segments\n","2021-11-09 23:05:25,820 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 1685590716 bytes\n","2021-11-09 23:05:25,896 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000003_0 decomp: 422186645 len: 422186649 to MEMORY\n","2021-11-09 23:05:31,308 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000016_0 succeeded at 402520.19 MB/s) Aggregated copy rate(32 of 41 at 12852739.00 MB/s)\n","2021-11-09 23:05:32,077 INFO mapreduce.Job:  map 100% reduce 26%\n","2021-11-09 23:05:33,321 INFO reduce.MergeManagerImpl: attempt_local1236956871_0001_r_000000_0 Merge of the 4 files in-memory complete. Local file is /tmp/hadoop-root/mapred/local/localRunner/root/jobcache/job_local1236956871_0001/attempt_local1236956871_0001_r_000000_0/output/map_17.out.merged of size 1685592073\n","2021-11-09 23:05:37,309 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000016_0 succeeded at 402520.19 MB/s) Aggregated copy rate(32 of 41 at 12852739.00 MB/s)\n","2021-11-09 23:05:37,883 INFO reduce.InMemoryMapOutput: Read 422186645 bytes from map-output for attempt_local1236956871_0001_m_000003_0\n","2021-11-09 23:05:37,883 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 422186645, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->422186645\n","2021-11-09 23:05:37,943 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000028_0 decomp: 421051653 len: 421051657 to MEMORY\n","2021-11-09 23:05:43,309 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000003_0 succeeded at 402628.56 MB/s) Aggregated copy rate(33 of 41 at 13255367.00 MB/s)\n","2021-11-09 23:05:44,082 INFO mapreduce.Job:  map 100% reduce 27%\n","2021-11-09 23:05:45,716 INFO reduce.InMemoryMapOutput: Read 421051653 bytes from map-output for attempt_local1236956871_0001_m_000028_0\n","2021-11-09 23:05:45,717 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 421051653, inMemoryMapOutputs.size() -> 2, commitMemory -> 422186645, usedMemory ->843238298\n","2021-11-09 23:05:45,759 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000015_0 decomp: 419584785 len: 419584789 to MEMORY\n","2021-11-09 23:05:49,311 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000028_0 succeeded at 401546.16 MB/s) Aggregated copy rate(34 of 41 at 13656913.00 MB/s)\n","2021-11-09 23:05:50,083 INFO mapreduce.Job:  map 100% reduce 28%\n","2021-11-09 23:05:53,513 INFO reduce.InMemoryMapOutput: Read 419584785 bytes from map-output for attempt_local1236956871_0001_m_000015_0\n","2021-11-09 23:05:53,514 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 419584785, inMemoryMapOutputs.size() -> 3, commitMemory -> 843238298, usedMemory ->1262823083\n","2021-11-09 23:05:53,574 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000002_0 decomp: 420661480 len: 420661484 to MEMORY\n","2021-11-09 23:05:55,311 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000015_0 succeeded at 400147.25 MB/s) Aggregated copy rate(35 of 41 at 14057061.00 MB/s)\n","2021-11-09 23:06:01,312 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000015_0 succeeded at 400147.25 MB/s) Aggregated copy rate(35 of 41 at 14057061.00 MB/s)\n","2021-11-09 23:06:01,427 INFO reduce.InMemoryMapOutput: Read 420661480 bytes from map-output for attempt_local1236956871_0001_m_000002_0\n","2021-11-09 23:06:01,427 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420661480, inMemoryMapOutputs.size() -> 4, commitMemory -> 1262823083, usedMemory ->1683484563\n","2021-11-09 23:06:01,427 INFO reduce.MergeManagerImpl: Starting inMemoryMerger's merge since commitMemory=1683484563 > mergeThreshold=1573467904. Current usedMemory=1683484563\n","2021-11-09 23:06:01,427 INFO reduce.MergeThread: InMemoryMerger - Thread to merge in-memory shuffled map-outputs: Starting merge with 4 segments, while ignoring 0 segments\n","2021-11-09 23:06:01,428 INFO reduce.MergeManagerImpl: Initiating in-memory merge with 4 segments...\n","2021-11-09 23:06:01,428 INFO mapred.Merger: Merging 4 sorted segments\n","2021-11-09 23:06:01,428 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 1683483613 bytes\n","2021-11-09 23:06:01,496 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000040_0 decomp: 262420887 len: 262420891 to MEMORY\n","2021-11-09 23:06:06,647 INFO reduce.InMemoryMapOutput: Read 262420887 bytes from map-output for attempt_local1236956871_0001_m_000040_0\n","2021-11-09 23:06:06,647 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 262420887, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1945905450\n","2021-11-09 23:06:06,713 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000027_0 decomp: 421610667 len: 421610671 to MEMORY\n","2021-11-09 23:06:07,313 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000040_0 succeeded at 250264.06 MB/s) Aggregated copy rate(37 of 41 at 14708499.00 MB/s)\n","2021-11-09 23:06:08,091 INFO mapreduce.Job:  map 100% reduce 30%\n","2021-11-09 23:06:08,922 INFO reduce.MergeManagerImpl: attempt_local1236956871_0001_r_000000_0 Merge of the 4 files in-memory complete. Local file is /tmp/hadoop-root/mapred/local/localRunner/root/jobcache/job_local1236956871_0001/attempt_local1236956871_0001_r_000000_0/output/map_15.out.merged of size 1683484561\n","2021-11-09 23:06:13,314 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000040_0 succeeded at 250264.06 MB/s) Aggregated copy rate(37 of 41 at 14708499.00 MB/s)\n","2021-11-09 23:06:17,514 INFO reduce.InMemoryMapOutput: Read 421610667 bytes from map-output for attempt_local1236956871_0001_m_000027_0\n","2021-11-09 23:06:17,515 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 421610667, inMemoryMapOutputs.size() -> 2, commitMemory -> 262420887, usedMemory ->684031554\n","2021-11-09 23:06:17,579 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000014_0 decomp: 420045372 len: 420045376 to MEMORY\n","2021-11-09 23:06:19,314 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000027_0 succeeded at 402079.25 MB/s) Aggregated copy rate(38 of 41 at 15110578.00 MB/s)\n","2021-11-09 23:06:20,095 INFO mapreduce.Job:  map 100% reduce 31%\n","2021-11-09 23:06:25,315 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000027_0 succeeded at 402079.25 MB/s) Aggregated copy rate(38 of 41 at 15110578.00 MB/s)\n","2021-11-09 23:06:25,317 INFO reduce.InMemoryMapOutput: Read 420045372 bytes from map-output for attempt_local1236956871_0001_m_000014_0\n","2021-11-09 23:06:25,317 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420045372, inMemoryMapOutputs.size() -> 3, commitMemory -> 684031554, usedMemory ->1104076926\n","2021-11-09 23:06:25,372 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000001_0 decomp: 420857691 len: 420857695 to MEMORY\n","2021-11-09 23:06:31,315 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000014_0 succeeded at 400586.50 MB/s) Aggregated copy rate(39 of 41 at 15511165.00 MB/s)\n","2021-11-09 23:06:32,100 INFO mapreduce.Job:  map 100% reduce 32%\n","2021-11-09 23:06:33,128 INFO reduce.InMemoryMapOutput: Read 420857691 bytes from map-output for attempt_local1236956871_0001_m_000001_0\n","2021-11-09 23:06:33,128 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 420857691, inMemoryMapOutputs.size() -> 4, commitMemory -> 1104076926, usedMemory ->1524934617\n","2021-11-09 23:06:33,184 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236956871_0001_m_000039_0 decomp: 419090653 len: 419090657 to MEMORY\n","2021-11-09 23:06:37,316 INFO mapred.LocalJobRunner: reduce > copy task(attempt_local1236956871_0001_m_000001_0 succeeded at 401361.19 MB/s) Aggregated copy rate(40 of 41 at 15912525.00 MB/s)\n","2021-11-09 23:06:38,102 INFO mapreduce.Job:  map 100% reduce 33%\n","2021-11-09 23:06:41,017 INFO reduce.InMemoryMapOutput: Read 419090653 bytes from map-output for attempt_local1236956871_0001_m_000039_0\n","2021-11-09 23:06:41,017 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 419090653, inMemoryMapOutputs.size() -> 5, commitMemory -> 1524934617, usedMemory ->1944025270\n","2021-11-09 23:06:41,017 INFO reduce.MergeManagerImpl: Starting inMemoryMerger's merge since commitMemory=1944025270 > mergeThreshold=1573467904. Current usedMemory=1944025270\n","2021-11-09 23:06:41,017 INFO reduce.MergeThread: InMemoryMerger - Thread to merge in-memory shuffled map-outputs: Starting merge with 5 segments, while ignoring 0 segments\n","2021-11-09 23:06:41,018 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2021-11-09 23:06:41,018 INFO mapred.LocalJobRunner: 41 / 41 copied.\n","2021-11-09 23:06:41,019 INFO reduce.MergeManagerImpl: Initiating in-memory merge with 5 segments...\n","2021-11-09 23:06:41,019 INFO mapred.Merger: Merging 5 sorted segments\n","2021-11-09 23:06:41,019 INFO mapred.Merger: Down to the last merge-pass, with 5 segments left of total size: 1944023623 bytes\n","2021-11-09 23:06:43,318 INFO mapred.LocalJobRunner: reduce > sort\n","2021-11-09 23:06:49,318 INFO mapred.LocalJobRunner: reduce > sort\n","2021-11-09 23:06:52,603 INFO reduce.MergeManagerImpl: attempt_local1236956871_0001_r_000000_0 Merge of the 5 files in-memory complete. Local file is /tmp/hadoop-root/mapred/local/localRunner/root/jobcache/job_local1236956871_0001/attempt_local1236956871_0001_r_000000_0/output/map_40.out.merged of size 1944025266\n","2021-11-09 23:06:52,603 INFO reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 10 on-disk map-outputs\n","2021-11-09 23:06:52,604 INFO reduce.MergeManagerImpl: Merging 10 files, 17104583182 bytes from disk\n","2021-11-09 23:06:52,605 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2021-11-09 23:06:52,605 INFO mapred.Merger: Merging 10 sorted segments\n","2021-11-09 23:06:55,319 INFO mapred.LocalJobRunner: reduce > sort\n","2021-11-09 23:06:55,447 INFO mapred.Merger: Down to the last merge-pass, with 10 segments left of total size: 17104579384 bytes\n","2021-11-09 23:06:55,448 INFO mapred.LocalJobRunner: reduce > sort\n","2021-11-09 23:06:55,564 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n","2021-11-09 23:06:55,568 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2021-11-09 23:06:55,569 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2021-11-09 23:06:55,628 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 23:06:55,629 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 23:06:55,630 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-09 23:07:00,331 INFO streaming.PipeMapRed: Records R/W=353/1\n","2021-11-09 23:07:00,464 INFO streaming.PipeMapRed: R/W/S=1000/836/0 in:250=1000/4 [rec/s] out:209=836/4 [rec/s]\n","2021-11-09 23:07:01,320 INFO mapred.LocalJobRunner: Records R/W=353/1 > reduce\n","2021-11-09 23:07:01,641 INFO streaming.PipeMapRed: R/W/S=10000/9343/0 in:1666=10000/6 [rec/s] out:1557=9343/6 [rec/s]\n","2021-11-09 23:07:02,109 INFO mapreduce.Job:  map 100% reduce 67%\n","2021-11-09 23:07:07,320 INFO mapred.LocalJobRunner: Records R/W=353/1 > reduce\n","2021-11-09 23:07:08,795 INFO streaming.PipeMapRed: R/W/S=100000/99496/0 in:7692=100000/13 [rec/s] out:7653=99496/13 [rec/s]\n","2021-11-09 23:07:10,338 INFO streaming.PipeMapRed: Records R/W=116793/116099\n","2021-11-09 23:07:13,321 INFO mapred.LocalJobRunner: Records R/W=116793/116099 > reduce\n","2021-11-09 23:07:18,198 INFO streaming.PipeMapRed: R/W/S=200000/199551/0 in:9090=200000/22 [rec/s] out:9070=199551/22 [rec/s]\n","2021-11-09 23:07:19,322 INFO mapred.LocalJobRunner: Records R/W=116793/116099 > reduce\n","2021-11-09 23:07:20,342 INFO streaming.PipeMapRed: Records R/W=222347/221905\n","2021-11-09 23:07:24,817 INFO streaming.PipeMapRed: R/W/S=300000/299107/0 in:10344=300000/29 [rec/s] out:10314=299107/29 [rec/s]\n","2021-11-09 23:07:25,322 INFO mapred.LocalJobRunner: Records R/W=222347/221905 > reduce\n","2021-11-09 23:07:30,343 INFO streaming.PipeMapRed: Records R/W=344241/343806\n","2021-11-09 23:07:31,323 INFO mapred.LocalJobRunner: Records R/W=344241/343806 > reduce\n","2021-11-09 23:07:34,485 INFO streaming.PipeMapRed: R/W/S=400000/399649/0 in:10526=400000/38 [rec/s] out:10517=399649/38 [rec/s]\n","2021-11-09 23:07:37,323 INFO mapred.LocalJobRunner: Records R/W=344241/343806 > reduce\n","2021-11-09 23:07:40,347 INFO streaming.PipeMapRed: Records R/W=453734/453303\n","2021-11-09 23:07:43,324 INFO mapred.LocalJobRunner: Records R/W=453734/453303 > reduce\n","2021-11-09 23:07:44,958 INFO streaming.PipeMapRed: R/W/S=500000/499393/0 in:10204=500000/49 [rec/s] out:10191=499393/49 [rec/s]\n","2021-11-09 23:07:49,324 INFO mapred.LocalJobRunner: Records R/W=453734/453303 > reduce\n","2021-11-09 23:07:50,349 INFO streaming.PipeMapRed: Records R/W=547082/546413\n","2021-11-09 23:07:54,809 INFO streaming.PipeMapRed: R/W/S=600000/599696/0 in:10169=600000/59 [rec/s] out:10164=599696/59 [rec/s]\n","2021-11-09 23:07:55,325 INFO mapred.LocalJobRunner: Records R/W=547082/546413 > reduce\n","2021-11-09 23:08:00,352 INFO streaming.PipeMapRed: Records R/W=664738/664334\n","2021-11-09 23:08:01,325 INFO mapred.LocalJobRunner: Records R/W=664738/664334 > reduce\n","2021-11-09 23:08:03,340 INFO streaming.PipeMapRed: R/W/S=700000/699527/0 in:10447=700000/67 [rec/s] out:10440=699527/67 [rec/s]\n","2021-11-09 23:08:07,326 INFO mapred.LocalJobRunner: Records R/W=664738/664334 > reduce\n","2021-11-09 23:08:10,359 INFO streaming.PipeMapRed: Records R/W=781016/780521\n","2021-11-09 23:08:12,007 INFO streaming.PipeMapRed: R/W/S=800000/799667/0 in:10526=800000/76 [rec/s] out:10521=799667/76 [rec/s]\n","2021-11-09 23:08:13,326 INFO mapred.LocalJobRunner: Records R/W=781016/780521 > reduce\n","2021-11-09 23:08:18,021 INFO streaming.PipeMapRed: R/W/S=900000/899697/0 in:10975=900000/82 [rec/s] out:10971=899697/82 [rec/s]\n","2021-11-09 23:08:19,327 INFO mapred.LocalJobRunner: Records R/W=781016/780521 > reduce\n","2021-11-09 23:08:20,361 INFO streaming.PipeMapRed: Records R/W=926480/925974\n","2021-11-09 23:08:25,328 INFO mapred.LocalJobRunner: Records R/W=926480/925974 > reduce\n","2021-11-09 23:08:26,974 INFO streaming.PipeMapRed: R/W/S=1000000/999412/0 in:10989=1000000/91 [rec/s] out:10982=999412/91 [rec/s]\n","2021-11-09 23:08:30,362 INFO streaming.PipeMapRed: Records R/W=1038780/1038339\n","2021-11-09 23:08:31,328 INFO mapred.LocalJobRunner: Records R/W=1038780/1038339 > reduce\n","2021-11-09 23:08:35,552 INFO streaming.PipeMapRed: R/W/S=1100000/1099503/0 in:11111=1100000/99 [rec/s] out:11106=1099503/99 [rec/s]\n","2021-11-09 23:08:37,329 INFO mapred.LocalJobRunner: Records R/W=1038780/1038339 > reduce\n","2021-11-09 23:08:40,377 INFO streaming.PipeMapRed: Records R/W=1141390/1141220\n","2021-11-09 23:08:43,330 INFO mapred.LocalJobRunner: Records R/W=1141390/1141220 > reduce\n","2021-11-09 23:08:47,301 INFO streaming.PipeMapRed: R/W/S=1200000/1198939/0 in:10810=1200000/111 [rec/s] out:10801=1198939/111 [rec/s]\n","2021-11-09 23:08:49,330 INFO mapred.LocalJobRunner: Records R/W=1141390/1141220 > reduce\n","2021-11-09 23:08:50,382 INFO streaming.PipeMapRed: Records R/W=1235295/1234841\n","2021-11-09 23:08:55,331 INFO mapred.LocalJobRunner: Records R/W=1235295/1234841 > reduce\n","2021-11-09 23:08:56,917 INFO streaming.PipeMapRed: R/W/S=1300000/1299720/0 in:10743=1300000/121 [rec/s] out:10741=1299720/121 [rec/s]\n","2021-11-09 23:09:00,384 INFO streaming.PipeMapRed: Records R/W=1320238/1319727\n","2021-11-09 23:09:01,331 INFO mapred.LocalJobRunner: Records R/W=1320238/1319727 > reduce\n","2021-11-09 23:09:07,332 INFO mapred.LocalJobRunner: Records R/W=1320238/1319727 > reduce\n","2021-11-09 23:09:07,835 INFO streaming.PipeMapRed: R/W/S=1400000/1399519/0 in:10606=1400000/132 [rec/s] out:10602=1399519/132 [rec/s]\n","2021-11-09 23:09:08,148 INFO mapreduce.Job:  map 100% reduce 68%\n","2021-11-09 23:09:10,387 INFO streaming.PipeMapRed: Records R/W=1426101/1425361\n","2021-11-09 23:09:13,333 INFO mapred.LocalJobRunner: Records R/W=1426101/1425361 > reduce\n","2021-11-09 23:09:16,752 INFO streaming.PipeMapRed: R/W/S=1500000/1499920/0 in:10638=1500000/141 [rec/s] out:10637=1499920/141 [rec/s]\n","2021-11-09 23:09:19,337 INFO mapred.LocalJobRunner: Records R/W=1426101/1425361 > reduce\n","2021-11-09 23:09:20,388 INFO streaming.PipeMapRed: Records R/W=1531398/1530911\n","2021-11-09 23:09:25,337 INFO mapred.LocalJobRunner: Records R/W=1531398/1530911 > reduce\n","2021-11-09 23:09:27,069 INFO streaming.PipeMapRed: R/W/S=1600000/1599726/0 in:10596=1600000/151 [rec/s] out:10594=1599726/151 [rec/s]\n","2021-11-09 23:09:30,392 INFO streaming.PipeMapRed: Records R/W=1644958/1644275\n","2021-11-09 23:09:31,338 INFO mapred.LocalJobRunner: Records R/W=1644958/1644275 > reduce\n","2021-11-09 23:09:35,995 INFO streaming.PipeMapRed: R/W/S=1700000/1699588/0 in:10625=1700000/160 [rec/s] out:10622=1699588/160 [rec/s]\n","2021-11-09 23:09:37,338 INFO mapred.LocalJobRunner: Records R/W=1644958/1644275 > reduce\n","2021-11-09 23:09:40,395 INFO streaming.PipeMapRed: Records R/W=1729321/1728924\n","2021-11-09 23:09:43,339 INFO mapred.LocalJobRunner: Records R/W=1729321/1728924 > reduce\n","2021-11-09 23:09:49,340 INFO mapred.LocalJobRunner: Records R/W=1729321/1728924 > reduce\n","2021-11-09 23:09:50,402 INFO streaming.PipeMapRed: Records R/W=1788167/1787543\n","2021-11-09 23:09:51,781 INFO streaming.PipeMapRed: R/W/S=1800000/1799591/0 in:10227=1800000/176 [rec/s] out:10224=1799591/176 [rec/s]\n","2021-11-09 23:09:55,340 INFO mapred.LocalJobRunner: Records R/W=1788167/1787543 > reduce\n","2021-11-09 23:10:00,406 INFO streaming.PipeMapRed: Records R/W=1886235/1885650\n","2021-11-09 23:10:01,341 INFO mapred.LocalJobRunner: Records R/W=1886235/1885650 > reduce\n","2021-11-09 23:10:02,067 INFO streaming.PipeMapRed: R/W/S=1900000/1899675/0 in:10215=1900000/186 [rec/s] out:10213=1899675/186 [rec/s]\n","2021-11-09 23:10:07,341 INFO mapred.LocalJobRunner: Records R/W=1886235/1885650 > reduce\n","2021-11-09 23:10:10,408 INFO streaming.PipeMapRed: Records R/W=1977187/1976778\n","2021-11-09 23:10:12,184 INFO streaming.PipeMapRed: R/W/S=2000000/1999407/0 in:10204=2000000/196 [rec/s] out:10201=1999407/196 [rec/s]\n","2021-11-09 23:10:13,345 INFO mapred.LocalJobRunner: Records R/W=1977187/1976778 > reduce\n","2021-11-09 23:10:19,346 INFO mapred.LocalJobRunner: Records R/W=1977187/1976778 > reduce\n","2021-11-09 23:10:20,223 INFO streaming.PipeMapRed: R/W/S=2100000/2099308/0 in:10294=2100000/204 [rec/s] out:10290=2099308/204 [rec/s]\n","2021-11-09 23:10:20,412 INFO streaming.PipeMapRed: Records R/W=2101872/2101384\n","2021-11-09 23:10:25,346 INFO mapred.LocalJobRunner: Records R/W=2101872/2101384 > reduce\n","2021-11-09 23:10:30,436 INFO streaming.PipeMapRed: Records R/W=2168583/2168482\n","2021-11-09 23:10:31,347 INFO mapred.LocalJobRunner: Records R/W=2168583/2168482 > reduce\n","2021-11-09 23:10:34,380 INFO streaming.PipeMapRed: R/W/S=2200000/2199503/0 in:10091=2200000/218 [rec/s] out:10089=2199503/218 [rec/s]\n","2021-11-09 23:10:37,347 INFO mapred.LocalJobRunner: Records R/W=2168583/2168482 > reduce\n","2021-11-09 23:10:40,439 INFO streaming.PipeMapRed: Records R/W=2275730/2275262\n","2021-11-09 23:10:42,316 INFO streaming.PipeMapRed: R/W/S=2300000/2299245/0 in:10176=2300000/226 [rec/s] out:10173=2299245/226 [rec/s]\n","2021-11-09 23:10:43,348 INFO mapred.LocalJobRunner: Records R/W=2275730/2275262 > reduce\n","2021-11-09 23:10:49,348 INFO mapred.LocalJobRunner: Records R/W=2275730/2275262 > reduce\n","2021-11-09 23:10:50,442 INFO streaming.PipeMapRed: Records R/W=2396514/2396077\n","2021-11-09 23:10:50,734 INFO streaming.PipeMapRed: R/W/S=2400000/2399477/0 in:10212=2400000/235 [rec/s] out:10210=2399477/235 [rec/s]\n","2021-11-09 23:10:55,349 INFO mapred.LocalJobRunner: Records R/W=2396514/2396077 > reduce\n","2021-11-09 23:10:58,842 INFO streaming.PipeMapRed: R/W/S=2500000/2499459/0 in:10288=2500000/243 [rec/s] out:10285=2499459/243 [rec/s]\n","2021-11-09 23:11:00,446 INFO streaming.PipeMapRed: Records R/W=2519956/2519293\n","2021-11-09 23:11:01,349 INFO mapred.LocalJobRunner: Records R/W=2519956/2519293 > reduce\n","2021-11-09 23:11:07,073 INFO streaming.PipeMapRed: R/W/S=2600000/2599252/0 in:10358=2600000/251 [rec/s] out:10355=2599252/251 [rec/s]\n","2021-11-09 23:11:07,350 INFO mapred.LocalJobRunner: Records R/W=2519956/2519293 > reduce\n","2021-11-09 23:11:10,449 INFO streaming.PipeMapRed: Records R/W=2638428/2638039\n","2021-11-09 23:11:13,350 INFO mapred.LocalJobRunner: Records R/W=2638428/2638039 > reduce\n","2021-11-09 23:11:15,575 INFO streaming.PipeMapRed: R/W/S=2700000/2699733/0 in:10384=2700000/260 [rec/s] out:10383=2699733/260 [rec/s]\n","2021-11-09 23:11:19,351 INFO mapred.LocalJobRunner: Records R/W=2638428/2638039 > reduce\n","2021-11-09 23:11:20,452 INFO streaming.PipeMapRed: Records R/W=2758889/2758540\n","2021-11-09 23:11:23,749 INFO streaming.PipeMapRed: R/W/S=2800000/2799433/0 in:10447=2800000/268 [rec/s] out:10445=2799433/268 [rec/s]\n","2021-11-09 23:11:25,351 INFO mapred.LocalJobRunner: Records R/W=2758889/2758540 > reduce\n","2021-11-09 23:11:30,455 INFO streaming.PipeMapRed: Records R/W=2884499/2884196\n","2021-11-09 23:11:31,352 INFO mapred.LocalJobRunner: Records R/W=2884499/2884196 > reduce\n","2021-11-09 23:11:31,653 INFO streaming.PipeMapRed: R/W/S=2900000/2899573/0 in:10507=2900000/276 [rec/s] out:10505=2899573/276 [rec/s]\n","2021-11-09 23:11:37,352 INFO mapred.LocalJobRunner: Records R/W=2884499/2884196 > reduce\n","2021-11-09 23:11:38,198 INFO mapreduce.Job:  map 100% reduce 69%\n","2021-11-09 23:11:39,671 INFO streaming.PipeMapRed: R/W/S=3000000/2999491/0 in:10563=3000000/284 [rec/s] out:10561=2999491/284 [rec/s]\n","2021-11-09 23:11:40,458 INFO streaming.PipeMapRed: Records R/W=3009950/3009367\n","2021-11-09 23:11:43,352 INFO mapred.LocalJobRunner: Records R/W=3009950/3009367 > reduce\n","2021-11-09 23:11:48,112 INFO streaming.PipeMapRed: R/W/S=3100000/3099525/0 in:10616=3100000/292 [rec/s] out:10614=3099525/292 [rec/s]\n","2021-11-09 23:11:49,353 INFO mapred.LocalJobRunner: Records R/W=3009950/3009367 > reduce\n","2021-11-09 23:11:50,461 INFO streaming.PipeMapRed: Records R/W=3132586/3132197\n","2021-11-09 23:11:55,353 INFO mapred.LocalJobRunner: Records R/W=3132586/3132197 > reduce\n","2021-11-09 23:11:56,008 INFO streaming.PipeMapRed: R/W/S=3200000/3199671/0 in:10666=3200000/300 [rec/s] out:10665=3199671/300 [rec/s]\n","2021-11-09 23:12:00,466 INFO streaming.PipeMapRed: Records R/W=3253707/3253225\n","2021-11-09 23:12:01,354 INFO mapred.LocalJobRunner: Records R/W=3253707/3253225 > reduce\n","2021-11-09 23:12:04,341 INFO streaming.PipeMapRed: R/W/S=3300000/3299636/0 in:10714=3300000/308 [rec/s] out:10713=3299636/308 [rec/s]\n","2021-11-09 23:12:07,354 INFO mapred.LocalJobRunner: Records R/W=3253707/3253225 > reduce\n","2021-11-09 23:12:10,467 INFO streaming.PipeMapRed: Records R/W=3375672/3374886\n","2021-11-09 23:12:12,441 INFO streaming.PipeMapRed: R/W/S=3400000/3399303/0 in:10759=3400000/316 [rec/s] out:10757=3399303/316 [rec/s]\n","2021-11-09 23:12:13,355 INFO mapred.LocalJobRunner: Records R/W=3375672/3374886 > reduce\n","2021-11-09 23:12:19,355 INFO mapred.LocalJobRunner: Records R/W=3375672/3374886 > reduce\n","2021-11-09 23:12:20,353 INFO streaming.PipeMapRed: R/W/S=3500000/3499403/0 in:10802=3500000/324 [rec/s] out:10800=3499403/324 [rec/s]\n","2021-11-09 23:12:20,468 INFO streaming.PipeMapRed: Records R/W=3501761/3501027\n","2021-11-09 23:12:25,356 INFO mapred.LocalJobRunner: Records R/W=3501761/3501027 > reduce\n","2021-11-09 23:12:28,470 INFO streaming.PipeMapRed: R/W/S=3600000/3599578/0 in:10843=3600000/332 [rec/s] out:10842=3599578/332 [rec/s]\n","2021-11-09 23:12:30,469 INFO streaming.PipeMapRed: Records R/W=3622892/3622386\n","2021-11-09 23:12:31,356 INFO mapred.LocalJobRunner: Records R/W=3622892/3622386 > reduce\n","2021-11-09 23:12:36,800 INFO streaming.PipeMapRed: R/W/S=3700000/3699428/0 in:10850=3700000/341 [rec/s] out:10848=3699428/341 [rec/s]\n","2021-11-09 23:12:37,356 INFO mapred.LocalJobRunner: Records R/W=3622892/3622386 > reduce\n","2021-11-09 23:12:40,476 INFO streaming.PipeMapRed: Records R/W=3745044/3744244\n","2021-11-09 23:12:43,357 INFO mapred.LocalJobRunner: Records R/W=3745044/3744244 > reduce\n","2021-11-09 23:12:45,032 INFO streaming.PipeMapRed: R/W/S=3800000/3799694/0 in:10888=3800000/349 [rec/s] out:10887=3799694/349 [rec/s]\n","2021-11-09 23:12:49,357 INFO mapred.LocalJobRunner: Records R/W=3745044/3744244 > reduce\n","2021-11-09 23:12:50,477 INFO streaming.PipeMapRed: Records R/W=3864804/3864356\n","2021-11-09 23:12:53,378 INFO streaming.PipeMapRed: R/W/S=3900000/3899621/0 in:10924=3900000/357 [rec/s] out:10923=3899621/357 [rec/s]\n","2021-11-09 23:12:55,358 INFO mapred.LocalJobRunner: Records R/W=3864804/3864356 > reduce\n","2021-11-09 23:13:00,479 INFO streaming.PipeMapRed: Records R/W=3988587/3987919\n","2021-11-09 23:13:01,358 INFO mapred.LocalJobRunner: Records R/W=3988587/3987919 > reduce\n","2021-11-09 23:13:01,468 INFO streaming.PipeMapRed: R/W/S=4000000/3999738/0 in:10958=4000000/365 [rec/s] out:10958=3999738/365 [rec/s]\n","2021-11-09 23:13:07,359 INFO mapred.LocalJobRunner: Records R/W=3988587/3987919 > reduce\n","2021-11-09 23:13:10,481 INFO streaming.PipeMapRed: Records R/W=4074158/4073598\n","2021-11-09 23:13:13,360 INFO mapred.LocalJobRunner: Records R/W=4074158/4073598 > reduce\n","2021-11-09 23:13:13,599 INFO streaming.PipeMapRed: R/W/S=4100000/4099644/0 in:10846=4100000/378 [rec/s] out:10845=4099644/378 [rec/s]\n","2021-11-09 23:13:19,361 INFO mapred.LocalJobRunner: Records R/W=4074158/4073598 > reduce\n","2021-11-09 23:13:20,482 INFO streaming.PipeMapRed: Records R/W=4184966/4184400\n","2021-11-09 23:13:21,522 INFO streaming.PipeMapRed: R/W/S=4200000/4199568/0 in:10909=4200000/385 [rec/s] out:10907=4199568/385 [rec/s]\n","2021-11-09 23:13:25,362 INFO mapred.LocalJobRunner: Records R/W=4184966/4184400 > reduce\n","2021-11-09 23:13:29,831 INFO streaming.PipeMapRed: R/W/S=4300000/4299491/0 in:10913=4300000/394 [rec/s] out:10912=4299491/394 [rec/s]\n","2021-11-09 23:13:30,485 INFO streaming.PipeMapRed: Records R/W=4306301/4305794\n","2021-11-09 23:13:31,363 INFO mapred.LocalJobRunner: Records R/W=4306301/4305794 > reduce\n","2021-11-09 23:13:34,270 INFO streaming.PipeMapRed: R/W/S=4400000/4399329/0 in:11055=4400000/398 [rec/s] out:11053=4399329/398 [rec/s]\n","2021-11-09 23:13:36,214 INFO streaming.PipeMapRed: R/W/S=4500000/4499098/0 in:11250=4500000/400 [rec/s] out:11247=4499098/400 [rec/s]\n","2021-11-09 23:13:37,364 INFO mapred.LocalJobRunner: Records R/W=4306301/4305794 > reduce\n","2021-11-09 23:13:38,254 INFO streaming.PipeMapRed: R/W/S=4600000/4598867/0 in:11442=4600000/402 [rec/s] out:11439=4598867/402 [rec/s]\n","2021-11-09 23:13:40,486 INFO streaming.PipeMapRed: Records R/W=4684455/4683971\n","2021-11-09 23:13:41,762 INFO streaming.PipeMapRed: R/W/S=4700000/4699401/0 in:11576=4700000/406 [rec/s] out:11574=4699401/406 [rec/s]\n","2021-11-09 23:13:43,364 INFO mapred.LocalJobRunner: Records R/W=4684455/4683971 > reduce\n","2021-11-09 23:13:49,365 INFO mapred.LocalJobRunner: Records R/W=4684455/4683971 > reduce\n","2021-11-09 23:13:49,996 INFO streaming.PipeMapRed: R/W/S=4800000/4799369/0 in:11594=4800000/414 [rec/s] out:11592=4799369/414 [rec/s]\n","2021-11-09 23:13:50,490 INFO streaming.PipeMapRed: Records R/W=4805490/4804664\n","2021-11-09 23:13:55,366 INFO mapred.LocalJobRunner: Records R/W=4805490/4804664 > reduce\n","2021-11-09 23:13:58,399 INFO streaming.PipeMapRed: R/W/S=4900000/4899621/0 in:11611=4900000/422 [rec/s] out:11610=4899621/422 [rec/s]\n","2021-11-09 23:14:00,495 INFO streaming.PipeMapRed: Records R/W=4924092/4923601\n","2021-11-09 23:14:01,366 INFO mapred.LocalJobRunner: Records R/W=4924092/4923601 > reduce\n","2021-11-09 23:14:02,236 INFO mapreduce.Job:  map 100% reduce 70%\n","2021-11-09 23:14:06,884 INFO streaming.PipeMapRed: R/W/S=5000000/4999482/0 in:11600=5000000/431 [rec/s] out:11599=4999482/431 [rec/s]\n","2021-11-09 23:14:07,367 INFO mapred.LocalJobRunner: Records R/W=4924092/4923601 > reduce\n","2021-11-09 23:14:10,497 INFO streaming.PipeMapRed: Records R/W=5043048/5042390\n","2021-11-09 23:14:13,368 INFO mapred.LocalJobRunner: Records R/W=5043048/5042390 > reduce\n","2021-11-09 23:14:15,220 INFO streaming.PipeMapRed: R/W/S=5100000/5099426/0 in:11617=5100000/439 [rec/s] out:11616=5099426/439 [rec/s]\n","2021-11-09 23:14:19,369 INFO mapred.LocalJobRunner: Records R/W=5043048/5042390 > reduce\n","2021-11-09 23:14:20,499 INFO streaming.PipeMapRed: Records R/W=5162252/5161701\n","2021-11-09 23:14:23,698 INFO streaming.PipeMapRed: R/W/S=5200000/5199584/0 in:11607=5200000/448 [rec/s] out:11606=5199584/448 [rec/s]\n","2021-11-09 23:14:25,369 INFO mapred.LocalJobRunner: Records R/W=5162252/5161701 > reduce\n","2021-11-09 23:14:30,500 INFO streaming.PipeMapRed: Records R/W=5280377/5279959\n","2021-11-09 23:14:31,370 INFO mapred.LocalJobRunner: Records R/W=5280377/5279959 > reduce\n","2021-11-09 23:14:32,151 INFO streaming.PipeMapRed: R/W/S=5300000/5299564/0 in:11622=5300000/456 [rec/s] out:11621=5299564/456 [rec/s]\n","2021-11-09 23:14:37,370 INFO mapred.LocalJobRunner: Records R/W=5280377/5279959 > reduce\n","2021-11-09 23:14:40,502 INFO streaming.PipeMapRed: Records R/W=5397493/5397124\n","2021-11-09 23:14:40,703 INFO streaming.PipeMapRed: R/W/S=5400000/5399462/0 in:11612=5400000/465 [rec/s] out:11611=5399462/465 [rec/s]\n","2021-11-09 23:14:43,371 INFO mapred.LocalJobRunner: Records R/W=5397493/5397124 > reduce\n","2021-11-09 23:14:48,984 INFO streaming.PipeMapRed: R/W/S=5500000/5499564/0 in:11627=5500000/473 [rec/s] out:11626=5499564/473 [rec/s]\n","2021-11-09 23:14:49,371 INFO mapred.LocalJobRunner: Records R/W=5397493/5397124 > reduce\n","2021-11-09 23:14:50,506 INFO streaming.PipeMapRed: Records R/W=5517544/5517253\n","2021-11-09 23:14:55,372 INFO mapred.LocalJobRunner: Records R/W=5517544/5517253 > reduce\n","2021-11-09 23:14:57,493 INFO streaming.PipeMapRed: R/W/S=5600000/5599357/0 in:11642=5600000/481 [rec/s] out:11641=5599357/481 [rec/s]\n","2021-11-09 23:15:00,507 INFO streaming.PipeMapRed: Records R/W=5635105/5634487\n","2021-11-09 23:15:01,372 INFO mapred.LocalJobRunner: Records R/W=5635105/5634487 > reduce\n","2021-11-09 23:15:06,060 INFO streaming.PipeMapRed: R/W/S=5700000/5699440/0 in:11632=5700000/490 [rec/s] out:11631=5699440/490 [rec/s]\n","2021-11-09 23:15:07,373 INFO mapred.LocalJobRunner: Records R/W=5635105/5634487 > reduce\n","2021-11-09 23:15:10,511 INFO streaming.PipeMapRed: Records R/W=5752014/5751450\n","2021-11-09 23:15:13,373 INFO mapred.LocalJobRunner: Records R/W=5752014/5751450 > reduce\n","2021-11-09 23:15:14,465 INFO streaming.PipeMapRed: R/W/S=5800000/5799678/0 in:11646=5800000/498 [rec/s] out:11645=5799678/498 [rec/s]\n","2021-11-09 23:15:19,374 INFO mapred.LocalJobRunner: Records R/W=5752014/5751450 > reduce\n","2021-11-09 23:15:20,514 INFO streaming.PipeMapRed: Records R/W=5871725/5871050\n","2021-11-09 23:15:22,858 INFO streaming.PipeMapRed: R/W/S=5900000/5899359/0 in:11637=5900000/507 [rec/s] out:11635=5899359/507 [rec/s]\n","2021-11-09 23:15:25,374 INFO mapred.LocalJobRunner: Records R/W=5871725/5871050 > reduce\n","2021-11-09 23:15:30,518 INFO streaming.PipeMapRed: Records R/W=5988505/5987882\n","2021-11-09 23:15:31,375 INFO mapred.LocalJobRunner: Records R/W=5988505/5987882 > reduce\n","2021-11-09 23:15:31,620 INFO streaming.PipeMapRed: R/W/S=6000000/5999762/0 in:11627=6000000/516 [rec/s] out:11627=5999762/516 [rec/s]\n","2021-11-09 23:15:37,375 INFO mapred.LocalJobRunner: Records R/W=5988505/5987882 > reduce\n","2021-11-09 23:15:40,175 INFO streaming.PipeMapRed: R/W/S=6100000/6099413/0 in:11641=6100000/524 [rec/s] out:11640=6099413/524 [rec/s]\n","2021-11-09 23:15:40,521 INFO streaming.PipeMapRed: Records R/W=6104012/6103648\n","2021-11-09 23:15:43,376 INFO mapred.LocalJobRunner: Records R/W=6104012/6103648 > reduce\n","2021-11-09 23:15:48,629 INFO streaming.PipeMapRed: R/W/S=6200000/6199444/0 in:11632=6200000/533 [rec/s] out:11631=6199444/533 [rec/s]\n","2021-11-09 23:15:49,376 INFO mapred.LocalJobRunner: Records R/W=6104012/6103648 > reduce\n","2021-11-09 23:15:50,522 INFO streaming.PipeMapRed: Records R/W=6220801/6220508\n","2021-11-09 23:15:55,376 INFO mapred.LocalJobRunner: Records R/W=6220801/6220508 > reduce\n","2021-11-09 23:15:57,172 INFO streaming.PipeMapRed: R/W/S=6300000/6299670/0 in:11645=6300000/541 [rec/s] out:11644=6299670/541 [rec/s]\n","2021-11-09 23:16:00,523 INFO streaming.PipeMapRed: Records R/W=6338982/6338376\n","2021-11-09 23:16:01,377 INFO mapred.LocalJobRunner: Records R/W=6338982/6338376 > reduce\n","2021-11-09 23:16:05,809 INFO streaming.PipeMapRed: R/W/S=6400000/6399617/0 in:11636=6400000/550 [rec/s] out:11635=6399617/550 [rec/s]\n","2021-11-09 23:16:07,377 INFO mapred.LocalJobRunner: Records R/W=6338982/6338376 > reduce\n","2021-11-09 23:16:10,527 INFO streaming.PipeMapRed: Records R/W=6454765/6454469\n","2021-11-09 23:16:13,378 INFO mapred.LocalJobRunner: Records R/W=6454765/6454469 > reduce\n","2021-11-09 23:16:14,311 INFO streaming.PipeMapRed: R/W/S=6500000/6499555/0 in:11648=6500000/558 [rec/s] out:11647=6499555/558 [rec/s]\n","2021-11-09 23:16:19,378 INFO mapred.LocalJobRunner: Records R/W=6454765/6454469 > reduce\n","2021-11-09 23:16:20,531 INFO streaming.PipeMapRed: Records R/W=6575311/6574533\n","2021-11-09 23:16:22,579 INFO streaming.PipeMapRed: R/W/S=6600000/6599492/0 in:11640=6600000/567 [rec/s] out:11639=6599492/567 [rec/s]\n","2021-11-09 23:16:25,379 INFO mapred.LocalJobRunner: Records R/W=6575311/6574533 > reduce\n","2021-11-09 23:16:30,535 INFO streaming.PipeMapRed: Records R/W=6684439/6684040\n","2021-11-09 23:16:31,379 INFO mapred.LocalJobRunner: Records R/W=6684439/6684040 > reduce\n","2021-11-09 23:16:32,279 INFO mapreduce.Job:  map 100% reduce 71%\n","2021-11-09 23:16:32,849 INFO streaming.PipeMapRed: R/W/S=6700000/6699510/0 in:11611=6700000/577 [rec/s] out:11610=6699510/577 [rec/s]\n","2021-11-09 23:16:37,380 INFO mapred.LocalJobRunner: Records R/W=6684439/6684040 > reduce\n","2021-11-09 23:16:40,542 INFO streaming.PipeMapRed: Records R/W=6760746/6760298\n","2021-11-09 23:16:43,380 INFO mapred.LocalJobRunner: Records R/W=6760746/6760298 > reduce\n","2021-11-09 23:16:44,341 INFO streaming.PipeMapRed: R/W/S=6800000/6799745/0 in:11564=6800000/588 [rec/s] out:11564=6799745/588 [rec/s]\n","2021-11-09 23:16:49,381 INFO mapred.LocalJobRunner: Records R/W=6760746/6760298 > reduce\n","2021-11-09 23:16:50,551 INFO streaming.PipeMapRed: Records R/W=6850570/6850004\n","2021-11-09 23:16:55,381 INFO mapred.LocalJobRunner: Records R/W=6850570/6850004 > reduce\n","2021-11-09 23:16:56,286 INFO streaming.PipeMapRed: R/W/S=6900000/6899666/0 in:11500=6900000/600 [rec/s] out:11499=6899666/600 [rec/s]\n","2021-11-09 23:17:00,555 INFO streaming.PipeMapRed: Records R/W=6948301/6947752\n","2021-11-09 23:17:01,381 INFO mapred.LocalJobRunner: Records R/W=6948301/6947752 > reduce\n","2021-11-09 23:17:05,598 INFO streaming.PipeMapRed: R/W/S=7000000/6999452/0 in:11475=7000000/610 [rec/s] out:11474=6999452/610 [rec/s]\n","2021-11-09 23:17:07,382 INFO mapred.LocalJobRunner: Records R/W=6948301/6947752 > reduce\n","2021-11-09 23:17:10,560 INFO streaming.PipeMapRed: Records R/W=7048050/7047797\n","2021-11-09 23:17:13,382 INFO mapred.LocalJobRunner: Records R/W=7048050/7047797 > reduce\n","2021-11-09 23:17:17,366 INFO streaming.PipeMapRed: R/W/S=7100000/7099603/0 in:11433=7100000/621 [rec/s] out:11432=7099603/621 [rec/s]\n","2021-11-09 23:17:19,383 INFO mapred.LocalJobRunner: Records R/W=7048050/7047797 > reduce\n","2021-11-09 23:17:20,562 INFO streaming.PipeMapRed: Records R/W=7127407/7126921\n","2021-11-09 23:17:25,383 INFO mapred.LocalJobRunner: Records R/W=7127407/7126921 > reduce\n","2021-11-09 23:17:28,453 INFO streaming.PipeMapRed: R/W/S=7200000/7199561/0 in:11392=7200000/632 [rec/s] out:11391=7199561/632 [rec/s]\n","2021-11-09 23:17:30,569 INFO streaming.PipeMapRed: Records R/W=7218183/7217722\n","2021-11-09 23:17:31,384 INFO mapred.LocalJobRunner: Records R/W=7218183/7217722 > reduce\n","2021-11-09 23:17:37,385 INFO mapred.LocalJobRunner: Records R/W=7218183/7217722 > reduce\n","2021-11-09 23:17:38,697 INFO streaming.PipeMapRed: R/W/S=7300000/7299496/0 in:11353=7300000/643 [rec/s] out:11352=7299496/643 [rec/s]\n","2021-11-09 23:17:40,572 INFO streaming.PipeMapRed: Records R/W=7317984/7317615\n","2021-11-09 23:17:43,385 INFO mapred.LocalJobRunner: Records R/W=7317984/7317615 > reduce\n","2021-11-09 23:17:48,020 INFO streaming.PipeMapRed: R/W/S=7400000/7399537/0 in:11349=7400000/652 [rec/s] out:11348=7399537/652 [rec/s]\n","2021-11-09 23:17:49,386 INFO mapred.LocalJobRunner: Records R/W=7317984/7317615 > reduce\n","2021-11-09 23:17:50,578 INFO streaming.PipeMapRed: Records R/W=7424932/7424403\n","2021-11-09 23:17:55,386 INFO mapred.LocalJobRunner: Records R/W=7424932/7424403 > reduce\n","2021-11-09 23:17:55,604 INFO streaming.PipeMapRed: R/W/S=7500000/7499506/0 in:11363=7500000/660 [rec/s] out:11362=7499506/660 [rec/s]\n","2021-11-09 23:18:00,579 INFO streaming.PipeMapRed: Records R/W=7559312/7559090\n","2021-11-09 23:18:01,387 INFO mapred.LocalJobRunner: Records R/W=7559312/7559090 > reduce\n","2021-11-09 23:18:03,994 INFO streaming.PipeMapRed: R/W/S=7600000/7599695/0 in:11377=7600000/668 [rec/s] out:11376=7599695/668 [rec/s]\n","2021-11-09 23:18:07,387 INFO mapred.LocalJobRunner: Records R/W=7559312/7559090 > reduce\n","2021-11-09 23:18:10,584 INFO streaming.PipeMapRed: Records R/W=7676710/7676276\n","2021-11-09 23:18:12,961 INFO streaming.PipeMapRed: R/W/S=7700000/7699681/0 in:11373=7700000/677 [rec/s] out:11373=7699681/677 [rec/s]\n","2021-11-09 23:18:13,388 INFO mapred.LocalJobRunner: Records R/W=7676710/7676276 > reduce\n","2021-11-09 23:18:19,389 INFO mapred.LocalJobRunner: Records R/W=7676710/7676276 > reduce\n","2021-11-09 23:18:20,587 INFO streaming.PipeMapRed: Records R/W=7769434/7768730\n","2021-11-09 23:18:23,421 INFO streaming.PipeMapRed: R/W/S=7800000/7799331/0 in:11353=7800000/687 [rec/s] out:11352=7799331/687 [rec/s]\n","2021-11-09 23:18:25,389 INFO mapred.LocalJobRunner: Records R/W=7769434/7768730 > reduce\n","2021-11-09 23:18:30,463 INFO streaming.PipeMapRed: R/W/S=7900000/7899304/0 in:11383=7900000/694 [rec/s] out:11382=7899304/694 [rec/s]\n","2021-11-09 23:18:30,591 INFO streaming.PipeMapRed: Records R/W=7901630/7901091\n","2021-11-09 23:18:31,389 INFO mapred.LocalJobRunner: Records R/W=7901630/7901091 > reduce\n","2021-11-09 23:18:36,816 INFO streaming.PipeMapRed: R/W/S=8000000/7999189/0 in:11412=8000000/701 [rec/s] out:11411=7999189/701 [rec/s]\n","2021-11-09 23:18:37,390 INFO mapred.LocalJobRunner: Records R/W=7901630/7901091 > reduce\n","2021-11-09 23:18:40,594 INFO streaming.PipeMapRed: Records R/W=8057587/8057174\n","2021-11-09 23:18:43,390 INFO mapred.LocalJobRunner: Records R/W=8057587/8057174 > reduce\n","2021-11-09 23:18:43,632 INFO streaming.PipeMapRed: R/W/S=8100000/8099613/0 in:11440=8100000/708 [rec/s] out:11440=8099613/708 [rec/s]\n","2021-11-09 23:18:49,391 INFO mapred.LocalJobRunner: Records R/W=8057587/8057174 > reduce\n","2021-11-09 23:18:50,595 INFO streaming.PipeMapRed: Records R/W=8198777/8198356\n","2021-11-09 23:18:50,676 INFO streaming.PipeMapRed: R/W/S=8200000/8199538/0 in:11468=8200000/715 [rec/s] out:11467=8199538/715 [rec/s]\n","2021-11-09 23:18:55,391 INFO mapred.LocalJobRunner: Records R/W=8198777/8198356 > reduce\n","2021-11-09 23:19:00,270 INFO streaming.PipeMapRed: R/W/S=8300000/8299411/0 in:11464=8300000/724 [rec/s] out:11463=8299411/724 [rec/s]\n","2021-11-09 23:19:00,598 INFO streaming.PipeMapRed: Records R/W=8303184/8302855\n","2021-11-09 23:19:01,392 INFO mapred.LocalJobRunner: Records R/W=8303184/8302855 > reduce\n","2021-11-09 23:19:02,324 INFO mapreduce.Job:  map 100% reduce 72%\n","2021-11-09 23:19:07,392 INFO mapred.LocalJobRunner: Records R/W=8303184/8302855 > reduce\n","2021-11-09 23:19:10,601 INFO streaming.PipeMapRed: Records R/W=8387987/8387619\n","2021-11-09 23:19:11,798 INFO streaming.PipeMapRed: R/W/S=8400000/8399743/0 in:11413=8400000/736 [rec/s] out:11412=8399743/736 [rec/s]\n","2021-11-09 23:19:13,393 INFO mapred.LocalJobRunner: Records R/W=8387987/8387619 > reduce\n","2021-11-09 23:19:19,393 INFO mapred.LocalJobRunner: Records R/W=8387987/8387619 > reduce\n","2021-11-09 23:19:20,602 INFO streaming.PipeMapRed: Records R/W=8497401/8496993\n","2021-11-09 23:19:20,849 INFO streaming.PipeMapRed: R/W/S=8500000/8499843/0 in:11409=8500000/745 [rec/s] out:11409=8499843/745 [rec/s]\n","2021-11-09 23:19:25,394 INFO mapred.LocalJobRunner: Records R/W=8497401/8496993 > reduce\n","2021-11-09 23:19:26,972 INFO streaming.PipeMapRed: R/W/S=8600000/8599299/0 in:11451=8600000/751 [rec/s] out:11450=8599299/751 [rec/s]\n","2021-11-09 23:19:30,609 INFO streaming.PipeMapRed: Records R/W=8631841/8631637\n","2021-11-09 23:19:31,394 INFO mapred.LocalJobRunner: Records R/W=8631841/8631637 > reduce\n","2021-11-09 23:19:36,711 INFO streaming.PipeMapRed: R/W/S=8700000/8699448/0 in:11432=8700000/761 [rec/s] out:11431=8699448/761 [rec/s]\n","2021-11-09 23:19:37,395 INFO mapred.LocalJobRunner: Records R/W=8631841/8631637 > reduce\n","2021-11-09 23:19:40,615 INFO streaming.PipeMapRed: Records R/W=8732587/8732329\n","2021-11-09 23:19:43,395 INFO mapred.LocalJobRunner: Records R/W=8732587/8732329 > reduce\n","2021-11-09 23:19:49,396 INFO mapred.LocalJobRunner: Records R/W=8732587/8732329 > reduce\n","2021-11-09 23:19:50,620 INFO streaming.PipeMapRed: Records R/W=8797587/8797163\n","2021-11-09 23:19:51,060 INFO streaming.PipeMapRed: R/W/S=8800000/8799728/0 in:11354=8800000/775 [rec/s] out:11354=8799728/775 [rec/s]\n","2021-11-09 23:19:55,396 INFO mapred.LocalJobRunner: Records R/W=8797587/8797163 > reduce\n","2021-11-09 23:20:00,621 INFO streaming.PipeMapRed: Records R/W=8899659/8899206\n","2021-11-09 23:20:00,636 INFO streaming.PipeMapRed: R/W/S=8900000/8899423/0 in:11337=8900000/785 [rec/s] out:11336=8899423/785 [rec/s]\n","2021-11-09 23:20:01,398 INFO mapred.LocalJobRunner: Records R/W=8899659/8899206 > reduce\n","2021-11-09 23:20:07,399 INFO mapred.LocalJobRunner: Records R/W=8899659/8899206 > reduce\n","2021-11-09 23:20:09,100 INFO streaming.PipeMapRed: R/W/S=9000000/8999255/0 in:11349=9000000/793 [rec/s] out:11348=8999255/793 [rec/s]\n","2021-11-09 23:20:10,623 INFO streaming.PipeMapRed: Records R/W=9021280/9020728\n","2021-11-09 23:20:13,399 INFO mapred.LocalJobRunner: Records R/W=9021280/9020728 > reduce\n","2021-11-09 23:20:17,486 INFO streaming.PipeMapRed: R/W/S=9100000/9099583/0 in:11360=9100000/801 [rec/s] out:11360=9099583/801 [rec/s]\n","2021-11-09 23:20:19,400 INFO mapred.LocalJobRunner: Records R/W=9021280/9020728 > reduce\n","2021-11-09 23:20:20,626 INFO streaming.PipeMapRed: Records R/W=9131689/9131042\n","2021-11-09 23:20:25,139 INFO streaming.PipeMapRed: R/W/S=9200000/9199530/0 in:11372=9200000/809 [rec/s] out:11371=9199530/809 [rec/s]\n","2021-11-09 23:20:25,401 INFO mapred.LocalJobRunner: Records R/W=9131689/9131042 > reduce\n","2021-11-09 23:20:30,628 INFO streaming.PipeMapRed: Records R/W=9272573/9272190\n","2021-11-09 23:20:31,402 INFO mapred.LocalJobRunner: Records R/W=9272573/9272190 > reduce\n","2021-11-09 23:20:33,992 INFO streaming.PipeMapRed: R/W/S=9300000/9299521/0 in:11369=9300000/818 [rec/s] out:11368=9299521/818 [rec/s]\n","2021-11-09 23:20:37,402 INFO mapred.LocalJobRunner: Records R/W=9272573/9272190 > reduce\n","2021-11-09 23:20:40,631 INFO streaming.PipeMapRed: Records R/W=9362138/9361778\n","2021-11-09 23:20:43,403 INFO mapred.LocalJobRunner: Records R/W=9362138/9361778 > reduce\n","2021-11-09 23:20:44,682 INFO streaming.PipeMapRed: R/W/S=9400000/9399464/0 in:11338=9400000/829 [rec/s] out:11338=9399464/829 [rec/s]\n","2021-11-09 23:20:49,404 INFO mapred.LocalJobRunner: Records R/W=9362138/9361778 > reduce\n","2021-11-09 23:20:50,633 INFO streaming.PipeMapRed: Records R/W=9476745/9476124\n","2021-11-09 23:20:51,894 INFO streaming.PipeMapRed: R/W/S=9500000/9499695/0 in:11363=9500000/836 [rec/s] out:11363=9499695/836 [rec/s]\n","2021-11-09 23:20:55,405 INFO mapred.LocalJobRunner: Records R/W=9476745/9476124 > reduce\n","2021-11-09 23:21:00,029 INFO streaming.PipeMapRed: R/W/S=9600000/9599644/0 in:11374=9600000/844 [rec/s] out:11373=9599644/844 [rec/s]\n","2021-11-09 23:21:00,634 INFO streaming.PipeMapRed: Records R/W=9607892/9607227\n","2021-11-09 23:21:01,405 INFO mapred.LocalJobRunner: Records R/W=9607892/9607227 > reduce\n","2021-11-09 23:21:07,406 INFO mapred.LocalJobRunner: Records R/W=9607892/9607227 > reduce\n","2021-11-09 23:21:10,061 INFO streaming.PipeMapRed: R/W/S=9700000/9699509/0 in:11358=9700000/854 [rec/s] out:11357=9699509/854 [rec/s]\n","2021-11-09 23:21:10,639 INFO streaming.PipeMapRed: Records R/W=9705668/9705166\n","2021-11-09 23:21:13,407 INFO mapred.LocalJobRunner: Records R/W=9705668/9705166 > reduce\n","2021-11-09 23:21:19,407 INFO mapred.LocalJobRunner: Records R/W=9705668/9705166 > reduce\n","2021-11-09 23:21:19,999 INFO streaming.PipeMapRed: R/W/S=9800000/9799655/0 in:11342=9800000/864 [rec/s] out:11342=9799655/864 [rec/s]\n","2021-11-09 23:21:20,643 INFO streaming.PipeMapRed: Records R/W=9806367/9805763\n","2021-11-09 23:21:25,408 INFO mapred.LocalJobRunner: Records R/W=9806367/9805763 > reduce\n","2021-11-09 23:21:30,650 INFO streaming.PipeMapRed: Records R/W=9893570/9893271\n","2021-11-09 23:21:31,408 INFO mapred.LocalJobRunner: Records R/W=9893570/9893271 > reduce\n","2021-11-09 23:21:31,742 INFO streaming.PipeMapRed: R/W/S=9900000/9899655/0 in:11301=9900000/876 [rec/s] out:11300=9899655/876 [rec/s]\n","2021-11-09 23:21:32,368 INFO mapreduce.Job:  map 100% reduce 73%\n","2021-11-09 23:21:37,409 INFO mapred.LocalJobRunner: Records R/W=9893570/9893271 > reduce\n","2021-11-09 23:21:40,676 INFO streaming.PipeMapRed: Records R/W=9945767/9945415\n","2021-11-09 23:21:43,409 INFO mapred.LocalJobRunner: Records R/W=9945767/9945415 > reduce\n","2021-11-09 23:21:49,410 INFO mapred.LocalJobRunner: Records R/W=9945767/9945415 > reduce\n","2021-11-09 23:21:50,450 INFO streaming.PipeMapRed: R/W/S=10000000/9999537/0 in:11185=10000000/894 [rec/s] out:11185=9999537/894 [rec/s]\n","2021-11-09 23:21:50,680 INFO streaming.PipeMapRed: Records R/W=10002939/10002465\n","2021-11-09 23:21:55,410 INFO mapred.LocalJobRunner: Records R/W=10002939/10002465 > reduce\n","2021-11-09 23:21:59,892 INFO streaming.PipeMapRed: R/W/S=10100000/10099437/0 in:11172=10100000/904 [rec/s] out:11171=10099437/904 [rec/s]\n","2021-11-09 23:22:00,687 INFO streaming.PipeMapRed: Records R/W=10107158/10106852\n","2021-11-09 23:22:01,411 INFO mapred.LocalJobRunner: Records R/W=10107158/10106852 > reduce\n","2021-11-09 23:22:07,411 INFO mapred.LocalJobRunner: Records R/W=10107158/10106852 > reduce\n","2021-11-09 23:22:09,975 INFO streaming.PipeMapRed: R/W/S=10200000/10199507/0 in:11159=10200000/914 [rec/s] out:11159=10199507/914 [rec/s]\n","2021-11-09 23:22:10,689 INFO streaming.PipeMapRed: Records R/W=10207634/10207338\n","2021-11-09 23:22:13,412 INFO mapred.LocalJobRunner: Records R/W=10207634/10207338 > reduce\n","2021-11-09 23:22:19,412 INFO mapred.LocalJobRunner: Records R/W=10207634/10207338 > reduce\n","2021-11-09 23:22:19,421 INFO streaming.PipeMapRed: R/W/S=10300000/10299354/0 in:11159=10300000/923 [rec/s] out:11158=10299354/923 [rec/s]\n","2021-11-09 23:22:20,691 INFO streaming.PipeMapRed: Records R/W=10313450/10312757\n","2021-11-09 23:22:25,413 INFO mapred.LocalJobRunner: Records R/W=10313450/10312757 > reduce\n","2021-11-09 23:22:29,320 INFO streaming.PipeMapRed: R/W/S=10400000/10399446/0 in:11146=10400000/933 [rec/s] out:11146=10399446/933 [rec/s]\n","2021-11-09 23:22:30,692 INFO streaming.PipeMapRed: Records R/W=10413117/10412738\n","2021-11-09 23:22:31,413 INFO mapred.LocalJobRunner: Records R/W=10413117/10412738 > reduce\n","2021-11-09 23:22:37,414 INFO mapred.LocalJobRunner: Records R/W=10413117/10412738 > reduce\n","2021-11-09 23:22:37,734 INFO streaming.PipeMapRed: R/W/S=10500000/10499360/0 in:11146=10500000/942 [rec/s] out:11145=10499360/942 [rec/s]\n","2021-11-09 23:22:40,695 INFO streaming.PipeMapRed: Records R/W=10530280/10529606\n","2021-11-09 23:22:43,414 INFO mapred.LocalJobRunner: Records R/W=10530280/10529606 > reduce\n","2021-11-09 23:22:47,401 INFO streaming.PipeMapRed: R/W/S=10600000/10599238/0 in:11146=10600000/951 [rec/s] out:11145=10599292/951 [rec/s]\n","2021-11-09 23:22:49,415 INFO mapred.LocalJobRunner: Records R/W=10530280/10529606 > reduce\n","2021-11-09 23:22:50,699 INFO streaming.PipeMapRed: Records R/W=10633714/10633281\n","2021-11-09 23:22:55,415 INFO mapred.LocalJobRunner: Records R/W=10633714/10633281 > reduce\n","2021-11-09 23:22:57,170 INFO streaming.PipeMapRed: R/W/S=10700000/10699659/0 in:11134=10700000/961 [rec/s] out:11133=10699659/961 [rec/s]\n","2021-11-09 23:23:00,705 INFO streaming.PipeMapRed: Records R/W=10731580/10731037\n","2021-11-09 23:23:01,416 INFO mapred.LocalJobRunner: Records R/W=10731580/10731037 > reduce\n","2021-11-09 23:23:07,417 INFO mapred.LocalJobRunner: Records R/W=10731580/10731037 > reduce\n","2021-11-09 23:23:09,628 INFO streaming.PipeMapRed: R/W/S=10800000/10799615/0 in:11088=10800000/974 [rec/s] out:11087=10799615/974 [rec/s]\n","2021-11-09 23:23:10,706 INFO streaming.PipeMapRed: Records R/W=10807462/10807149\n","2021-11-09 23:23:13,417 INFO mapred.LocalJobRunner: Records R/W=10807462/10807149 > reduce\n","2021-11-09 23:23:19,418 INFO mapred.LocalJobRunner: Records R/W=10807462/10807149 > reduce\n","2021-11-09 23:23:20,732 INFO streaming.PipeMapRed: Records R/W=10872589/10872364\n","2021-11-09 23:23:24,944 INFO streaming.PipeMapRed: R/W/S=10900000/10899326/0 in:11021=10900000/989 [rec/s] out:11020=10899326/989 [rec/s]\n","2021-11-09 23:23:25,419 INFO mapred.LocalJobRunner: Records R/W=10872589/10872364 > reduce\n","2021-11-09 23:23:30,737 INFO streaming.PipeMapRed: Records R/W=10964424/10963837\n","2021-11-09 23:23:31,419 INFO mapred.LocalJobRunner: Records R/W=10964424/10963837 > reduce\n","2021-11-09 23:23:33,880 INFO streaming.PipeMapRed: R/W/S=11000000/10999362/0 in:11022=11000000/998 [rec/s] out:11021=10999362/998 [rec/s]\n","2021-11-09 23:23:37,420 INFO mapred.LocalJobRunner: Records R/W=10964424/10963837 > reduce\n","2021-11-09 23:23:40,740 INFO streaming.PipeMapRed: Records R/W=11077021/11076451\n","2021-11-09 23:23:41,736 INFO streaming.PipeMapRed: R/W/S=11100000/11099480/0 in:11033=11100000/1006 [rec/s] out:11033=11099480/1006 [rec/s]\n","2021-11-09 23:23:43,420 INFO mapred.LocalJobRunner: Records R/W=11077021/11076451 > reduce\n","2021-11-09 23:23:49,421 INFO mapred.LocalJobRunner: Records R/W=11077021/11076451 > reduce\n","2021-11-09 23:23:50,585 INFO streaming.PipeMapRed: R/W/S=11200000/11199453/0 in:11034=11200000/1015 [rec/s] out:11033=11199453/1015 [rec/s]\n","2021-11-09 23:23:50,745 INFO streaming.PipeMapRed: Records R/W=11201518/11201220\n","2021-11-09 23:23:55,421 INFO mapred.LocalJobRunner: Records R/W=11201518/11201220 > reduce\n","2021-11-09 23:23:59,949 INFO streaming.PipeMapRed: R/W/S=11300000/11299586/0 in:11035=11300000/1024 [rec/s] out:11034=11299586/1024 [rec/s]\n","2021-11-09 23:24:00,752 INFO streaming.PipeMapRed: Records R/W=11306518/11306000\n","2021-11-09 23:24:01,422 INFO mapred.LocalJobRunner: Records R/W=11306518/11306000 > reduce\n","2021-11-09 23:24:07,422 INFO mapred.LocalJobRunner: Records R/W=11306518/11306000 > reduce\n","2021-11-09 23:24:10,470 INFO streaming.PipeMapRed: R/W/S=11400000/11399506/0 in:11025=11400000/1034 [rec/s] out:11024=11399506/1034 [rec/s]\n","2021-11-09 23:24:10,758 INFO streaming.PipeMapRed: Records R/W=11402192/11401836\n","2021-11-09 23:24:13,423 INFO mapred.LocalJobRunner: Records R/W=11402192/11401836 > reduce\n","2021-11-09 23:24:14,417 INFO mapreduce.Job:  map 100% reduce 74%\n","2021-11-09 23:24:19,423 INFO mapred.LocalJobRunner: Records R/W=11402192/11401836 > reduce\n","2021-11-09 23:24:20,760 INFO streaming.PipeMapRed: Records R/W=11494017/11493690\n","2021-11-09 23:24:21,712 INFO streaming.PipeMapRed: R/W/S=11500000/11499649/0 in:10994=11500000/1046 [rec/s] out:10993=11499649/1046 [rec/s]\n","2021-11-09 23:24:25,424 INFO mapred.LocalJobRunner: Records R/W=11494017/11493690 > reduce\n","2021-11-09 23:24:30,763 INFO streaming.PipeMapRed: Records R/W=11578906/11578511\n","2021-11-09 23:24:31,424 INFO mapred.LocalJobRunner: Records R/W=11578906/11578511 > reduce\n","2021-11-09 23:24:33,387 INFO streaming.PipeMapRed: R/W/S=11600000/11599646/0 in:10974=11600000/1057 [rec/s] out:10974=11599646/1057 [rec/s]\n","2021-11-09 23:24:37,425 INFO mapred.LocalJobRunner: Records R/W=11578906/11578511 > reduce\n","2021-11-09 23:24:40,768 INFO streaming.PipeMapRed: Records R/W=11665519/11665148\n","2021-11-09 23:24:43,425 INFO mapred.LocalJobRunner: Records R/W=11665519/11665148 > reduce\n","2021-11-09 23:24:44,723 INFO streaming.PipeMapRed: R/W/S=11700000/11699541/0 in:10944=11700000/1069 [rec/s] out:10944=11699541/1069 [rec/s]\n","2021-11-09 23:24:49,426 INFO mapred.LocalJobRunner: Records R/W=11665519/11665148 > reduce\n","2021-11-09 23:24:50,777 INFO streaming.PipeMapRed: Records R/W=11748706/11748216\n","2021-11-09 23:24:55,426 INFO mapred.LocalJobRunner: Records R/W=11748706/11748216 > reduce\n","2021-11-09 23:24:55,868 INFO streaming.PipeMapRed: R/W/S=11800000/11799653/0 in:10925=11800000/1080 [rec/s] out:10925=11799653/1080 [rec/s]\n","2021-11-09 23:25:00,782 INFO streaming.PipeMapRed: Records R/W=11850854/11850605\n","2021-11-09 23:25:01,427 INFO mapred.LocalJobRunner: Records R/W=11850854/11850605 > reduce\n","2021-11-09 23:25:05,133 INFO streaming.PipeMapRed: R/W/S=11900000/11899273/0 in:10927=11900000/1089 [rec/s] out:10926=11899273/1089 [rec/s]\n","2021-11-09 23:25:07,427 INFO mapred.LocalJobRunner: Records R/W=11850854/11850605 > reduce\n","2021-11-09 23:25:10,784 INFO streaming.PipeMapRed: Records R/W=11983078/11982701\n","2021-11-09 23:25:11,953 INFO streaming.PipeMapRed: R/W/S=12000000/11999234/0 in:10948=12000000/1096 [rec/s] out:10948=11999234/1096 [rec/s]\n","2021-11-09 23:25:13,447 INFO mapred.LocalJobRunner: Records R/W=11983078/11982701 > reduce\n","2021-11-09 23:25:19,448 INFO mapred.LocalJobRunner: Records R/W=11983078/11982701 > reduce\n","2021-11-09 23:25:20,789 INFO streaming.PipeMapRed: Records R/W=12092101/12091619\n","2021-11-09 23:25:21,847 INFO streaming.PipeMapRed: R/W/S=12100000/12099670/0 in:10940=12100000/1106 [rec/s] out:10940=12099670/1106 [rec/s]\n","2021-11-09 23:25:25,448 INFO mapred.LocalJobRunner: Records R/W=12092101/12091619 > reduce\n","2021-11-09 23:25:30,791 INFO streaming.PipeMapRed: Records R/W=12177751/12177193\n","2021-11-09 23:25:31,449 INFO mapred.LocalJobRunner: Records R/W=12177751/12177193 > reduce\n","2021-11-09 23:25:32,624 INFO streaming.PipeMapRed: R/W/S=12200000/12199588/0 in:10922=12200000/1117 [rec/s] out:10921=12199588/1117 [rec/s]\n","2021-11-09 23:25:37,449 INFO mapred.LocalJobRunner: Records R/W=12177751/12177193 > reduce\n","2021-11-09 23:25:39,386 INFO streaming.PipeMapRed: R/W/S=12300000/12299305/0 in:10952=12300000/1123 [rec/s] out:10952=12299305/1123 [rec/s]\n","2021-11-09 23:25:40,792 INFO streaming.PipeMapRed: Records R/W=12341815/12341090\n","2021-11-09 23:25:42,732 INFO streaming.PipeMapRed: R/W/S=12400000/12399309/0 in:11002=12400000/1127 [rec/s] out:11002=12399309/1127 [rec/s]\n","2021-11-09 23:25:43,450 INFO mapred.LocalJobRunner: Records R/W=12341815/12341090 > reduce\n","2021-11-09 23:25:49,451 INFO mapred.LocalJobRunner: Records R/W=12341815/12341090 > reduce\n","2021-11-09 23:25:50,796 INFO streaming.PipeMapRed: Records R/W=12486806/12486186\n","2021-11-09 23:25:52,440 INFO streaming.PipeMapRed: R/W/S=12500000/12499530/0 in:11003=12500000/1136 [rec/s] out:11003=12499530/1136 [rec/s]\n","2021-11-09 23:25:55,451 INFO mapred.LocalJobRunner: Records R/W=12486806/12486186 > reduce\n","2021-11-09 23:26:00,804 INFO streaming.PipeMapRed: Records R/W=12578395/12577953\n","2021-11-09 23:26:01,452 INFO mapred.LocalJobRunner: Records R/W=12578395/12577953 > reduce\n","2021-11-09 23:26:03,423 INFO streaming.PipeMapRed: R/W/S=12600000/12599630/0 in:10985=12600000/1147 [rec/s] out:10984=12599630/1147 [rec/s]\n","2021-11-09 23:26:07,452 INFO mapred.LocalJobRunner: Records R/W=12578395/12577953 > reduce\n","2021-11-09 23:26:10,805 INFO streaming.PipeMapRed: Records R/W=12685135/12684954\n","2021-11-09 23:26:12,823 INFO streaming.PipeMapRed: R/W/S=12700000/12699666/0 in:10976=12700000/1157 [rec/s] out:10976=12699666/1157 [rec/s]\n","2021-11-09 23:26:13,453 INFO mapred.LocalJobRunner: Records R/W=12685135/12684954 > reduce\n","2021-11-09 23:26:19,453 INFO mapred.LocalJobRunner: Records R/W=12685135/12684954 > reduce\n","2021-11-09 23:26:20,806 INFO streaming.PipeMapRed: Records R/W=12781016/12780218\n","2021-11-09 23:26:23,192 INFO streaming.PipeMapRed: R/W/S=12800000/12799761/0 in:10968=12800000/1167 [rec/s] out:10968=12799761/1167 [rec/s]\n","2021-11-09 23:26:25,454 INFO mapred.LocalJobRunner: Records R/W=12781016/12780218 > reduce\n","2021-11-09 23:26:30,807 INFO streaming.PipeMapRed: Records R/W=12860515/12860221\n","2021-11-09 23:26:31,456 INFO mapred.LocalJobRunner: Records R/W=12860515/12860221 > reduce\n","2021-11-09 23:26:34,615 INFO streaming.PipeMapRed: R/W/S=12900000/12899477/0 in:10941=12900000/1179 [rec/s] out:10941=12899477/1179 [rec/s]\n","2021-11-09 23:26:37,456 INFO mapred.LocalJobRunner: Records R/W=12860515/12860221 > reduce\n","2021-11-09 23:26:37,466 INFO mapreduce.Job:  map 100% reduce 75%\n","2021-11-09 23:26:40,815 INFO streaming.PipeMapRed: Records R/W=12973654/12973496\n","2021-11-09 23:26:43,457 INFO mapred.LocalJobRunner: Records R/W=12973654/12973496 > reduce\n","2021-11-09 23:26:43,932 INFO streaming.PipeMapRed: R/W/S=13000000/12999505/0 in:10942=13000000/1188 [rec/s] out:10942=12999505/1188 [rec/s]\n","2021-11-09 23:26:49,457 INFO mapred.LocalJobRunner: Records R/W=12973654/12973496 > reduce\n","2021-11-09 23:26:50,821 INFO streaming.PipeMapRed: Records R/W=13061366/13060862\n","2021-11-09 23:26:55,458 INFO mapred.LocalJobRunner: Records R/W=13061366/13060862 > reduce\n","2021-11-09 23:26:56,572 INFO streaming.PipeMapRed: R/W/S=13100000/13099737/0 in:10907=13100000/1201 [rec/s] out:10907=13099737/1201 [rec/s]\n","2021-11-09 23:27:00,832 INFO streaming.PipeMapRed: Records R/W=13154287/13153998\n","2021-11-09 23:27:01,458 INFO mapred.LocalJobRunner: Records R/W=13154287/13153998 > reduce\n","2021-11-09 23:27:07,422 INFO streaming.PipeMapRed: R/W/S=13200000/13199560/0 in:10900=13200000/1211 [rec/s] out:10899=13199560/1211 [rec/s]\n","2021-11-09 23:27:07,459 INFO mapred.LocalJobRunner: Records R/W=13154287/13153998 > reduce\n","2021-11-09 23:27:10,834 INFO streaming.PipeMapRed: Records R/W=13233140/13232684\n","2021-11-09 23:27:13,461 INFO mapred.LocalJobRunner: Records R/W=13233140/13232684 > reduce\n","2021-11-09 23:27:17,559 INFO streaming.PipeMapRed: R/W/S=13300000/13299751/0 in:10892=13300000/1221 [rec/s] out:10892=13299751/1221 [rec/s]\n","2021-11-09 23:27:19,461 INFO mapred.LocalJobRunner: Records R/W=13233140/13232684 > reduce\n","2021-11-09 23:27:20,837 INFO streaming.PipeMapRed: Records R/W=13335948/13335264\n","2021-11-09 23:27:25,462 INFO mapred.LocalJobRunner: Records R/W=13335948/13335264 > reduce\n","2021-11-09 23:27:26,345 INFO streaming.PipeMapRed: R/W/S=13400000/13399698/0 in:10894=13400000/1230 [rec/s] out:10894=13399698/1230 [rec/s]\n","2021-11-09 23:27:30,840 INFO streaming.PipeMapRed: Records R/W=13455913/13455380\n","2021-11-09 23:27:31,462 INFO mapred.LocalJobRunner: Records R/W=13455913/13455380 > reduce\n","2021-11-09 23:27:33,545 INFO streaming.PipeMapRed: R/W/S=13500000/13498910/0 in:10913=13500000/1237 [rec/s] out:10912=13498910/1237 [rec/s]\n","2021-11-09 23:27:37,463 INFO mapred.LocalJobRunner: Records R/W=13455913/13455380 > reduce\n","2021-11-09 23:27:40,841 INFO streaming.PipeMapRed: Records R/W=13592517/13591791\n","2021-11-09 23:27:41,502 INFO streaming.PipeMapRed: R/W/S=13600000/13599357/0 in:10923=13600000/1245 [rec/s] out:10923=13599357/1245 [rec/s]\n","2021-11-09 23:27:43,463 INFO mapred.LocalJobRunner: Records R/W=13592517/13591791 > reduce\n","2021-11-09 23:27:49,464 INFO mapred.LocalJobRunner: Records R/W=13592517/13591791 > reduce\n","2021-11-09 23:27:50,843 INFO streaming.PipeMapRed: Records R/W=13692993/13692392\n","2021-11-09 23:27:51,239 INFO streaming.PipeMapRed: R/W/S=13700000/13699618/0 in:10916=13700000/1255 [rec/s] out:10916=13699618/1255 [rec/s]\n","2021-11-09 23:27:55,464 INFO mapred.LocalJobRunner: Records R/W=13692993/13692392 > reduce\n","2021-11-09 23:27:55,853 INFO streaming.PipeMapRed: R/W/S=13800000/13799493/0 in:10952=13800000/1260 [rec/s] out:10951=13799493/1260 [rec/s]\n","2021-11-09 23:28:00,845 INFO streaming.PipeMapRed: Records R/W=13881241/13880811\n","2021-11-09 23:28:01,465 INFO mapred.LocalJobRunner: Records R/W=13881241/13880811 > reduce\n","2021-11-09 23:28:01,631 INFO streaming.PipeMapRed: R/W/S=13900000/13899161/0 in:10979=13900000/1266 [rec/s] out:10978=13899161/1266 [rec/s]\n","2021-11-09 23:28:07,465 INFO mapred.LocalJobRunner: Records R/W=13881241/13880811 > reduce\n","2021-11-09 23:28:08,961 INFO streaming.PipeMapRed: R/W/S=14000000/13999461/0 in:10997=14000000/1273 [rec/s] out:10997=13999461/1273 [rec/s]\n","2021-11-09 23:28:10,848 INFO streaming.PipeMapRed: Records R/W=14026549/14025774\n","2021-11-09 23:28:13,466 INFO mapred.LocalJobRunner: Records R/W=14026549/14025774 > reduce\n","2021-11-09 23:28:18,095 INFO streaming.PipeMapRed: R/W/S=14100000/14099567/0 in:10998=14100000/1282 [rec/s] out:10998=14099567/1282 [rec/s]\n","2021-11-09 23:28:19,466 INFO mapred.LocalJobRunner: Records R/W=14026549/14025774 > reduce\n","2021-11-09 23:28:20,852 INFO streaming.PipeMapRed: Records R/W=14126183/14125847\n","2021-11-09 23:28:25,467 INFO mapred.LocalJobRunner: Records R/W=14126183/14125847 > reduce\n","2021-11-09 23:28:28,963 INFO streaming.PipeMapRed: R/W/S=14200000/14199585/0 in:10982=14200000/1293 [rec/s] out:10981=14199585/1293 [rec/s]\n","2021-11-09 23:28:30,853 INFO streaming.PipeMapRed: Records R/W=14224042/14223612\n","2021-11-09 23:28:31,467 INFO mapred.LocalJobRunner: Records R/W=14224042/14223612 > reduce\n","2021-11-09 23:28:36,987 INFO streaming.PipeMapRed: R/W/S=14300000/14299394/0 in:10991=14300000/1301 [rec/s] out:10991=14299394/1301 [rec/s]\n","2021-11-09 23:28:37,468 INFO mapred.LocalJobRunner: Records R/W=14224042/14223612 > reduce\n","2021-11-09 23:28:40,857 INFO streaming.PipeMapRed: Records R/W=14341838/14341503\n","2021-11-09 23:28:43,469 INFO mapred.LocalJobRunner: Records R/W=14341838/14341503 > reduce\n","2021-11-09 23:28:46,028 INFO streaming.PipeMapRed: R/W/S=14400000/14399161/0 in:10992=14400000/1310 [rec/s] out:10991=14399161/1310 [rec/s]\n","2021-11-09 23:28:49,469 INFO mapred.LocalJobRunner: Records R/W=14341838/14341503 > reduce\n","2021-11-09 23:28:50,859 INFO streaming.PipeMapRed: Records R/W=14454162/14453707\n","2021-11-09 23:28:55,190 INFO streaming.PipeMapRed: R/W/S=14500000/14499592/0 in:10993=14500000/1319 [rec/s] out:10992=14499592/1319 [rec/s]\n","2021-11-09 23:28:55,477 INFO mapred.LocalJobRunner: Records R/W=14454162/14453707 > reduce\n","2021-11-09 23:29:00,864 INFO streaming.PipeMapRed: Records R/W=14546767/14546272\n","2021-11-09 23:29:01,478 INFO mapred.LocalJobRunner: Records R/W=14546767/14546272 > reduce\n","2021-11-09 23:29:05,952 INFO streaming.PipeMapRed: R/W/S=14600000/14599523/0 in:10977=14600000/1330 [rec/s] out:10977=14599523/1330 [rec/s]\n","2021-11-09 23:29:07,478 INFO mapred.LocalJobRunner: Records R/W=14546767/14546272 > reduce\n","2021-11-09 23:29:07,539 INFO mapreduce.Job:  map 100% reduce 76%\n","2021-11-09 23:29:10,867 INFO streaming.PipeMapRed: Records R/W=14649264/14648561\n","2021-11-09 23:29:13,481 INFO mapred.LocalJobRunner: Records R/W=14649264/14648561 > reduce\n","2021-11-09 23:29:15,986 INFO streaming.PipeMapRed: R/W/S=14700000/14699453/0 in:10970=14700000/1340 [rec/s] out:10969=14699453/1340 [rec/s]\n","2021-11-09 23:29:19,482 INFO mapred.LocalJobRunner: Records R/W=14649264/14648561 > reduce\n","2021-11-09 23:29:20,874 INFO streaming.PipeMapRed: Records R/W=14769270/14768808\n","2021-11-09 23:29:23,972 INFO streaming.PipeMapRed: R/W/S=14800000/14799632/0 in:10979=14800000/1348 [rec/s] out:10978=14799632/1348 [rec/s]\n","2021-11-09 23:29:25,487 INFO mapred.LocalJobRunner: Records R/W=14769270/14768808 > reduce\n","2021-11-09 23:29:30,876 INFO streaming.PipeMapRed: Records R/W=14867665/14867330\n","2021-11-09 23:29:31,489 INFO mapred.LocalJobRunner: Records R/W=14867665/14867330 > reduce\n","2021-11-09 23:29:35,297 INFO streaming.PipeMapRed: R/W/S=14900000/14899621/0 in:10963=14900000/1359 [rec/s] out:10963=14899621/1359 [rec/s]\n","2021-11-09 23:29:37,489 INFO mapred.LocalJobRunner: Records R/W=14867665/14867330 > reduce\n","2021-11-09 23:29:40,877 INFO streaming.PipeMapRed: Records R/W=14979062/14978763\n","2021-11-09 23:29:43,190 INFO streaming.PipeMapRed: R/W/S=15000000/14999697/0 in:10972=15000000/1367 [rec/s] out:10972=14999697/1367 [rec/s]\n","2021-11-09 23:29:43,490 INFO mapred.LocalJobRunner: Records R/W=14979062/14978763 > reduce\n","2021-11-09 23:29:49,490 INFO mapred.LocalJobRunner: Records R/W=14979062/14978763 > reduce\n","2021-11-09 23:29:50,879 INFO streaming.PipeMapRed: Records R/W=15083751/15083581\n","2021-11-09 23:29:52,932 INFO streaming.PipeMapRed: R/W/S=15100000/15099784/0 in:10965=15100000/1377 [rec/s] out:10965=15099784/1377 [rec/s]\n","2021-11-09 23:29:55,491 INFO mapred.LocalJobRunner: Records R/W=15083751/15083581 > reduce\n","2021-11-09 23:30:00,059 INFO streaming.PipeMapRed: R/W/S=15200000/15199561/0 in:10982=15200000/1384 [rec/s] out:10982=15199561/1384 [rec/s]\n","2021-11-09 23:30:00,883 INFO streaming.PipeMapRed: Records R/W=15209275/15208985\n","2021-11-09 23:30:01,491 INFO mapred.LocalJobRunner: Records R/W=15209275/15208985 > reduce\n","2021-11-09 23:30:07,493 INFO mapred.LocalJobRunner: Records R/W=15209275/15208985 > reduce\n","2021-11-09 23:30:09,066 INFO streaming.PipeMapRed: R/W/S=15300000/15299532/0 in:10983=15300000/1393 [rec/s] out:10983=15299532/1393 [rec/s]\n","2021-11-09 23:30:10,884 INFO streaming.PipeMapRed: Records R/W=15322032/15321358\n","2021-11-09 23:30:13,493 INFO mapred.LocalJobRunner: Records R/W=15322032/15321358 > reduce\n","2021-11-09 23:30:17,751 INFO streaming.PipeMapRed: R/W/S=15400000/15399669/0 in:10984=15400000/1402 [rec/s] out:10984=15399669/1402 [rec/s]\n","2021-11-09 23:30:19,494 INFO mapred.LocalJobRunner: Records R/W=15322032/15321358 > reduce\n","2021-11-09 23:30:20,888 INFO streaming.PipeMapRed: Records R/W=15433800/15433387\n","2021-11-09 23:30:25,494 INFO mapred.LocalJobRunner: Records R/W=15433800/15433387 > reduce\n","2021-11-09 23:30:28,589 INFO streaming.PipeMapRed: R/W/S=15500000/15499528/0 in:10969=15500000/1413 [rec/s] out:10969=15499528/1413 [rec/s]\n","2021-11-09 23:30:30,889 INFO streaming.PipeMapRed: Records R/W=15518888/15518401\n","2021-11-09 23:30:31,495 INFO mapred.LocalJobRunner: Records R/W=15518888/15518401 > reduce\n","2021-11-09 23:30:37,495 INFO mapred.LocalJobRunner: Records R/W=15518888/15518401 > reduce\n","2021-11-09 23:30:40,417 INFO streaming.PipeMapRed: R/W/S=15600000/15599780/0 in:10955=15600000/1424 [rec/s] out:10954=15599780/1424 [rec/s]\n","2021-11-09 23:30:40,892 INFO streaming.PipeMapRed: Records R/W=15603888/15603512\n","2021-11-09 23:30:43,496 INFO mapred.LocalJobRunner: Records R/W=15603888/15603512 > reduce\n","2021-11-09 23:30:49,496 INFO mapred.LocalJobRunner: Records R/W=15603888/15603512 > reduce\n","2021-11-09 23:30:50,395 INFO streaming.PipeMapRed: R/W/S=15700000/15699786/0 in:10948=15700000/1434 [rec/s] out:10948=15699786/1434 [rec/s]\n","2021-11-09 23:30:50,905 INFO streaming.PipeMapRed: Records R/W=15705076/15704754\n","2021-11-09 23:30:55,497 INFO mapred.LocalJobRunner: Records R/W=15705076/15704754 > reduce\n","2021-11-09 23:31:00,906 INFO streaming.PipeMapRed: Records R/W=15781064/15780742\n","2021-11-09 23:31:01,498 INFO mapred.LocalJobRunner: Records R/W=15781064/15780742 > reduce\n","2021-11-09 23:31:02,598 INFO streaming.PipeMapRed: R/W/S=15800000/15798700/0 in:10919=15800000/1447 [rec/s] out:10918=15798700/1447 [rec/s]\n","2021-11-09 23:31:07,498 INFO mapred.LocalJobRunner: Records R/W=15781064/15780742 > reduce\n","2021-11-09 23:31:10,911 INFO streaming.PipeMapRed: Records R/W=15896052/15895350\n","2021-11-09 23:31:11,250 INFO streaming.PipeMapRed: R/W/S=15900000/15899558/0 in:10927=15900000/1455 [rec/s] out:10927=15899558/1455 [rec/s]\n","2021-11-09 23:31:13,499 INFO mapred.LocalJobRunner: Records R/W=15896052/15895350 > reduce\n","2021-11-09 23:31:19,500 INFO mapred.LocalJobRunner: Records R/W=15896052/15895350 > reduce\n","2021-11-09 23:31:20,913 INFO streaming.PipeMapRed: Records R/W=15990425/15990002\n","2021-11-09 23:31:21,893 INFO streaming.PipeMapRed: R/W/S=16000000/15999706/0 in:10914=16000000/1466 [rec/s] out:10913=15999706/1466 [rec/s]\n","2021-11-09 23:31:25,500 INFO mapred.LocalJobRunner: Records R/W=15990425/15990002 > reduce\n","2021-11-09 23:31:27,868 INFO streaming.PipeMapRed: R/W/S=16100000/16099235/0 in:10937=16100000/1472 [rec/s] out:10936=16099235/1472 [rec/s]\n","2021-11-09 23:31:30,917 INFO streaming.PipeMapRed: Records R/W=16138923/16138444\n","2021-11-09 23:31:31,501 INFO mapred.LocalJobRunner: Records R/W=16138923/16138444 > reduce\n","2021-11-09 23:31:31,580 INFO mapreduce.Job:  map 100% reduce 77%\n","2021-11-09 23:31:35,922 INFO streaming.PipeMapRed: R/W/S=16200000/16199386/0 in:10945=16200000/1480 [rec/s] out:10945=16199386/1480 [rec/s]\n","2021-11-09 23:31:37,503 INFO mapred.LocalJobRunner: Records R/W=16138923/16138444 > reduce\n","2021-11-09 23:31:40,921 INFO streaming.PipeMapRed: Records R/W=16271762/16271153\n","2021-11-09 23:31:43,229 INFO streaming.PipeMapRed: R/W/S=16300000/16299466/0 in:10961=16300000/1487 [rec/s] out:10961=16299466/1487 [rec/s]\n","2021-11-09 23:31:43,504 INFO mapred.LocalJobRunner: Records R/W=16271762/16271153 > reduce\n","2021-11-09 23:31:49,504 INFO mapred.LocalJobRunner: Records R/W=16271762/16271153 > reduce\n","2021-11-09 23:31:50,923 INFO streaming.PipeMapRed: Records R/W=16374129/16373553\n","2021-11-09 23:31:52,751 INFO streaming.PipeMapRed: R/W/S=16400000/16399465/0 in:10955=16400000/1497 [rec/s] out:10954=16399465/1497 [rec/s]\n","2021-11-09 23:31:55,505 INFO mapred.LocalJobRunner: Records R/W=16374129/16373553 > reduce\n","2021-11-09 23:32:00,929 INFO streaming.PipeMapRed: Records R/W=16487691/16487406\n","2021-11-09 23:32:01,506 INFO mapred.LocalJobRunner: Records R/W=16487691/16487406 > reduce\n","2021-11-09 23:32:02,323 INFO streaming.PipeMapRed: R/W/S=16500000/16499660/0 in:10956=16500000/1506 [rec/s] out:10955=16499660/1506 [rec/s]\n","2021-11-09 23:32:07,506 INFO mapred.LocalJobRunner: Records R/W=16487691/16487406 > reduce\n","2021-11-09 23:32:10,931 INFO streaming.PipeMapRed: Records R/W=16582413/16581669\n","2021-11-09 23:32:12,670 INFO streaming.PipeMapRed: R/W/S=16600000/16599264/0 in:10942=16600000/1517 [rec/s] out:10942=16599264/1517 [rec/s]\n","2021-11-09 23:32:13,507 INFO mapred.LocalJobRunner: Records R/W=16582413/16581669 > reduce\n","2021-11-09 23:32:19,507 INFO mapred.LocalJobRunner: Records R/W=16582413/16581669 > reduce\n","2021-11-09 23:32:20,932 INFO streaming.PipeMapRed: Records R/W=16691459/16691079\n","2021-11-09 23:32:21,679 INFO streaming.PipeMapRed: R/W/S=16700000/16699606/0 in:10943=16700000/1526 [rec/s] out:10943=16699606/1526 [rec/s]\n","2021-11-09 23:32:25,508 INFO mapred.LocalJobRunner: Records R/W=16691459/16691079 > reduce\n","2021-11-09 23:32:30,937 INFO streaming.PipeMapRed: Records R/W=16767768/16767488\n","2021-11-09 23:32:31,508 INFO mapred.LocalJobRunner: Records R/W=16767768/16767488 > reduce\n","2021-11-09 23:32:33,835 INFO streaming.PipeMapRed: R/W/S=16800000/16799425/0 in:10923=16800000/1538 [rec/s] out:10922=16799425/1538 [rec/s]\n","2021-11-09 23:32:37,509 INFO mapred.LocalJobRunner: Records R/W=16767768/16767488 > reduce\n","2021-11-09 23:32:40,939 INFO streaming.PipeMapRed: Records R/W=16877814/16877182\n","2021-11-09 23:32:42,563 INFO streaming.PipeMapRed: R/W/S=16900000/16899205/0 in:10931=16900000/1546 [rec/s] out:10930=16899205/1546 [rec/s]\n","2021-11-09 23:32:43,512 INFO mapred.LocalJobRunner: Records R/W=16877814/16877182 > reduce\n","2021-11-09 23:32:49,515 INFO mapred.LocalJobRunner: Records R/W=16877814/16877182 > reduce\n","2021-11-09 23:32:50,944 INFO streaming.PipeMapRed: Records R/W=16989090/16988631\n","2021-11-09 23:32:51,976 INFO streaming.PipeMapRed: R/W/S=17000000/16999819/0 in:10925=17000000/1556 [rec/s] out:10925=16999819/1556 [rec/s]\n","2021-11-09 23:32:55,516 INFO mapred.LocalJobRunner: Records R/W=16989090/16988631 > reduce\n","2021-11-09 23:32:59,733 INFO streaming.PipeMapRed: R/W/S=17100000/17099309/0 in:10933=17100000/1564 [rec/s] out:10933=17099309/1564 [rec/s]\n","2021-11-09 23:33:00,946 INFO streaming.PipeMapRed: Records R/W=17111379/17110831\n","2021-11-09 23:33:01,516 INFO mapred.LocalJobRunner: Records R/W=17111379/17110831 > reduce\n","2021-11-09 23:33:07,517 INFO mapred.LocalJobRunner: Records R/W=17111379/17110831 > reduce\n","2021-11-09 23:33:10,524 INFO streaming.PipeMapRed: R/W/S=17200000/17199296/0 in:10927=17200000/1574 [rec/s] out:10927=17199296/1574 [rec/s]\n","2021-11-09 23:33:10,949 INFO streaming.PipeMapRed: Records R/W=17204657/17204128\n","2021-11-09 23:33:13,517 INFO mapred.LocalJobRunner: Records R/W=17204657/17204128 > reduce\n","2021-11-09 23:33:19,014 INFO streaming.PipeMapRed: R/W/S=17300000/17299659/0 in:10928=17300000/1583 [rec/s] out:10928=17299659/1583 [rec/s]\n","2021-11-09 23:33:19,518 INFO mapred.LocalJobRunner: Records R/W=17204657/17204128 > reduce\n","2021-11-09 23:33:20,950 INFO streaming.PipeMapRed: Records R/W=17320963/17320682\n","2021-11-09 23:33:25,518 INFO mapred.LocalJobRunner: Records R/W=17320963/17320682 > reduce\n","2021-11-09 23:33:28,462 INFO streaming.PipeMapRed: R/W/S=17400000/17399267/0 in:10929=17400000/1592 [rec/s] out:10929=17399267/1592 [rec/s]\n","2021-11-09 23:33:30,951 INFO streaming.PipeMapRed: Records R/W=17411115/17410371\n","2021-11-09 23:33:31,519 INFO mapred.LocalJobRunner: Records R/W=17411115/17410371 > reduce\n","2021-11-09 23:33:37,520 INFO mapred.LocalJobRunner: Records R/W=17411115/17410371 > reduce\n","2021-11-09 23:33:39,788 INFO streaming.PipeMapRed: R/W/S=17500000/17499576/0 in:10910=17500000/1604 [rec/s] out:10909=17499576/1604 [rec/s]\n","2021-11-09 23:33:40,955 INFO streaming.PipeMapRed: Records R/W=17510746/17510185\n","2021-11-09 23:33:43,520 INFO mapred.LocalJobRunner: Records R/W=17510746/17510185 > reduce\n","2021-11-09 23:33:49,521 INFO mapred.LocalJobRunner: Records R/W=17510746/17510185 > reduce\n","2021-11-09 23:33:50,827 INFO streaming.PipeMapRed: R/W/S=17600000/17599474/0 in:10897=17600000/1615 [rec/s] out:10897=17599474/1615 [rec/s]\n","2021-11-09 23:33:50,957 INFO streaming.PipeMapRed: Records R/W=17600846/17600536\n","2021-11-09 23:33:55,523 INFO mapred.LocalJobRunner: Records R/W=17600846/17600536 > reduce\n","2021-11-09 23:34:00,961 INFO streaming.PipeMapRed: Records R/W=17695370/17694901\n","2021-11-09 23:34:01,523 INFO mapred.LocalJobRunner: Records R/W=17695370/17694901 > reduce\n","2021-11-09 23:34:01,538 INFO streaming.PipeMapRed: R/W/S=17700000/17699543/0 in:10892=17700000/1625 [rec/s] out:10892=17699543/1625 [rec/s]\n","2021-11-09 23:34:07,524 INFO mapred.LocalJobRunner: Records R/W=17695370/17694901 > reduce\n","2021-11-09 23:34:07,635 INFO mapreduce.Job:  map 100% reduce 78%\n","2021-11-09 23:34:10,963 INFO streaming.PipeMapRed: Records R/W=17789731/17789350\n","2021-11-09 23:34:11,953 INFO streaming.PipeMapRed: R/W/S=17800000/17799373/0 in:10880=17800000/1636 [rec/s] out:10879=17799373/1636 [rec/s]\n","2021-11-09 23:34:13,524 INFO mapred.LocalJobRunner: Records R/W=17789731/17789350 > reduce\n","2021-11-09 23:34:19,524 INFO mapred.LocalJobRunner: Records R/W=17789731/17789350 > reduce\n","2021-11-09 23:34:19,602 INFO streaming.PipeMapRed: R/W/S=17900000/17898994/0 in:10888=17900000/1644 [rec/s] out:10887=17898994/1644 [rec/s]\n","2021-11-09 23:34:20,964 INFO streaming.PipeMapRed: Records R/W=17933644/17932755\n","2021-11-09 23:34:25,525 INFO mapred.LocalJobRunner: Records R/W=17933644/17932755 > reduce\n","2021-11-09 23:34:27,183 INFO streaming.PipeMapRed: R/W/S=18000000/17999247/0 in:10902=18000000/1651 [rec/s] out:10902=17999247/1651 [rec/s]\n","2021-11-09 23:34:30,967 INFO streaming.PipeMapRed: Records R/W=18034587/18034108\n","2021-11-09 23:34:31,525 INFO mapred.LocalJobRunner: Records R/W=18034587/18034108 > reduce\n","2021-11-09 23:34:37,526 INFO mapred.LocalJobRunner: Records R/W=18034587/18034108 > reduce\n","2021-11-09 23:34:38,048 INFO streaming.PipeMapRed: R/W/S=18100000/18099464/0 in:10890=18100000/1662 [rec/s] out:10890=18099464/1662 [rec/s]\n","2021-11-09 23:34:40,970 INFO streaming.PipeMapRed: Records R/W=18149720/18149466\n","2021-11-09 23:34:43,526 INFO mapred.LocalJobRunner: Records R/W=18149720/18149466 > reduce\n","2021-11-09 23:34:45,763 INFO streaming.PipeMapRed: R/W/S=18200000/18199699/0 in:10898=18200000/1670 [rec/s] out:10898=18199699/1670 [rec/s]\n","2021-11-09 23:34:49,527 INFO mapred.LocalJobRunner: Records R/W=18149720/18149466 > reduce\n","2021-11-09 23:34:50,976 INFO streaming.PipeMapRed: Records R/W=18263896/18263188\n","2021-11-09 23:34:54,580 INFO streaming.PipeMapRed: R/W/S=18300000/18299467/0 in:10899=18300000/1679 [rec/s] out:10899=18299467/1679 [rec/s]\n","2021-11-09 23:34:55,527 INFO mapred.LocalJobRunner: Records R/W=18263896/18263188 > reduce\n","2021-11-09 23:35:00,990 INFO streaming.PipeMapRed: Records R/W=18346329/18346043\n","2021-11-09 23:35:01,528 INFO mapred.LocalJobRunner: Records R/W=18346329/18346043 > reduce\n","2021-11-09 23:35:07,528 INFO mapred.LocalJobRunner: Records R/W=18346329/18346043 > reduce\n","2021-11-09 23:35:08,482 INFO streaming.PipeMapRed: R/W/S=18400000/18399585/0 in:10874=18400000/1692 [rec/s] out:10874=18399585/1692 [rec/s]\n","2021-11-09 23:35:10,993 INFO streaming.PipeMapRed: Records R/W=18436174/18435463\n","2021-11-09 23:35:13,529 INFO mapred.LocalJobRunner: Records R/W=18436174/18435463 > reduce\n","2021-11-09 23:35:15,761 INFO streaming.PipeMapRed: R/W/S=18500000/18499654/0 in:10882=18500000/1700 [rec/s] out:10882=18499654/1700 [rec/s]\n","2021-11-09 23:35:19,529 INFO mapred.LocalJobRunner: Records R/W=18436174/18435463 > reduce\n","2021-11-09 23:35:20,999 INFO streaming.PipeMapRed: Records R/W=18575561/18575145\n","2021-11-09 23:35:23,777 INFO streaming.PipeMapRed: R/W/S=18600000/18599487/0 in:10889=18600000/1708 [rec/s] out:10889=18599487/1708 [rec/s]\n","2021-11-09 23:35:25,530 INFO mapred.LocalJobRunner: Records R/W=18575561/18575145 > reduce\n","2021-11-09 23:35:31,003 INFO streaming.PipeMapRed: Records R/W=18671003/18670619\n","2021-11-09 23:35:31,530 INFO mapred.LocalJobRunner: Records R/W=18671003/18670619 > reduce\n","2021-11-09 23:35:34,203 INFO streaming.PipeMapRed: R/W/S=18700000/18699427/0 in:10884=18700000/1718 [rec/s] out:10884=18699427/1718 [rec/s]\n","2021-11-09 23:35:37,531 INFO mapred.LocalJobRunner: Records R/W=18671003/18670619 > reduce\n","2021-11-09 23:35:41,004 INFO streaming.PipeMapRed: Records R/W=18764804/18764502\n","2021-11-09 23:35:43,444 INFO streaming.PipeMapRed: R/W/S=18800000/18799469/0 in:10885=18800000/1727 [rec/s] out:10885=18799469/1727 [rec/s]\n","2021-11-09 23:35:43,531 INFO mapred.LocalJobRunner: Records R/W=18764804/18764502 > reduce\n","2021-11-09 23:35:48,181 INFO streaming.PipeMapRed: R/W/S=18900000/18899168/0 in:10912=18900000/1732 [rec/s] out:10911=18899168/1732 [rec/s]\n","2021-11-09 23:35:49,531 INFO mapred.LocalJobRunner: Records R/W=18764804/18764502 > reduce\n","2021-11-09 23:35:51,006 INFO streaming.PipeMapRed: Records R/W=18950282/18949850\n","2021-11-09 23:35:53,171 INFO streaming.PipeMapRed: R/W/S=19000000/18999354/0 in:10938=19000000/1737 [rec/s] out:10938=18999354/1737 [rec/s]\n","2021-11-09 23:35:55,532 INFO mapred.LocalJobRunner: Records R/W=18950282/18949850 > reduce\n","2021-11-09 23:35:57,123 INFO streaming.PipeMapRed: R/W/S=19100000/19099652/0 in:10970=19100000/1741 [rec/s] out:10970=19099652/1741 [rec/s]\n","2021-11-09 23:36:01,011 INFO streaming.PipeMapRed: Records R/W=19134149/19133859\n","2021-11-09 23:36:01,532 INFO mapred.LocalJobRunner: Records R/W=19134149/19133859 > reduce\n","2021-11-09 23:36:07,533 INFO mapred.LocalJobRunner: Records R/W=19134149/19133859 > reduce\n","2021-11-09 23:36:07,574 INFO streaming.PipeMapRed: R/W/S=19200000/19199743/0 in:10958=19200000/1752 [rec/s] out:10958=19199743/1752 [rec/s]\n","2021-11-09 23:36:11,015 INFO streaming.PipeMapRed: Records R/W=19238854/19238339\n","2021-11-09 23:36:13,533 INFO mapred.LocalJobRunner: Records R/W=19238854/19238339 > reduce\n","2021-11-09 23:36:17,873 INFO streaming.PipeMapRed: R/W/S=19300000/19299577/0 in:10953=19300000/1762 [rec/s] out:10953=19299577/1762 [rec/s]\n","2021-11-09 23:36:19,534 INFO mapred.LocalJobRunner: Records R/W=19238854/19238339 > reduce\n","2021-11-09 23:36:21,016 INFO streaming.PipeMapRed: Records R/W=19326386/19326094\n","2021-11-09 23:36:25,534 INFO mapred.LocalJobRunner: Records R/W=19326386/19326094 > reduce\n","2021-11-09 23:36:30,120 INFO streaming.PipeMapRed: R/W/S=19400000/19399652/0 in:10935=19400000/1774 [rec/s] out:10935=19399652/1774 [rec/s]\n","2021-11-09 23:36:31,020 INFO streaming.PipeMapRed: Records R/W=19409176/19408883\n","2021-11-09 23:36:31,535 INFO mapred.LocalJobRunner: Records R/W=19409176/19408883 > reduce\n","2021-11-09 23:36:31,704 INFO mapreduce.Job:  map 100% reduce 79%\n","2021-11-09 23:36:37,535 INFO mapred.LocalJobRunner: Records R/W=19409176/19408883 > reduce\n","2021-11-09 23:36:38,330 INFO streaming.PipeMapRed: R/W/S=19500000/19499710/0 in:10942=19500000/1782 [rec/s] out:10942=19499710/1782 [rec/s]\n","2021-11-09 23:36:41,023 INFO streaming.PipeMapRed: Records R/W=19541704/19541156\n","2021-11-09 23:36:43,535 INFO mapred.LocalJobRunner: Records R/W=19541704/19541156 > reduce\n","2021-11-09 23:36:45,735 INFO streaming.PipeMapRed: R/W/S=19600000/19599307/0 in:10949=19600000/1790 [rec/s] out:10949=19599307/1790 [rec/s]\n","2021-11-09 23:36:49,536 INFO mapred.LocalJobRunner: Records R/W=19541704/19541156 > reduce\n","2021-11-09 23:36:51,025 INFO streaming.PipeMapRed: Records R/W=19680263/19679807\n","2021-11-09 23:36:52,437 INFO streaming.PipeMapRed: R/W/S=19700000/19699393/0 in:10968=19700000/1796 [rec/s] out:10968=19699393/1796 [rec/s]\n","2021-11-09 23:36:55,536 INFO mapred.LocalJobRunner: Records R/W=19680263/19679807 > reduce\n","2021-11-09 23:36:58,972 INFO streaming.PipeMapRed: R/W/S=19800000/19799376/0 in:10981=19800000/1803 [rec/s] out:10981=19799376/1803 [rec/s]\n","2021-11-09 23:37:01,026 INFO streaming.PipeMapRed: Records R/W=19823940/19823674\n","2021-11-09 23:37:01,537 INFO mapred.LocalJobRunner: Records R/W=19823940/19823674 > reduce\n","2021-11-09 23:37:06,406 INFO streaming.PipeMapRed: R/W/S=19900000/19898984/0 in:10994=19900000/1810 [rec/s] out:10993=19898984/1810 [rec/s]\n","2021-11-09 23:37:07,537 INFO mapred.LocalJobRunner: Records R/W=19823940/19823674 > reduce\n","2021-11-09 23:37:11,029 INFO streaming.PipeMapRed: Records R/W=19955301/19954780\n","2021-11-09 23:37:13,538 INFO mapred.LocalJobRunner: Records R/W=19955301/19954780 > reduce\n","2021-11-09 23:37:15,609 INFO streaming.PipeMapRed: R/W/S=20000000/19999557/0 in:10989=20000000/1820 [rec/s] out:10988=19999557/1820 [rec/s]\n","2021-11-09 23:37:19,538 INFO mapred.LocalJobRunner: Records R/W=19955301/19954780 > reduce\n","2021-11-09 23:37:21,030 INFO streaming.PipeMapRed: Records R/W=20056685/20055907\n","2021-11-09 23:37:25,298 INFO streaming.PipeMapRed: R/W/S=20100000/20099700/0 in:10989=20100000/1829 [rec/s] out:10989=20099700/1829 [rec/s]\n","2021-11-09 23:37:25,538 INFO mapred.LocalJobRunner: Records R/W=20056685/20055907 > reduce\n","2021-11-09 23:37:31,034 INFO streaming.PipeMapRed: Records R/W=20153873/20153568\n","2021-11-09 23:37:31,539 INFO mapred.LocalJobRunner: Records R/W=20153873/20153568 > reduce\n","2021-11-09 23:37:34,532 INFO streaming.PipeMapRed: R/W/S=20200000/20199528/0 in:10990=20200000/1838 [rec/s] out:10989=20199528/1838 [rec/s]\n","2021-11-09 23:37:37,539 INFO mapred.LocalJobRunner: Records R/W=20153873/20153568 > reduce\n","2021-11-09 23:37:39,871 INFO streaming.PipeMapRed: R/W/S=20300000/20299459/0 in:11008=20300000/1844 [rec/s] out:11008=20299459/1844 [rec/s]\n","2021-11-09 23:37:41,037 INFO streaming.PipeMapRed: Records R/W=20321902/20321465\n","2021-11-09 23:37:43,540 INFO mapred.LocalJobRunner: Records R/W=20321902/20321465 > reduce\n","2021-11-09 23:37:47,521 INFO streaming.PipeMapRed: R/W/S=20400000/20399512/0 in:11021=20400000/1851 [rec/s] out:11020=20399512/1851 [rec/s]\n","2021-11-09 23:37:49,540 INFO mapred.LocalJobRunner: Records R/W=20321902/20321465 > reduce\n","2021-11-09 23:37:51,038 INFO streaming.PipeMapRed: Records R/W=20432421/20431780\n","2021-11-09 23:37:55,541 INFO mapred.LocalJobRunner: Records R/W=20432421/20431780 > reduce\n","2021-11-09 23:37:56,934 INFO streaming.PipeMapRed: R/W/S=20500000/20499313/0 in:11015=20500000/1861 [rec/s] out:11015=20499313/1861 [rec/s]\n","2021-11-09 23:38:01,052 INFO streaming.PipeMapRed: Records R/W=20539883/20539659\n","2021-11-09 23:38:01,541 INFO mapred.LocalJobRunner: Records R/W=20539883/20539659 > reduce\n","2021-11-09 23:38:07,542 INFO mapred.LocalJobRunner: Records R/W=20539883/20539659 > reduce\n","2021-11-09 23:38:07,571 INFO streaming.PipeMapRed: R/W/S=20600000/20599529/0 in:11010=20600000/1871 [rec/s] out:11009=20599529/1871 [rec/s]\n","2021-11-09 23:38:11,056 INFO streaming.PipeMapRed: Records R/W=20643170/20642659\n","2021-11-09 23:38:13,542 INFO mapred.LocalJobRunner: Records R/W=20643170/20642659 > reduce\n","2021-11-09 23:38:15,683 INFO streaming.PipeMapRed: R/W/S=20700000/20699320/0 in:11010=20700000/1880 [rec/s] out:11010=20699320/1880 [rec/s]\n","2021-11-09 23:38:19,542 INFO mapred.LocalJobRunner: Records R/W=20643170/20642659 > reduce\n","2021-11-09 23:38:21,058 INFO streaming.PipeMapRed: Records R/W=20782949/20782445\n","2021-11-09 23:38:22,310 INFO streaming.PipeMapRed: R/W/S=20800000/20799367/0 in:11028=20800000/1886 [rec/s] out:11028=20799367/1886 [rec/s]\n","2021-11-09 23:38:25,543 INFO mapred.LocalJobRunner: Records R/W=20782949/20782445 > reduce\n","2021-11-09 23:38:31,063 INFO streaming.PipeMapRed: Records R/W=20893087/20892847\n","2021-11-09 23:38:31,543 INFO mapred.LocalJobRunner: Records R/W=20893087/20892847 > reduce\n","2021-11-09 23:38:31,993 INFO streaming.PipeMapRed: R/W/S=20900000/20899758/0 in:11023=20900000/1896 [rec/s] out:11023=20899758/1896 [rec/s]\n","2021-11-09 23:38:37,544 INFO mapred.LocalJobRunner: Records R/W=20893087/20892847 > reduce\n","2021-11-09 23:38:41,064 INFO streaming.PipeMapRed: Records R/W=20996077/20995753\n","2021-11-09 23:38:41,459 INFO streaming.PipeMapRed: R/W/S=21000000/20999467/0 in:11023=21000000/1905 [rec/s] out:11023=20999467/1905 [rec/s]\n","2021-11-09 23:38:43,544 INFO mapred.LocalJobRunner: Records R/W=20996077/20995753 > reduce\n","2021-11-09 23:38:49,545 INFO mapred.LocalJobRunner: Records R/W=20996077/20995753 > reduce\n","2021-11-09 23:38:49,617 INFO streaming.PipeMapRed: R/W/S=21100000/21099842/0 in:11024=21100000/1914 [rec/s] out:11023=21099842/1914 [rec/s]\n","2021-11-09 23:38:49,759 INFO mapreduce.Job:  map 100% reduce 80%\n","2021-11-09 23:38:51,068 INFO streaming.PipeMapRed: Records R/W=21112096/21111671\n","2021-11-09 23:38:55,545 INFO mapred.LocalJobRunner: Records R/W=21112096/21111671 > reduce\n","2021-11-09 23:38:58,507 INFO streaming.PipeMapRed: R/W/S=21200000/21199419/0 in:11030=21200000/1922 [rec/s] out:11029=21199419/1922 [rec/s]\n","2021-11-09 23:39:01,071 INFO streaming.PipeMapRed: Records R/W=21232803/21232373\n","2021-11-09 23:39:01,546 INFO mapred.LocalJobRunner: Records R/W=21232803/21232373 > reduce\n","2021-11-09 23:39:06,065 INFO streaming.PipeMapRed: R/W/S=21300000/21299674/0 in:11036=21300000/1930 [rec/s] out:11036=21299674/1930 [rec/s]\n","2021-11-09 23:39:07,546 INFO mapred.LocalJobRunner: Records R/W=21232803/21232373 > reduce\n","2021-11-09 23:39:11,074 INFO streaming.PipeMapRed: Records R/W=21365208/21364534\n","2021-11-09 23:39:13,256 INFO streaming.PipeMapRed: R/W/S=21400000/21398899/0 in:11048=21400000/1937 [rec/s] out:11047=21398899/1937 [rec/s]\n","2021-11-09 23:39:13,546 INFO mapred.LocalJobRunner: Records R/W=21365208/21364534 > reduce\n","2021-11-09 23:39:19,547 INFO mapred.LocalJobRunner: Records R/W=21365208/21364534 > reduce\n","2021-11-09 23:39:20,499 INFO streaming.PipeMapRed: R/W/S=21500000/21499189/0 in:11059=21500000/1944 [rec/s] out:11059=21499189/1944 [rec/s]\n","2021-11-09 23:39:21,075 INFO streaming.PipeMapRed: Records R/W=21507265/21506647\n","2021-11-09 23:39:25,547 INFO mapred.LocalJobRunner: Records R/W=21507265/21506647 > reduce\n","2021-11-09 23:39:28,551 INFO streaming.PipeMapRed: R/W/S=21600000/21599583/0 in:11065=21600000/1952 [rec/s] out:11065=21599583/1952 [rec/s]\n","2021-11-09 23:39:31,077 INFO streaming.PipeMapRed: Records R/W=21626857/21626285\n","2021-11-09 23:39:31,548 INFO mapred.LocalJobRunner: Records R/W=21626857/21626285 > reduce\n","2021-11-09 23:39:37,278 INFO streaming.PipeMapRed: R/W/S=21700000/21699417/0 in:11065=21700000/1961 [rec/s] out:11065=21699417/1961 [rec/s]\n","2021-11-09 23:39:37,548 INFO mapred.LocalJobRunner: Records R/W=21626857/21626285 > reduce\n","2021-11-09 23:39:41,084 INFO streaming.PipeMapRed: Records R/W=21751260/21750740\n","2021-11-09 23:39:43,549 INFO mapred.LocalJobRunner: Records R/W=21751260/21750740 > reduce\n","2021-11-09 23:39:46,357 INFO streaming.PipeMapRed: R/W/S=21800000/21799526/0 in:11065=21800000/1970 [rec/s] out:11065=21799526/1970 [rec/s]\n","2021-11-09 23:39:49,549 INFO mapred.LocalJobRunner: Records R/W=21751260/21750740 > reduce\n","2021-11-09 23:39:51,086 INFO streaming.PipeMapRed: Records R/W=21847022/21846714\n","2021-11-09 23:39:55,549 INFO mapred.LocalJobRunner: Records R/W=21847022/21846714 > reduce\n","2021-11-09 23:39:56,850 INFO streaming.PipeMapRed: R/W/S=21900000/21899716/0 in:11055=21900000/1981 [rec/s] out:11054=21899716/1981 [rec/s]\n","2021-11-09 23:40:01,088 INFO streaming.PipeMapRed: Records R/W=21938438/21937910\n","2021-11-09 23:40:01,550 INFO mapred.LocalJobRunner: Records R/W=21938438/21937910 > reduce\n","2021-11-09 23:40:07,303 INFO streaming.PipeMapRed: R/W/S=22000000/21999239/0 in:11049=22000000/1991 [rec/s] out:11049=21999239/1991 [rec/s]\n","2021-11-09 23:40:07,550 INFO mapred.LocalJobRunner: Records R/W=21938438/21937910 > reduce\n","2021-11-09 23:40:11,096 INFO streaming.PipeMapRed: Records R/W=22049705/22049040\n","2021-11-09 23:40:13,551 INFO mapred.LocalJobRunner: Records R/W=22049705/22049040 > reduce\n","2021-11-09 23:40:16,311 INFO streaming.PipeMapRed: R/W/S=22100000/22099679/0 in:11050=22100000/2000 [rec/s] out:11049=22099679/2000 [rec/s]\n","2021-11-09 23:40:19,551 INFO mapred.LocalJobRunner: Records R/W=22049705/22049040 > reduce\n","2021-11-09 23:40:21,099 INFO streaming.PipeMapRed: Records R/W=22149868/22149385\n","2021-11-09 23:40:25,552 INFO mapred.LocalJobRunner: Records R/W=22149868/22149385 > reduce\n","2021-11-09 23:40:25,956 INFO streaming.PipeMapRed: R/W/S=22200000/22199680/0 in:11044=22200000/2010 [rec/s] out:11044=22199680/2010 [rec/s]\n","2021-11-09 23:40:31,107 INFO streaming.PipeMapRed: Records R/W=22275245/22274983\n","2021-11-09 23:40:31,552 INFO mapred.LocalJobRunner: Records R/W=22275245/22274983 > reduce\n","2021-11-09 23:40:33,576 INFO streaming.PipeMapRed: R/W/S=22300000/22299534/0 in:11050=22300000/2018 [rec/s] out:11050=22299534/2018 [rec/s]\n","2021-11-09 23:40:37,552 INFO mapred.LocalJobRunner: Records R/W=22275245/22274983 > reduce\n","2021-11-09 23:40:41,115 INFO streaming.PipeMapRed: Records R/W=22364798/22364438\n","2021-11-09 23:40:43,553 INFO mapred.LocalJobRunner: Records R/W=22364798/22364438 > reduce\n","2021-11-09 23:40:45,822 INFO streaming.PipeMapRed: R/W/S=22400000/22399702/0 in:11034=22400000/2030 [rec/s] out:11034=22399702/2030 [rec/s]\n","2021-11-09 23:40:49,553 INFO mapred.LocalJobRunner: Records R/W=22364798/22364438 > reduce\n","2021-11-09 23:40:51,122 INFO streaming.PipeMapRed: Records R/W=22463479/22463086\n","2021-11-09 23:40:54,131 INFO streaming.PipeMapRed: R/W/S=22500000/22499485/0 in:11040=22500000/2038 [rec/s] out:11039=22499485/2038 [rec/s]\n","2021-11-09 23:40:55,554 INFO mapred.LocalJobRunner: Records R/W=22463479/22463086 > reduce\n","2021-11-09 23:41:01,130 INFO streaming.PipeMapRed: Records R/W=22590096/22589692\n","2021-11-09 23:41:01,554 INFO mapred.LocalJobRunner: Records R/W=22590096/22589692 > reduce\n","2021-11-09 23:41:02,092 INFO streaming.PipeMapRed: R/W/S=22600000/22599593/0 in:11045=22600000/2046 [rec/s] out:11045=22599593/2046 [rec/s]\n","2021-11-09 23:41:07,554 INFO mapred.LocalJobRunner: Records R/W=22590096/22589692 > reduce\n","2021-11-09 23:41:10,557 INFO streaming.PipeMapRed: R/W/S=22700000/22699659/0 in:11051=22700000/2054 [rec/s] out:11051=22699659/2054 [rec/s]\n","2021-11-09 23:41:11,131 INFO streaming.PipeMapRed: Records R/W=22706748/22706155\n","2021-11-09 23:41:13,555 INFO mapred.LocalJobRunner: Records R/W=22706748/22706155 > reduce\n","2021-11-09 23:41:13,824 INFO mapreduce.Job:  map 100% reduce 81%\n","2021-11-09 23:41:19,555 INFO mapred.LocalJobRunner: Records R/W=22706748/22706155 > reduce\n","2021-11-09 23:41:21,142 INFO streaming.PipeMapRed: Records R/W=22783149/22782994\n","2021-11-09 23:41:24,470 INFO streaming.PipeMapRed: R/W/S=22800000/22799033/0 in:11025=22800000/2068 [rec/s] out:11024=22799033/2068 [rec/s]\n","2021-11-09 23:41:25,556 INFO mapred.LocalJobRunner: Records R/W=22783149/22782994 > reduce\n","2021-11-09 23:41:31,143 INFO streaming.PipeMapRed: Records R/W=22854515/22853950\n","2021-11-09 23:41:31,556 INFO mapred.LocalJobRunner: Records R/W=22854515/22853950 > reduce\n","2021-11-09 23:41:35,309 INFO streaming.PipeMapRed: R/W/S=22900000/22899520/0 in:11014=22900000/2079 [rec/s] out:11014=22899520/2079 [rec/s]\n","2021-11-09 23:41:37,557 INFO mapred.LocalJobRunner: Records R/W=22854515/22853950 > reduce\n","2021-11-09 23:41:41,147 INFO streaming.PipeMapRed: Records R/W=22967336/22966811\n","2021-11-09 23:41:43,557 INFO mapred.LocalJobRunner: Records R/W=22967336/22966811 > reduce\n","2021-11-09 23:41:44,251 INFO streaming.PipeMapRed: R/W/S=23000000/22999370/0 in:11015=23000000/2088 [rec/s] out:11015=22999370/2088 [rec/s]\n","2021-11-09 23:41:49,557 INFO mapred.LocalJobRunner: Records R/W=22967336/22966811 > reduce\n","2021-11-09 23:41:51,155 INFO streaming.PipeMapRed: Records R/W=23064850/23064406\n","2021-11-09 23:41:55,488 INFO streaming.PipeMapRed: R/W/S=23100000/23098724/0 in:11005=23100000/2099 [rec/s] out:11004=23098724/2099 [rec/s]\n","2021-11-09 23:41:55,558 INFO mapred.LocalJobRunner: Records R/W=23064850/23064406 > reduce\n","2021-11-09 23:42:01,160 INFO streaming.PipeMapRed: Records R/W=23142351/23142013\n","2021-11-09 23:42:01,558 INFO mapred.LocalJobRunner: Records R/W=23142351/23142013 > reduce\n","2021-11-09 23:42:07,157 INFO streaming.PipeMapRed: R/W/S=23200000/23199713/0 in:10990=23200000/2111 [rec/s] out:10989=23199713/2111 [rec/s]\n","2021-11-09 23:42:07,559 INFO mapred.LocalJobRunner: Records R/W=23142351/23142013 > reduce\n","2021-11-09 23:42:11,162 INFO streaming.PipeMapRed: Records R/W=23233382/23233009\n","2021-11-09 23:42:13,559 INFO mapred.LocalJobRunner: Records R/W=23233382/23233009 > reduce\n","2021-11-09 23:42:17,100 INFO streaming.PipeMapRed: R/W/S=23300000/23299757/0 in:10985=23300000/2121 [rec/s] out:10985=23299757/2121 [rec/s]\n","2021-11-09 23:42:19,560 INFO mapred.LocalJobRunner: Records R/W=23233382/23233009 > reduce\n","2021-11-09 23:42:21,168 INFO streaming.PipeMapRed: Records R/W=23344200/23343884\n","2021-11-09 23:42:25,560 INFO mapred.LocalJobRunner: Records R/W=23344200/23343884 > reduce\n","2021-11-09 23:42:26,599 INFO streaming.PipeMapRed: R/W/S=23400000/23399583/0 in:10980=23400000/2131 [rec/s] out:10980=23399583/2131 [rec/s]\n","2021-11-09 23:42:31,169 INFO streaming.PipeMapRed: Records R/W=23467738/23467256\n","2021-11-09 23:42:31,560 INFO mapred.LocalJobRunner: Records R/W=23467738/23467256 > reduce\n","2021-11-09 23:42:33,332 INFO streaming.PipeMapRed: R/W/S=23500000/23499443/0 in:10996=23500000/2137 [rec/s] out:10996=23499443/2137 [rec/s]\n","2021-11-09 23:42:37,561 INFO mapred.LocalJobRunner: Records R/W=23467738/23467256 > reduce\n","2021-11-09 23:42:39,267 INFO streaming.PipeMapRed: R/W/S=23600000/23599270/0 in:11012=23600000/2143 [rec/s] out:11012=23599270/2143 [rec/s]\n","2021-11-09 23:42:41,170 INFO streaming.PipeMapRed: Records R/W=23634072/23633360\n","2021-11-09 23:42:43,561 INFO mapred.LocalJobRunner: Records R/W=23634072/23633360 > reduce\n","2021-11-09 23:42:45,573 INFO streaming.PipeMapRed: R/W/S=23700000/23699186/0 in:11023=23700000/2150 [rec/s] out:11022=23699186/2150 [rec/s]\n","2021-11-09 23:42:49,562 INFO mapred.LocalJobRunner: Records R/W=23634072/23633360 > reduce\n","2021-11-09 23:42:51,171 INFO streaming.PipeMapRed: Records R/W=23781610/23781232\n","2021-11-09 23:42:52,875 INFO streaming.PipeMapRed: R/W/S=23800000/23799687/0 in:11033=23800000/2157 [rec/s] out:11033=23799687/2157 [rec/s]\n","2021-11-09 23:42:55,562 INFO mapred.LocalJobRunner: Records R/W=23781610/23781232 > reduce\n","2021-11-09 23:43:01,172 INFO streaming.PipeMapRed: Records R/W=23868933/23868454\n","2021-11-09 23:43:01,563 INFO mapred.LocalJobRunner: Records R/W=23868933/23868454 > reduce\n","2021-11-09 23:43:04,517 INFO streaming.PipeMapRed: R/W/S=23900000/23899647/0 in:11023=23900000/2168 [rec/s] out:11023=23899647/2168 [rec/s]\n","2021-11-09 23:43:07,563 INFO mapred.LocalJobRunner: Records R/W=23868933/23868454 > reduce\n","2021-11-09 23:43:11,173 INFO streaming.PipeMapRed: Records R/W=23976728/23976158\n","2021-11-09 23:43:13,563 INFO mapred.LocalJobRunner: Records R/W=23976728/23976158 > reduce\n","2021-11-09 23:43:13,858 INFO streaming.PipeMapRed: R/W/S=24000000/23999592/0 in:11019=24000000/2178 [rec/s] out:11019=23999592/2178 [rec/s]\n","2021-11-09 23:43:19,564 INFO mapred.LocalJobRunner: Records R/W=23976728/23976158 > reduce\n","2021-11-09 23:43:21,175 INFO streaming.PipeMapRed: Records R/W=24079498/24078891\n","2021-11-09 23:43:23,293 INFO streaming.PipeMapRed: R/W/S=24100000/24099348/0 in:11019=24100000/2187 [rec/s] out:11019=24099348/2187 [rec/s]\n","2021-11-09 23:43:25,564 INFO mapred.LocalJobRunner: Records R/W=24079498/24078891 > reduce\n","2021-11-09 23:43:31,179 INFO streaming.PipeMapRed: Records R/W=24197636/24197219\n","2021-11-09 23:43:31,383 INFO streaming.PipeMapRed: R/W/S=24200000/24199525/0 in:11025=24200000/2195 [rec/s] out:11024=24199525/2195 [rec/s]\n","2021-11-09 23:43:31,565 INFO mapred.LocalJobRunner: Records R/W=24197636/24197219 > reduce\n","2021-11-09 23:43:37,565 INFO mapred.LocalJobRunner: Records R/W=24197636/24197219 > reduce\n","2021-11-09 23:43:41,267 INFO streaming.PipeMapRed: Records R/W=24237885/24237629\n","2021-11-09 23:43:43,566 INFO mapred.LocalJobRunner: Records R/W=24237885/24237629 > reduce\n","2021-11-09 23:43:49,566 INFO mapred.LocalJobRunner: Records R/W=24237885/24237629 > reduce\n","2021-11-09 23:43:49,903 INFO mapreduce.Job:  map 100% reduce 82%\n","2021-11-09 23:43:51,272 INFO streaming.PipeMapRed: Records R/W=24262331/24262167\n","2021-11-09 23:43:55,567 INFO mapred.LocalJobRunner: Records R/W=24262331/24262167 > reduce\n","2021-11-09 23:43:57,785 INFO streaming.PipeMapRed: R/W/S=24300000/24299251/0 in:10936=24300000/2222 [rec/s] out:10935=24299251/2222 [rec/s]\n","2021-11-09 23:44:01,273 INFO streaming.PipeMapRed: Records R/W=24333237/24332939\n","2021-11-09 23:44:01,567 INFO mapred.LocalJobRunner: Records R/W=24333237/24332939 > reduce\n","2021-11-09 23:44:05,845 INFO streaming.PipeMapRed: R/W/S=24400000/24399230/0 in:10941=24400000/2230 [rec/s] out:10941=24399230/2230 [rec/s]\n","2021-11-09 23:44:07,567 INFO mapred.LocalJobRunner: Records R/W=24333237/24332939 > reduce\n","2021-11-09 23:44:11,274 INFO streaming.PipeMapRed: Records R/W=24458311/24457965\n","2021-11-09 23:44:13,568 INFO mapred.LocalJobRunner: Records R/W=24458311/24457965 > reduce\n","2021-11-09 23:44:15,788 INFO streaming.PipeMapRed: R/W/S=24500000/24499796/0 in:10937=24500000/2240 [rec/s] out:10937=24499796/2240 [rec/s]\n","2021-11-09 23:44:19,568 INFO mapred.LocalJobRunner: Records R/W=24458311/24457965 > reduce\n","2021-11-09 23:44:21,279 INFO streaming.PipeMapRed: Records R/W=24552418/24551915\n","2021-11-09 23:44:25,569 INFO mapred.LocalJobRunner: Records R/W=24552418/24551915 > reduce\n","2021-11-09 23:44:25,995 INFO streaming.PipeMapRed: R/W/S=24600000/24599511/0 in:10933=24600000/2250 [rec/s] out:10933=24599511/2250 [rec/s]\n","2021-11-09 23:44:31,287 INFO streaming.PipeMapRed: Records R/W=24662162/24661890\n","2021-11-09 23:44:31,569 INFO mapred.LocalJobRunner: Records R/W=24662162/24661890 > reduce\n","2021-11-09 23:44:33,928 INFO streaming.PipeMapRed: R/W/S=24700000/24699461/0 in:10938=24700000/2258 [rec/s] out:10938=24699461/2258 [rec/s]\n","2021-11-09 23:44:37,571 INFO mapred.LocalJobRunner: Records R/W=24662162/24661890 > reduce\n","2021-11-09 23:44:41,293 INFO streaming.PipeMapRed: Records R/W=24789892/24789331\n","2021-11-09 23:44:42,267 INFO streaming.PipeMapRed: R/W/S=24800000/24799686/0 in:10944=24800000/2266 [rec/s] out:10944=24799686/2266 [rec/s]\n","2021-11-09 23:44:43,571 INFO mapred.LocalJobRunner: Records R/W=24789892/24789331 > reduce\n","2021-11-09 23:44:49,572 INFO mapred.LocalJobRunner: Records R/W=24789892/24789331 > reduce\n","2021-11-09 23:44:51,298 INFO streaming.PipeMapRed: Records R/W=24897825/24897103\n","2021-11-09 23:44:51,459 INFO streaming.PipeMapRed: R/W/S=24900000/24899695/0 in:10945=24900000/2275 [rec/s] out:10944=24899695/2275 [rec/s]\n","2021-11-09 23:44:55,572 INFO mapred.LocalJobRunner: Records R/W=24897825/24897103 > reduce\n","2021-11-09 23:44:59,590 INFO streaming.PipeMapRed: R/W/S=25000000/24999431/0 in:10945=25000000/2284 [rec/s] out:10945=24999431/2284 [rec/s]\n","2021-11-09 23:45:01,300 INFO streaming.PipeMapRed: Records R/W=25016467/25015922\n","2021-11-09 23:45:01,573 INFO mapred.LocalJobRunner: Records R/W=25016467/25015922 > reduce\n","2021-11-09 23:45:07,573 INFO mapred.LocalJobRunner: Records R/W=25016467/25015922 > reduce\n","2021-11-09 23:45:09,838 INFO streaming.PipeMapRed: R/W/S=25100000/25099614/0 in:10941=25100000/2294 [rec/s] out:10941=25099614/2294 [rec/s]\n","2021-11-09 23:45:11,305 INFO streaming.PipeMapRed: Records R/W=25112145/25111600\n","2021-11-09 23:45:13,574 INFO mapred.LocalJobRunner: Records R/W=25112145/25111600 > reduce\n","2021-11-09 23:45:19,574 INFO mapred.LocalJobRunner: Records R/W=25112145/25111600 > reduce\n","2021-11-09 23:45:20,279 INFO streaming.PipeMapRed: R/W/S=25200000/25199541/0 in:10937=25200000/2304 [rec/s] out:10937=25199541/2304 [rec/s]\n","2021-11-09 23:45:21,309 INFO streaming.PipeMapRed: Records R/W=25210907/25210451\n","2021-11-09 23:45:25,575 INFO mapred.LocalJobRunner: Records R/W=25210907/25210451 > reduce\n","2021-11-09 23:45:30,663 INFO streaming.PipeMapRed: R/W/S=25300000/25299648/0 in:10928=25300000/2315 [rec/s] out:10928=25299648/2315 [rec/s]\n","2021-11-09 23:45:31,318 INFO streaming.PipeMapRed: Records R/W=25305076/25304496\n","2021-11-09 23:45:31,575 INFO mapred.LocalJobRunner: Records R/W=25305076/25304496 > reduce\n","2021-11-09 23:45:37,576 INFO mapred.LocalJobRunner: Records R/W=25305076/25304496 > reduce\n","2021-11-09 23:45:41,320 INFO streaming.PipeMapRed: Records R/W=25380351/25379843\n","2021-11-09 23:45:43,576 INFO mapred.LocalJobRunner: Records R/W=25380351/25379843 > reduce\n","2021-11-09 23:45:44,934 INFO streaming.PipeMapRed: R/W/S=25400000/25399665/0 in:10905=25400000/2329 [rec/s] out:10905=25399665/2329 [rec/s]\n","2021-11-09 23:45:49,576 INFO mapred.LocalJobRunner: Records R/W=25380351/25379843 > reduce\n","2021-11-09 23:45:51,321 INFO streaming.PipeMapRed: Records R/W=25440505/25440092\n","2021-11-09 23:45:55,577 INFO mapred.LocalJobRunner: Records R/W=25440505/25440092 > reduce\n","2021-11-09 23:45:58,555 INFO streaming.PipeMapRed: R/W/S=25500000/25499282/0 in:10888=25500000/2342 [rec/s] out:10887=25499282/2342 [rec/s]\n","2021-11-09 23:46:01,323 INFO streaming.PipeMapRed: Records R/W=25526436/25526130\n","2021-11-09 23:46:01,577 INFO mapred.LocalJobRunner: Records R/W=25526436/25526130 > reduce\n","2021-11-09 23:46:07,578 INFO mapred.LocalJobRunner: Records R/W=25526436/25526130 > reduce\n","2021-11-09 23:46:09,465 INFO streaming.PipeMapRed: R/W/S=25600000/25599666/0 in:10879=25600000/2353 [rec/s] out:10879=25599666/2353 [rec/s]\n","2021-11-09 23:46:11,327 INFO streaming.PipeMapRed: Records R/W=25615439/25615051\n","2021-11-09 23:46:13,578 INFO mapred.LocalJobRunner: Records R/W=25615439/25615051 > reduce\n","2021-11-09 23:46:19,579 INFO mapred.LocalJobRunner: Records R/W=25615439/25615051 > reduce\n","2021-11-09 23:46:20,491 INFO streaming.PipeMapRed: R/W/S=25700000/25699552/0 in:10871=25700000/2364 [rec/s] out:10871=25699552/2364 [rec/s]\n","2021-11-09 23:46:21,335 INFO streaming.PipeMapRed: Records R/W=25707048/25706707\n","2021-11-09 23:46:25,580 INFO mapred.LocalJobRunner: Records R/W=25707048/25706707 > reduce\n","2021-11-09 23:46:25,943 INFO mapreduce.Job:  map 100% reduce 83%\n","2021-11-09 23:46:31,344 INFO streaming.PipeMapRed: Records R/W=25792577/25792243\n","2021-11-09 23:46:31,582 INFO mapred.LocalJobRunner: Records R/W=25792577/25792243 > reduce\n","2021-11-09 23:46:32,247 INFO streaming.PipeMapRed: R/W/S=25800000/25799678/0 in:10858=25800000/2376 [rec/s] out:10858=25799678/2376 [rec/s]\n","2021-11-09 23:46:37,585 INFO mapred.LocalJobRunner: Records R/W=25792577/25792243 > reduce\n","2021-11-09 23:46:41,348 INFO streaming.PipeMapRed: Records R/W=25880635/25880427\n","2021-11-09 23:46:43,197 INFO streaming.PipeMapRed: R/W/S=25900000/25899745/0 in:10850=25900000/2387 [rec/s] out:10850=25899745/2387 [rec/s]\n","2021-11-09 23:46:43,585 INFO mapred.LocalJobRunner: Records R/W=25880635/25880427 > reduce\n","2021-11-09 23:46:49,163 INFO streaming.PipeMapRed: R/W/S=26000000/25999484/0 in:10865=26000000/2393 [rec/s] out:10864=25999484/2393 [rec/s]\n","2021-11-09 23:46:49,586 INFO mapred.LocalJobRunner: Records R/W=25880635/25880427 > reduce\n","2021-11-09 23:46:51,349 INFO streaming.PipeMapRed: Records R/W=26034231/26033840\n","2021-11-09 23:46:55,586 INFO mapred.LocalJobRunner: Records R/W=26034231/26033840 > reduce\n","2021-11-09 23:46:56,386 INFO streaming.PipeMapRed: R/W/S=26100000/26099525/0 in:10875=26100000/2400 [rec/s] out:10874=26099525/2400 [rec/s]\n","2021-11-09 23:47:01,354 INFO streaming.PipeMapRed: Records R/W=26147309/26146742\n","2021-11-09 23:47:01,587 INFO mapred.LocalJobRunner: Records R/W=26147309/26146742 > reduce\n","2021-11-09 23:47:05,640 INFO streaming.PipeMapRed: R/W/S=26200000/26199595/0 in:10871=26200000/2410 [rec/s] out:10871=26199595/2410 [rec/s]\n","2021-11-09 23:47:07,587 INFO mapred.LocalJobRunner: Records R/W=26147309/26146742 > reduce\n","2021-11-09 23:47:11,356 INFO streaming.PipeMapRed: Records R/W=26265690/26265150\n","2021-11-09 23:47:13,588 INFO mapred.LocalJobRunner: Records R/W=26265690/26265150 > reduce\n","2021-11-09 23:47:14,072 INFO streaming.PipeMapRed: R/W/S=26300000/26299275/0 in:10876=26300000/2418 [rec/s] out:10876=26299275/2418 [rec/s]\n","2021-11-09 23:47:19,588 INFO mapred.LocalJobRunner: Records R/W=26265690/26265150 > reduce\n","2021-11-09 23:47:21,358 INFO streaming.PipeMapRed: Records R/W=26386493/26386234\n","2021-11-09 23:47:22,694 INFO streaming.PipeMapRed: R/W/S=26400000/26399688/0 in:10877=26400000/2427 [rec/s] out:10877=26399688/2427 [rec/s]\n","2021-11-09 23:47:25,589 INFO mapred.LocalJobRunner: Records R/W=26386493/26386234 > reduce\n","2021-11-09 23:47:31,359 INFO streaming.PipeMapRed: Records R/W=26476269/26475741\n","2021-11-09 23:47:31,589 INFO mapred.LocalJobRunner: Records R/W=26476269/26475741 > reduce\n","2021-11-09 23:47:33,865 INFO streaming.PipeMapRed: R/W/S=26500000/26499495/0 in:10869=26500000/2438 [rec/s] out:10869=26499495/2438 [rec/s]\n","2021-11-09 23:47:37,590 INFO mapred.LocalJobRunner: Records R/W=26476269/26475741 > reduce\n","2021-11-09 23:47:41,360 INFO streaming.PipeMapRed: Records R/W=26583072/26582510\n","2021-11-09 23:47:43,013 INFO streaming.PipeMapRed: R/W/S=26600000/26599468/0 in:10870=26600000/2447 [rec/s] out:10870=26599468/2447 [rec/s]\n","2021-11-09 23:47:43,590 INFO mapred.LocalJobRunner: Records R/W=26583072/26582510 > reduce\n","2021-11-09 23:47:49,591 INFO mapred.LocalJobRunner: Records R/W=26583072/26582510 > reduce\n","2021-11-09 23:47:51,361 INFO streaming.PipeMapRed: Records R/W=26674418/26674011\n","2021-11-09 23:47:53,735 INFO streaming.PipeMapRed: R/W/S=26700000/26699543/0 in:10862=26700000/2458 [rec/s] out:10862=26699543/2458 [rec/s]\n","2021-11-09 23:47:55,591 INFO mapred.LocalJobRunner: Records R/W=26674418/26674011 > reduce\n","2021-11-09 23:48:01,366 INFO streaming.PipeMapRed: Records R/W=26760613/26760418\n","2021-11-09 23:48:01,595 INFO mapred.LocalJobRunner: Records R/W=26760613/26760418 > reduce\n","2021-11-09 23:48:04,998 INFO streaming.PipeMapRed: R/W/S=26800000/26799329/0 in:10854=26800000/2469 [rec/s] out:10854=26799329/2469 [rec/s]\n","2021-11-09 23:48:07,596 INFO mapred.LocalJobRunner: Records R/W=26760613/26760418 > reduce\n","2021-11-09 23:48:11,369 INFO streaming.PipeMapRed: Records R/W=26870619/26870178\n","2021-11-09 23:48:13,596 INFO mapred.LocalJobRunner: Records R/W=26870619/26870178 > reduce\n","2021-11-09 23:48:14,742 INFO streaming.PipeMapRed: R/W/S=26900000/26899404/0 in:10851=26900000/2479 [rec/s] out:10850=26899404/2479 [rec/s]\n","2021-11-09 23:48:19,597 INFO mapred.LocalJobRunner: Records R/W=26870619/26870178 > reduce\n","2021-11-09 23:48:21,374 INFO streaming.PipeMapRed: Records R/W=26964401/26963773\n","2021-11-09 23:48:25,019 INFO streaming.PipeMapRed: R/W/S=27000000/26999531/0 in:10847=27000000/2489 [rec/s] out:10847=26999531/2489 [rec/s]\n","2021-11-09 23:48:25,597 INFO mapred.LocalJobRunner: Records R/W=26964401/26963773 > reduce\n","2021-11-09 23:48:31,379 INFO streaming.PipeMapRed: Records R/W=27078306/27077755\n","2021-11-09 23:48:31,598 INFO mapred.LocalJobRunner: Records R/W=27078306/27077755 > reduce\n","2021-11-09 23:48:33,076 INFO streaming.PipeMapRed: R/W/S=27100000/27099058/0 in:10853=27100000/2497 [rec/s] out:10852=27099058/2497 [rec/s]\n","2021-11-09 23:48:37,598 INFO mapred.LocalJobRunner: Records R/W=27078306/27077755 > reduce\n","2021-11-09 23:48:41,386 INFO streaming.PipeMapRed: Records R/W=27181703/27181121\n","2021-11-09 23:48:43,055 INFO streaming.PipeMapRed: R/W/S=27200000/27199724/0 in:10849=27200000/2507 [rec/s] out:10849=27199724/2507 [rec/s]\n","2021-11-09 23:48:43,599 INFO mapred.LocalJobRunner: Records R/W=27181703/27181121 > reduce\n","2021-11-09 23:48:49,600 INFO mapred.LocalJobRunner: Records R/W=27181703/27181121 > reduce\n","2021-11-09 23:48:51,393 INFO streaming.PipeMapRed: Records R/W=27284937/27284476\n","2021-11-09 23:48:52,955 INFO streaming.PipeMapRed: R/W/S=27300000/27299579/0 in:10846=27300000/2517 [rec/s] out:10846=27299579/2517 [rec/s]\n","2021-11-09 23:48:55,600 INFO mapred.LocalJobRunner: Records R/W=27284937/27284476 > reduce\n","2021-11-09 23:48:56,023 INFO mapreduce.Job:  map 100% reduce 84%\n","2021-11-09 23:49:01,395 INFO streaming.PipeMapRed: Records R/W=27380170/27379900\n","2021-11-09 23:49:01,601 INFO mapred.LocalJobRunner: Records R/W=27380170/27379900 > reduce\n","2021-11-09 23:49:03,589 INFO streaming.PipeMapRed: R/W/S=27400000/27399655/0 in:10838=27400000/2528 [rec/s] out:10838=27399655/2528 [rec/s]\n","2021-11-09 23:49:07,601 INFO mapred.LocalJobRunner: Records R/W=27380170/27379900 > reduce\n","2021-11-09 23:49:11,419 INFO streaming.PipeMapRed: Records R/W=27466357/27466020\n","2021-11-09 23:49:13,602 INFO mapred.LocalJobRunner: Records R/W=27466357/27466020 > reduce\n","2021-11-09 23:49:14,541 INFO streaming.PipeMapRed: R/W/S=27500000/27499603/0 in:10835=27500000/2538 [rec/s] out:10835=27499603/2538 [rec/s]\n","2021-11-09 23:49:19,602 INFO mapred.LocalJobRunner: Records R/W=27466357/27466020 > reduce\n","2021-11-09 23:49:21,421 INFO streaming.PipeMapRed: Records R/W=27560255/27559706\n","2021-11-09 23:49:25,603 INFO mapred.LocalJobRunner: Records R/W=27560255/27559706 > reduce\n","2021-11-09 23:49:26,325 INFO streaming.PipeMapRed: R/W/S=27600000/27599487/0 in:10823=27600000/2550 [rec/s] out:10823=27599487/2550 [rec/s]\n","2021-11-09 23:49:31,425 INFO streaming.PipeMapRed: Records R/W=27653926/27653660\n","2021-11-09 23:49:31,603 INFO mapred.LocalJobRunner: Records R/W=27653926/27653660 > reduce\n","2021-11-09 23:49:35,827 INFO streaming.PipeMapRed: R/W/S=27700000/27699456/0 in:10820=27700000/2560 [rec/s] out:10820=27699456/2560 [rec/s]\n","2021-11-09 23:49:37,604 INFO mapred.LocalJobRunner: Records R/W=27653926/27653660 > reduce\n","2021-11-09 23:49:41,437 INFO streaming.PipeMapRed: Records R/W=27762158/27761646\n","2021-11-09 23:49:43,604 INFO mapred.LocalJobRunner: Records R/W=27762158/27761646 > reduce\n","2021-11-09 23:49:44,573 INFO streaming.PipeMapRed: R/W/S=27800000/27799668/0 in:10821=27800000/2569 [rec/s] out:10821=27799668/2569 [rec/s]\n","2021-11-09 23:49:49,605 INFO mapred.LocalJobRunner: Records R/W=27762158/27761646 > reduce\n","2021-11-09 23:49:51,440 INFO streaming.PipeMapRed: Records R/W=27839098/27838934\n","2021-11-09 23:49:55,605 INFO mapred.LocalJobRunner: Records R/W=27839098/27838934 > reduce\n","2021-11-09 23:50:00,201 INFO streaming.PipeMapRed: R/W/S=27900000/27899510/0 in:10797=27900000/2584 [rec/s] out:10797=27899513/2584 [rec/s]\n","2021-11-09 23:50:01,441 INFO streaming.PipeMapRed: Records R/W=27910351/27909989\n","2021-11-09 23:50:01,606 INFO mapred.LocalJobRunner: Records R/W=27910351/27909989 > reduce\n","2021-11-09 23:50:07,606 INFO mapred.LocalJobRunner: Records R/W=27910351/27909989 > reduce\n","2021-11-09 23:50:09,545 INFO streaming.PipeMapRed: R/W/S=28000000/27999623/0 in:10798=28000000/2593 [rec/s] out:10798=27999623/2593 [rec/s]\n","2021-11-09 23:50:11,442 INFO streaming.PipeMapRed: Records R/W=28018721/28018366\n","2021-11-09 23:50:13,606 INFO mapred.LocalJobRunner: Records R/W=28018721/28018366 > reduce\n","2021-11-09 23:50:19,516 INFO streaming.PipeMapRed: R/W/S=28100000/28099651/0 in:10795=28100000/2603 [rec/s] out:10795=28099651/2603 [rec/s]\n","2021-11-09 23:50:19,607 INFO mapred.LocalJobRunner: Records R/W=28018721/28018366 > reduce\n","2021-11-09 23:50:21,446 INFO streaming.PipeMapRed: Records R/W=28116876/28116520\n","2021-11-09 23:50:25,607 INFO mapred.LocalJobRunner: Records R/W=28116876/28116520 > reduce\n","2021-11-09 23:50:29,517 INFO streaming.PipeMapRed: R/W/S=28200000/28199642/0 in:10792=28200000/2613 [rec/s] out:10792=28199642/2613 [rec/s]\n","2021-11-09 23:50:31,466 INFO streaming.PipeMapRed: Records R/W=28224836/28224608\n","2021-11-09 23:50:31,608 INFO mapred.LocalJobRunner: Records R/W=28224836/28224608 > reduce\n","2021-11-09 23:50:37,608 INFO mapred.LocalJobRunner: Records R/W=28224836/28224608 > reduce\n","2021-11-09 23:50:39,763 INFO streaming.PipeMapRed: R/W/S=28300000/28299503/0 in:10785=28300000/2624 [rec/s] out:10784=28299503/2624 [rec/s]\n","2021-11-09 23:50:41,472 INFO streaming.PipeMapRed: Records R/W=28317895/28317336\n","2021-11-09 23:50:43,609 INFO mapred.LocalJobRunner: Records R/W=28317895/28317336 > reduce\n","2021-11-09 23:50:49,609 INFO mapred.LocalJobRunner: Records R/W=28317895/28317336 > reduce\n","2021-11-09 23:50:50,245 INFO streaming.PipeMapRed: R/W/S=28400000/28399676/0 in:10782=28400000/2634 [rec/s] out:10781=28399676/2634 [rec/s]\n","2021-11-09 23:50:51,474 INFO streaming.PipeMapRed: Records R/W=28409656/28409379\n","2021-11-09 23:50:55,610 INFO mapred.LocalJobRunner: Records R/W=28409656/28409379 > reduce\n","2021-11-09 23:51:01,483 INFO streaming.PipeMapRed: Records R/W=28494606/28494299\n","2021-11-09 23:51:01,610 INFO mapred.LocalJobRunner: Records R/W=28494606/28494299 > reduce\n","2021-11-09 23:51:02,062 INFO streaming.PipeMapRed: R/W/S=28500000/28499618/0 in:10770=28500000/2646 [rec/s] out:10770=28499618/2646 [rec/s]\n","2021-11-09 23:51:07,611 INFO mapred.LocalJobRunner: Records R/W=28494606/28494299 > reduce\n","2021-11-09 23:51:09,276 INFO streaming.PipeMapRed: R/W/S=28600000/28599591/0 in:10780=28600000/2653 [rec/s] out:10780=28599591/2653 [rec/s]\n","2021-11-09 23:51:11,486 INFO streaming.PipeMapRed: Records R/W=28628007/28627282\n","2021-11-09 23:51:13,611 INFO mapred.LocalJobRunner: Records R/W=28628007/28627282 > reduce\n","2021-11-09 23:51:16,506 INFO streaming.PipeMapRed: R/W/S=28700000/28699583/0 in:10789=28700000/2660 [rec/s] out:10789=28699583/2660 [rec/s]\n","2021-11-09 23:51:19,612 INFO mapred.LocalJobRunner: Records R/W=28628007/28627282 > reduce\n","2021-11-09 23:51:21,487 INFO streaming.PipeMapRed: Records R/W=28779978/28779256\n","2021-11-09 23:51:22,931 INFO streaming.PipeMapRed: R/W/S=28800000/28799435/0 in:10798=28800000/2667 [rec/s] out:10798=28799435/2667 [rec/s]\n","2021-11-09 23:51:25,612 INFO mapred.LocalJobRunner: Records R/W=28779978/28779256 > reduce\n","2021-11-09 23:51:26,074 INFO mapreduce.Job:  map 100% reduce 85%\n","2021-11-09 23:51:31,489 INFO streaming.PipeMapRed: Records R/W=28879776/28879102\n","2021-11-09 23:51:31,613 INFO mapred.LocalJobRunner: Records R/W=28879776/28879102 > reduce\n","2021-11-09 23:51:32,908 INFO streaming.PipeMapRed: R/W/S=28900000/28899545/0 in:10795=28900000/2677 [rec/s] out:10795=28899545/2677 [rec/s]\n","2021-11-09 23:51:37,613 INFO mapred.LocalJobRunner: Records R/W=28879776/28879102 > reduce\n","2021-11-09 23:51:39,008 INFO streaming.PipeMapRed: R/W/S=29000000/28999543/0 in:10808=29000000/2683 [rec/s] out:10808=28999543/2683 [rec/s]\n","2021-11-09 23:51:41,492 INFO streaming.PipeMapRed: Records R/W=29026392/29025977\n","2021-11-09 23:51:43,614 INFO mapred.LocalJobRunner: Records R/W=29026392/29025977 > reduce\n","2021-11-09 23:51:49,368 INFO streaming.PipeMapRed: R/W/S=29100000/29099645/0 in:10805=29100000/2693 [rec/s] out:10805=29099645/2693 [rec/s]\n","2021-11-09 23:51:49,615 INFO mapred.LocalJobRunner: Records R/W=29026392/29025977 > reduce\n","2021-11-09 23:51:51,495 INFO streaming.PipeMapRed: Records R/W=29121372/29121084\n","2021-11-09 23:51:55,615 INFO mapred.LocalJobRunner: Records R/W=29121372/29121084 > reduce\n","2021-11-09 23:51:59,192 INFO streaming.PipeMapRed: R/W/S=29200000/29199369/0 in:10802=29200000/2703 [rec/s] out:10802=29199369/2703 [rec/s]\n","2021-11-09 23:52:01,499 INFO streaming.PipeMapRed: Records R/W=29223493/29223227\n","2021-11-09 23:52:01,616 INFO mapred.LocalJobRunner: Records R/W=29223493/29223227 > reduce\n","2021-11-09 23:52:07,617 INFO mapred.LocalJobRunner: Records R/W=29223493/29223227 > reduce\n","2021-11-09 23:52:09,068 INFO streaming.PipeMapRed: R/W/S=29300000/29299747/0 in:10799=29300000/2713 [rec/s] out:10799=29299747/2713 [rec/s]\n","2021-11-09 23:52:11,503 INFO streaming.PipeMapRed: Records R/W=29315485/29315003\n","2021-11-09 23:52:13,617 INFO mapred.LocalJobRunner: Records R/W=29315485/29315003 > reduce\n","2021-11-09 23:52:19,281 INFO streaming.PipeMapRed: R/W/S=29400000/29399204/0 in:10796=29400000/2723 [rec/s] out:10796=29399204/2723 [rec/s]\n","2021-11-09 23:52:19,618 INFO mapred.LocalJobRunner: Records R/W=29315485/29315003 > reduce\n","2021-11-09 23:52:21,506 INFO streaming.PipeMapRed: Records R/W=29426262/29425974\n","2021-11-09 23:52:25,618 INFO mapred.LocalJobRunner: Records R/W=29426262/29425974 > reduce\n","2021-11-09 23:52:29,447 INFO streaming.PipeMapRed: R/W/S=29500000/29499298/0 in:10793=29500000/2733 [rec/s] out:10793=29499298/2733 [rec/s]\n","2021-11-09 23:52:31,515 INFO streaming.PipeMapRed: Records R/W=29537609/29537138\n","2021-11-09 23:52:31,619 INFO mapred.LocalJobRunner: Records R/W=29537609/29537138 > reduce\n","2021-11-09 23:52:37,619 INFO mapred.LocalJobRunner: Records R/W=29537609/29537138 > reduce\n","2021-11-09 23:52:38,266 INFO streaming.PipeMapRed: R/W/S=29600000/29599560/0 in:10795=29600000/2742 [rec/s] out:10794=29599560/2742 [rec/s]\n","2021-11-09 23:52:41,517 INFO streaming.PipeMapRed: Records R/W=29626726/29626458\n","2021-11-09 23:52:43,620 INFO mapred.LocalJobRunner: Records R/W=29626726/29626458 > reduce\n","2021-11-09 23:52:49,620 INFO mapred.LocalJobRunner: Records R/W=29626726/29626458 > reduce\n","2021-11-09 23:52:50,161 INFO streaming.PipeMapRed: R/W/S=29700000/29699614/0 in:10784=29700000/2754 [rec/s] out:10784=29699614/2754 [rec/s]\n","2021-11-09 23:52:51,520 INFO streaming.PipeMapRed: Records R/W=29715889/29715129\n","2021-11-09 23:52:55,621 INFO mapred.LocalJobRunner: Records R/W=29715889/29715129 > reduce\n","2021-11-09 23:52:59,006 INFO streaming.PipeMapRed: R/W/S=29800000/29799686/0 in:10785=29800000/2763 [rec/s] out:10785=29799686/2763 [rec/s]\n","2021-11-09 23:53:01,525 INFO streaming.PipeMapRed: Records R/W=29830515/29830149\n","2021-11-09 23:53:01,621 INFO mapred.LocalJobRunner: Records R/W=29830515/29830149 > reduce\n","2021-11-09 23:53:07,622 INFO mapred.LocalJobRunner: Records R/W=29830515/29830149 > reduce\n","2021-11-09 23:53:09,771 INFO streaming.PipeMapRed: R/W/S=29900000/29899691/0 in:10778=29900000/2774 [rec/s] out:10778=29899691/2774 [rec/s]\n","2021-11-09 23:53:11,528 INFO streaming.PipeMapRed: Records R/W=29920735/29920172\n","2021-11-09 23:53:13,622 INFO mapred.LocalJobRunner: Records R/W=29920735/29920172 > reduce\n","2021-11-09 23:53:18,206 INFO streaming.PipeMapRed: R/W/S=30000000/29999525/0 in:10783=30000000/2782 [rec/s] out:10783=29999525/2782 [rec/s]\n","2021-11-09 23:53:19,629 INFO mapred.LocalJobRunner: Records R/W=29920735/29920172 > reduce\n","2021-11-09 23:53:21,529 INFO streaming.PipeMapRed: Records R/W=30037027/30036539\n","2021-11-09 23:53:25,632 INFO mapred.LocalJobRunner: Records R/W=30037027/30036539 > reduce\n","2021-11-09 23:53:27,366 INFO streaming.PipeMapRed: R/W/S=30100000/30099384/0 in:10784=30100000/2791 [rec/s] out:10784=30099384/2791 [rec/s]\n","2021-11-09 23:53:31,535 INFO streaming.PipeMapRed: Records R/W=30133920/30133614\n","2021-11-09 23:53:31,633 INFO mapred.LocalJobRunner: Records R/W=30133920/30133614 > reduce\n","2021-11-09 23:53:37,633 INFO mapred.LocalJobRunner: Records R/W=30133920/30133614 > reduce\n","2021-11-09 23:53:39,137 INFO streaming.PipeMapRed: R/W/S=30200000/30199644/0 in:10774=30200000/2803 [rec/s] out:10774=30199644/2803 [rec/s]\n","2021-11-09 23:53:41,542 INFO streaming.PipeMapRed: Records R/W=30220920/30220605\n","2021-11-09 23:53:43,634 INFO mapred.LocalJobRunner: Records R/W=30220920/30220605 > reduce\n","2021-11-09 23:53:49,635 INFO mapred.LocalJobRunner: Records R/W=30220920/30220605 > reduce\n","2021-11-09 23:53:51,560 INFO streaming.PipeMapRed: Records R/W=30281780/30281531\n","2021-11-09 23:53:54,917 INFO streaming.PipeMapRed: R/W/S=30300000/30299748/0 in:10748=30300000/2819 [rec/s] out:10748=30299748/2819 [rec/s]\n","2021-11-09 23:53:55,635 INFO mapred.LocalJobRunner: Records R/W=30281780/30281531 > reduce\n","2021-11-09 23:53:56,118 INFO mapreduce.Job:  map 100% reduce 86%\n","2021-11-09 23:54:01,576 INFO streaming.PipeMapRed: Records R/W=30353792/30353382\n","2021-11-09 23:54:01,636 INFO mapred.LocalJobRunner: Records R/W=30353792/30353382 > reduce\n","2021-11-09 23:54:07,543 INFO streaming.PipeMapRed: R/W/S=30400000/30399644/0 in:10738=30400000/2831 [rec/s] out:10738=30399644/2831 [rec/s]\n","2021-11-09 23:54:07,636 INFO mapred.LocalJobRunner: Records R/W=30353792/30353382 > reduce\n","2021-11-09 23:54:11,578 INFO streaming.PipeMapRed: Records R/W=30430224/30429880\n","2021-11-09 23:54:13,637 INFO mapred.LocalJobRunner: Records R/W=30430224/30429880 > reduce\n","2021-11-09 23:54:19,637 INFO mapred.LocalJobRunner: Records R/W=30430224/30429880 > reduce\n","2021-11-09 23:54:21,580 INFO streaming.PipeMapRed: Records R/W=30498243/30498002\n","2021-11-09 23:54:21,853 INFO streaming.PipeMapRed: R/W/S=30500000/30499720/0 in:10716=30500000/2846 [rec/s] out:10716=30499720/2846 [rec/s]\n","2021-11-09 23:54:25,638 INFO mapred.LocalJobRunner: Records R/W=30498243/30498002 > reduce\n","2021-11-09 23:54:31,584 INFO streaming.PipeMapRed: Records R/W=30588128/30587852\n","2021-11-09 23:54:31,638 INFO mapred.LocalJobRunner: Records R/W=30588128/30587852 > reduce\n","2021-11-09 23:54:33,230 INFO streaming.PipeMapRed: R/W/S=30600000/30599553/0 in:10710=30600000/2857 [rec/s] out:10710=30599553/2857 [rec/s]\n","2021-11-09 23:54:37,639 INFO mapred.LocalJobRunner: Records R/W=30588128/30587852 > reduce\n","2021-11-09 23:54:41,589 INFO streaming.PipeMapRed: Records R/W=30655100/30654584\n","2021-11-09 23:54:43,639 INFO mapred.LocalJobRunner: Records R/W=30655100/30654584 > reduce\n","2021-11-09 23:54:45,101 INFO streaming.PipeMapRed: R/W/S=30700000/30699467/0 in:10700=30700000/2869 [rec/s] out:10700=30699467/2869 [rec/s]\n","2021-11-09 23:54:49,640 INFO mapred.LocalJobRunner: Records R/W=30655100/30654584 > reduce\n","2021-11-09 23:54:51,594 INFO streaming.PipeMapRed: Records R/W=30780664/30780039\n","2021-11-09 23:54:53,396 INFO streaming.PipeMapRed: R/W/S=30800000/30799700/0 in:10705=30800000/2877 [rec/s] out:10705=30799700/2877 [rec/s]\n","2021-11-09 23:54:55,640 INFO mapred.LocalJobRunner: Records R/W=30780664/30780039 > reduce\n","2021-11-09 23:55:01,600 INFO streaming.PipeMapRed: Records R/W=30869447/30869163\n","2021-11-09 23:55:01,641 INFO mapred.LocalJobRunner: Records R/W=30869447/30869163 > reduce\n","2021-11-09 23:55:04,320 INFO streaming.PipeMapRed: R/W/S=30900000/30899606/0 in:10699=30900000/2888 [rec/s] out:10699=30899606/2888 [rec/s]\n","2021-11-09 23:55:07,641 INFO mapred.LocalJobRunner: Records R/W=30869447/30869163 > reduce\n","2021-11-09 23:55:11,602 INFO streaming.PipeMapRed: Records R/W=30997013/30996493\n","2021-11-09 23:55:11,852 INFO streaming.PipeMapRed: R/W/S=31000000/30999623/0 in:10704=31000000/2896 [rec/s] out:10704=30999623/2896 [rec/s]\n","2021-11-09 23:55:13,642 INFO mapred.LocalJobRunner: Records R/W=30997013/30996493 > reduce\n","2021-11-09 23:55:19,642 INFO mapred.LocalJobRunner: Records R/W=30997013/30996493 > reduce\n","2021-11-09 23:55:21,605 INFO streaming.PipeMapRed: Records R/W=31099280/31098903\n","2021-11-09 23:55:21,710 INFO streaming.PipeMapRed: R/W/S=31100000/31099700/0 in:10701=31100000/2906 [rec/s] out:10701=31099700/2906 [rec/s]\n","2021-11-09 23:55:25,642 INFO mapred.LocalJobRunner: Records R/W=31099280/31098903 > reduce\n","2021-11-09 23:55:31,608 INFO streaming.PipeMapRed: Records R/W=31182853/31182496\n","2021-11-09 23:55:31,643 INFO mapred.LocalJobRunner: Records R/W=31182853/31182496 > reduce\n","2021-11-09 23:55:33,795 INFO streaming.PipeMapRed: R/W/S=31200000/31199581/0 in:10692=31200000/2918 [rec/s] out:10692=31199581/2918 [rec/s]\n","2021-11-09 23:55:37,643 INFO mapred.LocalJobRunner: Records R/W=31182853/31182496 > reduce\n","2021-11-09 23:55:41,616 INFO streaming.PipeMapRed: Records R/W=31269165/31268588\n","2021-11-09 23:55:43,644 INFO mapred.LocalJobRunner: Records R/W=31269165/31268588 > reduce\n","2021-11-09 23:55:44,087 INFO streaming.PipeMapRed: R/W/S=31300000/31299403/0 in:10689=31300000/2928 [rec/s] out:10689=31299403/2928 [rec/s]\n","2021-11-09 23:55:49,644 INFO mapred.LocalJobRunner: Records R/W=31269165/31268588 > reduce\n","2021-11-09 23:55:51,617 INFO streaming.PipeMapRed: Records R/W=31385541/31385026\n","2021-11-09 23:55:53,030 INFO streaming.PipeMapRed: R/W/S=31400000/31399346/0 in:10691=31400000/2937 [rec/s] out:10690=31399346/2937 [rec/s]\n","2021-11-09 23:55:55,645 INFO mapred.LocalJobRunner: Records R/W=31385541/31385026 > reduce\n","2021-11-09 23:56:01,471 INFO streaming.PipeMapRed: R/W/S=31500000/31499612/0 in:10696=31500000/2945 [rec/s] out:10695=31499612/2945 [rec/s]\n","2021-11-09 23:56:01,620 INFO streaming.PipeMapRed: Records R/W=31502188/31501551\n","2021-11-09 23:56:01,649 INFO mapred.LocalJobRunner: Records R/W=31502188/31501551 > reduce\n","2021-11-09 23:56:07,649 INFO mapred.LocalJobRunner: Records R/W=31502188/31501551 > reduce\n","2021-11-09 23:56:10,686 INFO streaming.PipeMapRed: R/W/S=31600000/31599611/0 in:10693=31600000/2955 [rec/s] out:10693=31599611/2955 [rec/s]\n","2021-11-09 23:56:11,622 INFO streaming.PipeMapRed: Records R/W=31609294/31608532\n","2021-11-09 23:56:13,650 INFO mapred.LocalJobRunner: Records R/W=31609294/31608532 > reduce\n","2021-11-09 23:56:17,030 INFO streaming.PipeMapRed: R/W/S=31700000/31698659/0 in:10705=31700000/2961 [rec/s] out:10705=31698659/2961 [rec/s]\n","2021-11-09 23:56:19,650 INFO mapred.LocalJobRunner: Records R/W=31609294/31608532 > reduce\n","2021-11-09 23:56:21,627 INFO streaming.PipeMapRed: Records R/W=31772191/31772004\n","2021-11-09 23:56:24,638 INFO streaming.PipeMapRed: R/W/S=31800000/31799178/0 in:10710=31800000/2969 [rec/s] out:10710=31799178/2969 [rec/s]\n","2021-11-09 23:56:25,651 INFO mapred.LocalJobRunner: Records R/W=31772191/31772004 > reduce\n","2021-11-09 23:56:31,542 INFO streaming.PipeMapRed: R/W/S=31900000/31899248/0 in:10722=31900000/2975 [rec/s] out:10722=31899248/2975 [rec/s]\n","2021-11-09 23:56:31,628 INFO streaming.PipeMapRed: Records R/W=31901031/31900290\n","2021-11-09 23:56:31,651 INFO mapred.LocalJobRunner: Records R/W=31901031/31900290 > reduce\n","2021-11-09 23:56:32,157 INFO mapreduce.Job:  map 100% reduce 87%\n","2021-11-09 23:56:37,652 INFO mapred.LocalJobRunner: Records R/W=31901031/31900290 > reduce\n","2021-11-09 23:56:38,494 INFO streaming.PipeMapRed: R/W/S=32000000/31999533/0 in:10731=32000000/2982 [rec/s] out:10730=31999533/2982 [rec/s]\n","2021-11-09 23:56:41,633 INFO streaming.PipeMapRed: Records R/W=32045430/32044788\n","2021-11-09 23:56:43,652 INFO mapred.LocalJobRunner: Records R/W=32045430/32044788 > reduce\n","2021-11-09 23:56:45,318 INFO streaming.PipeMapRed: R/W/S=32100000/32099328/0 in:10739=32100000/2989 [rec/s] out:10739=32099328/2989 [rec/s]\n","2021-11-09 23:56:49,652 INFO mapred.LocalJobRunner: Records R/W=32045430/32044788 > reduce\n","2021-11-09 23:56:51,636 INFO streaming.PipeMapRed: Records R/W=32191679/32191218\n","2021-11-09 23:56:52,207 INFO streaming.PipeMapRed: R/W/S=32200000/32199342/0 in:10747=32200000/2996 [rec/s] out:10747=32199342/2996 [rec/s]\n","2021-11-09 23:56:55,653 INFO mapred.LocalJobRunner: Records R/W=32191679/32191218 > reduce\n","2021-11-09 23:56:56,488 INFO streaming.PipeMapRed: R/W/S=32300000/32298880/0 in:10766=32300000/3000 [rec/s] out:10766=32298880/3000 [rec/s]\n","2021-11-09 23:57:01,639 INFO streaming.PipeMapRed: Records R/W=32380099/32379272\n","2021-11-09 23:57:01,653 INFO mapred.LocalJobRunner: Records R/W=32380099/32379272 > reduce\n","2021-11-09 23:57:03,078 INFO streaming.PipeMapRed: R/W/S=32400000/32399349/0 in:10774=32400000/3007 [rec/s] out:10774=32399349/3007 [rec/s]\n","2021-11-09 23:57:07,654 INFO mapred.LocalJobRunner: Records R/W=32380099/32379272 > reduce\n","2021-11-09 23:57:09,904 INFO streaming.PipeMapRed: R/W/S=32500000/32499608/0 in:10783=32500000/3014 [rec/s] out:10782=32499608/3014 [rec/s]\n","2021-11-09 23:57:11,641 INFO streaming.PipeMapRed: Records R/W=32525087/32524467\n","2021-11-09 23:57:13,654 INFO mapred.LocalJobRunner: Records R/W=32525087/32524467 > reduce\n","2021-11-09 23:57:16,808 INFO streaming.PipeMapRed: R/W/S=32600000/32599627/0 in:10791=32600000/3021 [rec/s] out:10791=32599627/3021 [rec/s]\n","2021-11-09 23:57:19,655 INFO mapred.LocalJobRunner: Records R/W=32525087/32524467 > reduce\n","2021-11-09 23:57:21,644 INFO streaming.PipeMapRed: Records R/W=32670288/32669454\n","2021-11-09 23:57:23,695 INFO streaming.PipeMapRed: R/W/S=32700000/32699567/0 in:10799=32700000/3028 [rec/s] out:10799=32699567/3028 [rec/s]\n","2021-11-09 23:57:25,655 INFO mapred.LocalJobRunner: Records R/W=32670288/32669454 > reduce\n","2021-11-09 23:57:31,647 INFO streaming.PipeMapRed: Records R/W=32793472/32792930\n","2021-11-09 23:57:31,656 INFO mapred.LocalJobRunner: Records R/W=32793472/32792930 > reduce\n","2021-11-09 23:57:32,340 INFO streaming.PipeMapRed: R/W/S=32800000/32799447/0 in:10803=32800000/3036 [rec/s] out:10803=32799447/3036 [rec/s]\n","2021-11-09 23:57:37,656 INFO mapred.LocalJobRunner: Records R/W=32793472/32792930 > reduce\n","2021-11-09 23:57:41,648 INFO streaming.PipeMapRed: Records R/W=32881299/32880695\n","2021-11-09 23:57:43,100 INFO streaming.PipeMapRed: R/W/S=32900000/32899304/0 in:10797=32900000/3047 [rec/s] out:10797=32899304/3047 [rec/s]\n","2021-11-09 23:57:43,661 INFO mapred.LocalJobRunner: Records R/W=32881299/32880695 > reduce\n","2021-11-09 23:57:49,661 INFO mapred.LocalJobRunner: Records R/W=32881299/32880695 > reduce\n","2021-11-09 23:57:51,192 INFO streaming.PipeMapRed: R/W/S=33000000/32999584/0 in:10801=33000000/3055 [rec/s] out:10801=32999584/3055 [rec/s]\n","2021-11-09 23:57:51,651 INFO streaming.PipeMapRed: Records R/W=33005558/33005157\n","2021-11-09 23:57:55,666 INFO mapred.LocalJobRunner: Records R/W=33005558/33005157 > reduce\n","2021-11-09 23:57:59,320 INFO streaming.PipeMapRed: R/W/S=33100000/33099368/0 in:10806=33100000/3063 [rec/s] out:10806=33099368/3063 [rec/s]\n","2021-11-09 23:58:01,654 INFO streaming.PipeMapRed: Records R/W=33127734/33127143\n","2021-11-09 23:58:01,666 INFO mapred.LocalJobRunner: Records R/W=33127734/33127143 > reduce\n","2021-11-09 23:58:05,134 INFO streaming.PipeMapRed: R/W/S=33200000/33199060/0 in:10817=33200000/3069 [rec/s] out:10817=33199060/3069 [rec/s]\n","2021-11-09 23:58:07,667 INFO mapred.LocalJobRunner: Records R/W=33127734/33127143 > reduce\n","2021-11-09 23:58:11,656 INFO streaming.PipeMapRed: Records R/W=33284288/33283819\n","2021-11-09 23:58:12,899 INFO streaming.PipeMapRed: R/W/S=33300000/33299382/0 in:10822=33300000/3077 [rec/s] out:10822=33299382/3077 [rec/s]\n","2021-11-09 23:58:13,668 INFO mapred.LocalJobRunner: Records R/W=33284288/33283819 > reduce\n","2021-11-09 23:58:19,668 INFO mapred.LocalJobRunner: Records R/W=33284288/33283819 > reduce\n","2021-11-09 23:58:21,657 INFO streaming.PipeMapRed: Records R/W=33395865/33395192\n","2021-11-09 23:58:21,973 INFO streaming.PipeMapRed: R/W/S=33400000/33399421/0 in:10823=33400000/3086 [rec/s] out:10822=33399421/3086 [rec/s]\n","2021-11-09 23:58:25,669 INFO mapred.LocalJobRunner: Records R/W=33395865/33395192 > reduce\n","2021-11-09 23:58:31,458 INFO streaming.PipeMapRed: R/W/S=33500000/33499636/0 in:10823=33500000/3095 [rec/s] out:10823=33499636/3095 [rec/s]\n","2021-11-09 23:58:31,661 INFO streaming.PipeMapRed: Records R/W=33503278/33502754\n","2021-11-09 23:58:31,669 INFO mapred.LocalJobRunner: Records R/W=33503278/33502754 > reduce\n","2021-11-09 23:58:37,680 INFO mapred.LocalJobRunner: Records R/W=33503278/33502754 > reduce\n","2021-11-09 23:58:37,728 INFO streaming.PipeMapRed: R/W/S=33600000/33598705/0 in:10831=33600000/3102 [rec/s] out:10831=33598705/3102 [rec/s]\n","2021-11-09 23:58:41,662 INFO streaming.PipeMapRed: Records R/W=33668561/33668204\n","2021-11-09 23:58:43,680 INFO mapred.LocalJobRunner: Records R/W=33668561/33668204 > reduce\n","2021-11-09 23:58:43,781 INFO streaming.PipeMapRed: R/W/S=33700000/33699652/0 in:10842=33700000/3108 [rec/s] out:10842=33699652/3108 [rec/s]\n","2021-11-09 23:58:49,680 INFO mapred.LocalJobRunner: Records R/W=33668561/33668204 > reduce\n","2021-11-09 23:58:50,193 INFO mapreduce.Job:  map 100% reduce 88%\n","2021-11-09 23:58:50,966 INFO streaming.PipeMapRed: R/W/S=33800000/33799276/0 in:10850=33800000/3115 [rec/s] out:10850=33799276/3115 [rec/s]\n","2021-11-09 23:58:51,663 INFO streaming.PipeMapRed: Records R/W=33814023/33813450\n","2021-11-09 23:58:54,920 INFO streaming.PipeMapRed: R/W/S=33900000/33899035/0 in:10868=33900000/3119 [rec/s] out:10868=33899035/3119 [rec/s]\n","2021-11-09 23:58:55,681 INFO mapred.LocalJobRunner: Records R/W=33814023/33813450 > reduce\n","2021-11-09 23:59:00,429 INFO streaming.PipeMapRed: R/W/S=34000000/33999717/0 in:10883=34000000/3124 [rec/s] out:10883=33999717/3124 [rec/s]\n","2021-11-09 23:59:01,666 INFO streaming.PipeMapRed: Records R/W=34011937/34011513\n","2021-11-09 23:59:01,681 INFO mapred.LocalJobRunner: Records R/W=34011937/34011513 > reduce\n","2021-11-09 23:59:07,682 INFO mapred.LocalJobRunner: Records R/W=34011937/34011513 > reduce\n","2021-11-09 23:59:09,971 INFO streaming.PipeMapRed: R/W/S=34100000/34099611/0 in:10880=34100000/3134 [rec/s] out:10880=34099611/3134 [rec/s]\n","2021-11-09 23:59:11,667 INFO streaming.PipeMapRed: Records R/W=34115588/34115221\n","2021-11-09 23:59:13,682 INFO mapred.LocalJobRunner: Records R/W=34115588/34115221 > reduce\n","2021-11-09 23:59:19,170 INFO streaming.PipeMapRed: R/W/S=34200000/34199473/0 in:10881=34200000/3143 [rec/s] out:10881=34199473/3143 [rec/s]\n","2021-11-09 23:59:19,682 INFO mapred.LocalJobRunner: Records R/W=34115588/34115221 > reduce\n","2021-11-09 23:59:21,673 INFO streaming.PipeMapRed: Records R/W=34224644/34224000\n","2021-11-09 23:59:25,683 INFO mapred.LocalJobRunner: Records R/W=34224644/34224000 > reduce\n","2021-11-09 23:59:29,676 INFO streaming.PipeMapRed: R/W/S=34300000/34299560/0 in:10875=34300000/3154 [rec/s] out:10874=34299560/3154 [rec/s]\n","2021-11-09 23:59:31,676 INFO streaming.PipeMapRed: Records R/W=34325216/34324857\n","2021-11-09 23:59:31,683 INFO mapred.LocalJobRunner: Records R/W=34325216/34324857 > reduce\n","2021-11-09 23:59:37,684 INFO mapred.LocalJobRunner: Records R/W=34325216/34324857 > reduce\n","2021-11-09 23:59:39,135 INFO streaming.PipeMapRed: R/W/S=34400000/34399444/0 in:10875=34400000/3163 [rec/s] out:10875=34399444/3163 [rec/s]\n","2021-11-09 23:59:41,681 INFO streaming.PipeMapRed: Records R/W=34421978/34421579\n","2021-11-09 23:59:43,684 INFO mapred.LocalJobRunner: Records R/W=34421978/34421579 > reduce\n","2021-11-09 23:59:49,308 INFO streaming.PipeMapRed: R/W/S=34500000/34499474/0 in:10872=34500000/3173 [rec/s] out:10872=34499474/3173 [rec/s]\n","2021-11-09 23:59:49,685 INFO mapred.LocalJobRunner: Records R/W=34421978/34421579 > reduce\n","2021-11-09 23:59:51,684 INFO streaming.PipeMapRed: Records R/W=34533293/34532544\n","2021-11-09 23:59:55,685 INFO mapred.LocalJobRunner: Records R/W=34533293/34532544 > reduce\n","2021-11-09 23:59:57,786 INFO streaming.PipeMapRed: R/W/S=34600000/34599856/0 in:10873=34600000/3182 [rec/s] out:10873=34599856/3182 [rec/s]\n","2021-11-10 00:00:01,686 INFO mapred.LocalJobRunner: Records R/W=34533293/34532544 > reduce\n","2021-11-10 00:00:01,688 INFO streaming.PipeMapRed: Records R/W=34651249/34650780\n","2021-11-10 00:00:07,122 INFO streaming.PipeMapRed: R/W/S=34700000/34699425/0 in:10874=34700000/3191 [rec/s] out:10874=34699425/3191 [rec/s]\n","2021-11-10 00:00:07,686 INFO mapred.LocalJobRunner: Records R/W=34651249/34650780 > reduce\n","2021-11-10 00:00:11,691 INFO streaming.PipeMapRed: Records R/W=34741674/34741386\n","2021-11-10 00:00:13,687 INFO mapred.LocalJobRunner: Records R/W=34741674/34741386 > reduce\n","2021-11-10 00:00:17,037 INFO streaming.PipeMapRed: R/W/S=34800000/34799608/0 in:10871=34800000/3201 [rec/s] out:10871=34799608/3201 [rec/s]\n","2021-11-10 00:00:19,687 INFO mapred.LocalJobRunner: Records R/W=34741674/34741386 > reduce\n","2021-11-10 00:00:21,692 INFO streaming.PipeMapRed: Records R/W=34849569/34849056\n","2021-11-10 00:00:25,688 INFO mapred.LocalJobRunner: Records R/W=34849569/34849056 > reduce\n","2021-11-10 00:00:27,847 INFO streaming.PipeMapRed: R/W/S=34900000/34899487/0 in:10865=34900000/3212 [rec/s] out:10865=34899487/3212 [rec/s]\n","2021-11-10 00:00:31,688 INFO mapred.LocalJobRunner: Records R/W=34849569/34849056 > reduce\n","2021-11-10 00:00:31,696 INFO streaming.PipeMapRed: Records R/W=34934000/34933398\n","2021-11-10 00:00:36,249 INFO streaming.PipeMapRed: R/W/S=35000000/34999624/0 in:10869=35000000/3220 [rec/s] out:10869=34999624/3220 [rec/s]\n","2021-11-10 00:00:37,689 INFO mapred.LocalJobRunner: Records R/W=34934000/34933398 > reduce\n","2021-11-10 00:00:41,702 INFO streaming.PipeMapRed: Records R/W=35060353/35060114\n","2021-11-10 00:00:43,689 INFO mapred.LocalJobRunner: Records R/W=35060353/35060114 > reduce\n","2021-11-10 00:00:44,831 INFO streaming.PipeMapRed: R/W/S=35100000/35099216/0 in:10870=35100000/3229 [rec/s] out:10869=35099216/3229 [rec/s]\n","2021-11-10 00:00:49,690 INFO mapred.LocalJobRunner: Records R/W=35060353/35060114 > reduce\n","2021-11-10 00:00:51,191 INFO streaming.PipeMapRed: R/W/S=35200000/35199538/0 in:10880=35200000/3235 [rec/s] out:10880=35199538/3235 [rec/s]\n","2021-11-10 00:00:51,704 INFO streaming.PipeMapRed: Records R/W=35208688/35207946\n","2021-11-10 00:00:55,690 INFO mapred.LocalJobRunner: Records R/W=35208688/35207946 > reduce\n","2021-11-10 00:00:56,809 INFO streaming.PipeMapRed: R/W/S=35300000/35299340/0 in:10891=35300000/3241 [rec/s] out:10891=35299340/3241 [rec/s]\n","2021-11-10 00:01:01,691 INFO mapred.LocalJobRunner: Records R/W=35208688/35207946 > reduce\n","2021-11-10 00:01:01,706 INFO streaming.PipeMapRed: Records R/W=35383077/35382455\n","2021-11-10 00:01:02,706 INFO streaming.PipeMapRed: R/W/S=35400000/35399363/0 in:10902=35400000/3247 [rec/s] out:10902=35399363/3247 [rec/s]\n","2021-11-10 00:01:07,691 INFO mapred.LocalJobRunner: Records R/W=35383077/35382455 > reduce\n","2021-11-10 00:01:09,513 INFO streaming.PipeMapRed: R/W/S=35500000/35499285/0 in:10913=35500000/3253 [rec/s] out:10912=35499285/3253 [rec/s]\n","2021-11-10 00:01:11,709 INFO streaming.PipeMapRed: Records R/W=35520320/35519732\n","2021-11-10 00:01:13,692 INFO mapred.LocalJobRunner: Records R/W=35520320/35519732 > reduce\n","2021-11-10 00:01:14,232 INFO mapreduce.Job:  map 100% reduce 89%\n","2021-11-10 00:01:19,253 INFO streaming.PipeMapRed: R/W/S=35600000/35599478/0 in:10910=35600000/3263 [rec/s] out:10910=35599478/3263 [rec/s]\n","2021-11-10 00:01:19,692 INFO mapred.LocalJobRunner: Records R/W=35520320/35519732 > reduce\n","2021-11-10 00:01:21,711 INFO streaming.PipeMapRed: Records R/W=35625062/35624783\n","2021-11-10 00:01:25,692 INFO mapred.LocalJobRunner: Records R/W=35625062/35624783 > reduce\n","2021-11-10 00:01:27,928 INFO streaming.PipeMapRed: R/W/S=35700000/35699509/0 in:10910=35700000/3272 [rec/s] out:10910=35699509/3272 [rec/s]\n","2021-11-10 00:01:31,693 INFO mapred.LocalJobRunner: Records R/W=35625062/35624783 > reduce\n","2021-11-10 00:01:31,712 INFO streaming.PipeMapRed: Records R/W=35747265/35746589\n","2021-11-10 00:01:36,029 INFO streaming.PipeMapRed: R/W/S=35800000/35799435/0 in:10914=35800000/3280 [rec/s] out:10914=35799435/3280 [rec/s]\n","2021-11-10 00:01:37,693 INFO mapred.LocalJobRunner: Records R/W=35747265/35746589 > reduce\n","2021-11-10 00:01:41,715 INFO streaming.PipeMapRed: Records R/W=35860421/35860085\n","2021-11-10 00:01:43,694 INFO mapred.LocalJobRunner: Records R/W=35860421/35860085 > reduce\n","2021-11-10 00:01:45,215 INFO streaming.PipeMapRed: R/W/S=35900000/35899616/0 in:10915=35900000/3289 [rec/s] out:10915=35899616/3289 [rec/s]\n","2021-11-10 00:01:49,699 INFO mapred.LocalJobRunner: Records R/W=35860421/35860085 > reduce\n","2021-11-10 00:01:51,719 INFO streaming.PipeMapRed: Records R/W=35963572/35963144\n","2021-11-10 00:01:55,700 INFO mapred.LocalJobRunner: Records R/W=35963572/35963144 > reduce\n","2021-11-10 00:01:56,576 INFO streaming.PipeMapRed: R/W/S=36000000/35999586/0 in:10905=36000000/3301 [rec/s] out:10905=35999586/3301 [rec/s]\n","2021-11-10 00:02:01,700 INFO mapred.LocalJobRunner: Records R/W=35963572/35963144 > reduce\n","2021-11-10 00:02:01,721 INFO streaming.PipeMapRed: Records R/W=36045414/36045102\n","2021-11-10 00:02:07,012 INFO streaming.PipeMapRed: R/W/S=36100000/36099590/0 in:10903=36100000/3311 [rec/s] out:10902=36099590/3311 [rec/s]\n","2021-11-10 00:02:07,701 INFO mapred.LocalJobRunner: Records R/W=36045414/36045102 > reduce\n","2021-11-10 00:02:11,724 INFO streaming.PipeMapRed: Records R/W=36135893/36135555\n","2021-11-10 00:02:13,701 INFO mapred.LocalJobRunner: Records R/W=36135893/36135555 > reduce\n","2021-11-10 00:02:17,572 INFO streaming.PipeMapRed: R/W/S=36200000/36199231/0 in:10897=36200000/3322 [rec/s] out:10896=36199231/3322 [rec/s]\n","2021-11-10 00:02:19,702 INFO mapred.LocalJobRunner: Records R/W=36135893/36135555 > reduce\n","2021-11-10 00:02:21,726 INFO streaming.PipeMapRed: Records R/W=36256032/36255435\n","2021-11-10 00:02:25,702 INFO mapred.LocalJobRunner: Records R/W=36256032/36255435 > reduce\n","2021-11-10 00:02:26,236 INFO streaming.PipeMapRed: R/W/S=36300000/36299394/0 in:10900=36300000/3330 [rec/s] out:10900=36299394/3330 [rec/s]\n","2021-11-10 00:02:31,702 INFO mapred.LocalJobRunner: Records R/W=36256032/36255435 > reduce\n","2021-11-10 00:02:31,728 INFO streaming.PipeMapRed: Records R/W=36355530/36355029\n","2021-11-10 00:02:36,203 INFO streaming.PipeMapRed: R/W/S=36400000/36399524/0 in:10898=36400000/3340 [rec/s] out:10898=36399524/3340 [rec/s]\n","2021-11-10 00:02:37,703 INFO mapred.LocalJobRunner: Records R/W=36355530/36355029 > reduce\n","2021-11-10 00:02:41,734 INFO streaming.PipeMapRed: Records R/W=36445944/36445579\n","2021-11-10 00:02:43,703 INFO mapred.LocalJobRunner: Records R/W=36445944/36445579 > reduce\n","2021-11-10 00:02:46,623 INFO streaming.PipeMapRed: R/W/S=36500000/36499644/0 in:10892=36500000/3351 [rec/s] out:10892=36499644/3351 [rec/s]\n","2021-11-10 00:02:49,704 INFO mapred.LocalJobRunner: Records R/W=36445944/36445579 > reduce\n","2021-11-10 00:02:51,735 INFO streaming.PipeMapRed: Records R/W=36558418/36558160\n","2021-11-10 00:02:55,127 INFO streaming.PipeMapRed: R/W/S=36600000/36599710/0 in:10896=36600000/3359 [rec/s] out:10896=36599710/3359 [rec/s]\n","2021-11-10 00:02:55,704 INFO mapred.LocalJobRunner: Records R/W=36558418/36558160 > reduce\n","2021-11-10 00:03:01,704 INFO mapred.LocalJobRunner: Records R/W=36558418/36558160 > reduce\n","2021-11-10 00:03:01,737 INFO streaming.PipeMapRed: Records R/W=36653671/36652979\n","2021-11-10 00:03:05,764 INFO streaming.PipeMapRed: R/W/S=36700000/36699738/0 in:10890=36700000/3370 [rec/s] out:10890=36699738/3370 [rec/s]\n","2021-11-10 00:03:07,705 INFO mapred.LocalJobRunner: Records R/W=36653671/36652979 > reduce\n","2021-11-10 00:03:11,739 INFO streaming.PipeMapRed: Records R/W=36772593/36771826\n","2021-11-10 00:03:13,705 INFO mapred.LocalJobRunner: Records R/W=36772593/36771826 > reduce\n","2021-11-10 00:03:14,159 INFO streaming.PipeMapRed: R/W/S=36800000/36799670/0 in:10894=36800000/3378 [rec/s] out:10893=36799670/3378 [rec/s]\n","2021-11-10 00:03:19,706 INFO mapred.LocalJobRunner: Records R/W=36772593/36771826 > reduce\n","2021-11-10 00:03:21,741 INFO streaming.PipeMapRed: Records R/W=36892474/36892105\n","2021-11-10 00:03:22,366 INFO streaming.PipeMapRed: R/W/S=36900000/36899463/0 in:10897=36900000/3386 [rec/s] out:10897=36899463/3386 [rec/s]\n","2021-11-10 00:03:25,706 INFO mapred.LocalJobRunner: Records R/W=36892474/36892105 > reduce\n","2021-11-10 00:03:31,062 INFO streaming.PipeMapRed: R/W/S=37000000/36999533/0 in:10898=37000000/3395 [rec/s] out:10898=36999533/3395 [rec/s]\n","2021-11-10 00:03:31,707 INFO mapred.LocalJobRunner: Records R/W=36892474/36892105 > reduce\n","2021-11-10 00:03:31,745 INFO streaming.PipeMapRed: Records R/W=37008036/37007661\n","2021-11-10 00:03:37,707 INFO mapred.LocalJobRunner: Records R/W=37008036/37007661 > reduce\n","2021-11-10 00:03:39,450 INFO streaming.PipeMapRed: R/W/S=37100000/37099382/0 in:10902=37100000/3403 [rec/s] out:10901=37099382/3403 [rec/s]\n","2021-11-10 00:03:41,747 INFO streaming.PipeMapRed: Records R/W=37127371/37126958\n","2021-11-10 00:03:43,707 INFO mapred.LocalJobRunner: Records R/W=37127371/37126958 > reduce\n","2021-11-10 00:03:44,272 INFO mapreduce.Job:  map 100% reduce 90%\n","2021-11-10 00:03:47,837 INFO streaming.PipeMapRed: R/W/S=37200000/37199653/0 in:10902=37200000/3412 [rec/s] out:10902=37199653/3412 [rec/s]\n","2021-11-10 00:03:49,708 INFO mapred.LocalJobRunner: Records R/W=37127371/37126958 > reduce\n","2021-11-10 00:03:51,749 INFO streaming.PipeMapRed: Records R/W=37282516/37282228\n","2021-11-10 00:03:53,293 INFO streaming.PipeMapRed: R/W/S=37300000/37299586/0 in:10916=37300000/3417 [rec/s] out:10915=37299586/3417 [rec/s]\n","2021-11-10 00:03:55,708 INFO mapred.LocalJobRunner: Records R/W=37282516/37282228 > reduce\n","2021-11-10 00:04:01,709 INFO mapred.LocalJobRunner: Records R/W=37282516/37282228 > reduce\n","2021-11-10 00:04:01,767 INFO streaming.PipeMapRed: Records R/W=37355301/37354854\n","2021-11-10 00:04:06,387 INFO streaming.PipeMapRed: R/W/S=37400000/37399414/0 in:10903=37400000/3430 [rec/s] out:10903=37399414/3430 [rec/s]\n","2021-11-10 00:04:07,709 INFO mapred.LocalJobRunner: Records R/W=37355301/37354854 > reduce\n","2021-11-10 00:04:11,772 INFO streaming.PipeMapRed: Records R/W=37466300/37465900\n","2021-11-10 00:04:13,710 INFO mapred.LocalJobRunner: Records R/W=37466300/37465900 > reduce\n","2021-11-10 00:04:14,588 INFO streaming.PipeMapRed: R/W/S=37500000/37499442/0 in:10904=37500000/3439 [rec/s] out:10904=37499442/3439 [rec/s]\n","2021-11-10 00:04:19,710 INFO mapred.LocalJobRunner: Records R/W=37466300/37465900 > reduce\n","2021-11-10 00:04:21,775 INFO streaming.PipeMapRed: Records R/W=37575160/37574942\n","2021-11-10 00:04:25,379 INFO streaming.PipeMapRed: R/W/S=37600000/37599609/0 in:10901=37600000/3449 [rec/s] out:10901=37599609/3449 [rec/s]\n","2021-11-10 00:04:25,711 INFO mapred.LocalJobRunner: Records R/W=37575160/37574942 > reduce\n","2021-11-10 00:04:31,712 INFO mapred.LocalJobRunner: Records R/W=37575160/37574942 > reduce\n","2021-11-10 00:04:31,786 INFO streaming.PipeMapRed: Records R/W=37645490/37645173\n","2021-11-10 00:04:37,712 INFO mapred.LocalJobRunner: Records R/W=37645490/37645173 > reduce\n","2021-11-10 00:04:38,077 INFO streaming.PipeMapRed: R/W/S=37700000/37699636/0 in:10889=37700000/3462 [rec/s] out:10889=37699636/3462 [rec/s]\n","2021-11-10 00:04:41,787 INFO streaming.PipeMapRed: Records R/W=37736399/37736037\n","2021-11-10 00:04:43,713 INFO mapred.LocalJobRunner: Records R/W=37736399/37736037 > reduce\n","2021-11-10 00:04:48,042 INFO streaming.PipeMapRed: R/W/S=37800000/37799635/0 in:10887=37800000/3472 [rec/s] out:10886=37799635/3472 [rec/s]\n","2021-11-10 00:04:49,713 INFO mapred.LocalJobRunner: Records R/W=37736399/37736037 > reduce\n","2021-11-10 00:04:51,793 INFO streaming.PipeMapRed: Records R/W=37830808/37830543\n","2021-11-10 00:04:55,713 INFO mapred.LocalJobRunner: Records R/W=37830808/37830543 > reduce\n","2021-11-10 00:05:00,145 INFO streaming.PipeMapRed: R/W/S=37900000/37899653/0 in:10878=37900000/3484 [rec/s] out:10878=37899653/3484 [rec/s]\n","2021-11-10 00:05:01,714 INFO mapred.LocalJobRunner: Records R/W=37830808/37830543 > reduce\n","2021-11-10 00:05:01,794 INFO streaming.PipeMapRed: Records R/W=37923233/37922586\n","2021-11-10 00:05:07,714 INFO mapred.LocalJobRunner: Records R/W=37923233/37922586 > reduce\n","2021-11-10 00:05:11,775 INFO streaming.PipeMapRed: R/W/S=38000000/37999641/0 in:10869=38000000/3496 [rec/s] out:10869=37999641/3496 [rec/s]\n","2021-11-10 00:05:11,806 INFO streaming.PipeMapRed: Records R/W=38000097/37999816\n","2021-11-10 00:05:13,715 INFO mapred.LocalJobRunner: Records R/W=38000097/37999816 > reduce\n","2021-11-10 00:05:19,715 INFO mapred.LocalJobRunner: Records R/W=38000097/37999816 > reduce\n","2021-11-10 00:05:21,811 INFO streaming.PipeMapRed: Records R/W=38097359/38096908\n","2021-11-10 00:05:22,094 INFO streaming.PipeMapRed: R/W/S=38100000/38099432/0 in:10867=38100000/3506 [rec/s] out:10866=38099432/3506 [rec/s]\n","2021-11-10 00:05:25,716 INFO mapred.LocalJobRunner: Records R/W=38097359/38096908 > reduce\n","2021-11-10 00:05:31,615 INFO streaming.PipeMapRed: R/W/S=38200000/38199503/0 in:10864=38200000/3516 [rec/s] out:10864=38199503/3516 [rec/s]\n","2021-11-10 00:05:31,716 INFO mapred.LocalJobRunner: Records R/W=38097359/38096908 > reduce\n","2021-11-10 00:05:31,817 INFO streaming.PipeMapRed: Records R/W=38202398/38201745\n","2021-11-10 00:05:37,717 INFO mapred.LocalJobRunner: Records R/W=38202398/38201745 > reduce\n","2021-11-10 00:05:40,766 INFO streaming.PipeMapRed: R/W/S=38300000/38299286/0 in:10865=38300000/3525 [rec/s] out:10865=38299286/3525 [rec/s]\n","2021-11-10 00:05:41,819 INFO streaming.PipeMapRed: Records R/W=38311802/38311193\n","2021-11-10 00:05:43,717 INFO mapred.LocalJobRunner: Records R/W=38311802/38311193 > reduce\n","2021-11-10 00:05:49,718 INFO mapred.LocalJobRunner: Records R/W=38311802/38311193 > reduce\n","2021-11-10 00:05:51,576 INFO streaming.PipeMapRed: R/W/S=38400000/38399459/0 in:10859=38400000/3536 [rec/s] out:10859=38399459/3536 [rec/s]\n","2021-11-10 00:05:51,820 INFO streaming.PipeMapRed: Records R/W=38402417/38401710\n","2021-11-10 00:05:55,718 INFO mapred.LocalJobRunner: Records R/W=38402417/38401710 > reduce\n","2021-11-10 00:06:01,719 INFO mapred.LocalJobRunner: Records R/W=38402417/38401710 > reduce\n","2021-11-10 00:06:01,822 INFO streaming.PipeMapRed: Records R/W=38495102/38494362\n","2021-11-10 00:06:02,358 INFO streaming.PipeMapRed: R/W/S=38500000/38499741/0 in:10857=38500000/3546 [rec/s] out:10857=38499741/3546 [rec/s]\n","2021-11-10 00:06:07,719 INFO mapred.LocalJobRunner: Records R/W=38495102/38494362 > reduce\n","2021-11-10 00:06:11,670 INFO streaming.PipeMapRed: R/W/S=38600000/38599553/0 in:10854=38600000/3556 [rec/s] out:10854=38599553/3556 [rec/s]\n","2021-11-10 00:06:11,834 INFO streaming.PipeMapRed: Records R/W=38601079/38600828\n","2021-11-10 00:06:13,720 INFO mapred.LocalJobRunner: Records R/W=38601079/38600828 > reduce\n","2021-11-10 00:06:18,084 INFO streaming.PipeMapRed: R/W/S=38700000/38699572/0 in:10864=38700000/3562 [rec/s] out:10864=38699572/3562 [rec/s]\n","2021-11-10 00:06:19,720 INFO mapred.LocalJobRunner: Records R/W=38601079/38600828 > reduce\n","2021-11-10 00:06:20,318 INFO mapreduce.Job:  map 100% reduce 91%\n","2021-11-10 00:06:21,835 INFO streaming.PipeMapRed: Records R/W=38762549/38761586\n","2021-11-10 00:06:23,576 INFO streaming.PipeMapRed: R/W/S=38800000/38799196/0 in:10874=38800000/3568 [rec/s] out:10874=38799196/3568 [rec/s]\n","2021-11-10 00:06:25,721 INFO mapred.LocalJobRunner: Records R/W=38762549/38761586 > reduce\n","2021-11-10 00:06:31,166 INFO streaming.PipeMapRed: R/W/S=38900000/38899322/0 in:10881=38900000/3575 [rec/s] out:10880=38899322/3575 [rec/s]\n","2021-11-10 00:06:31,721 INFO mapred.LocalJobRunner: Records R/W=38762549/38761586 > reduce\n","2021-11-10 00:06:31,840 INFO streaming.PipeMapRed: Records R/W=38908602/38908078\n","2021-11-10 00:06:37,721 INFO mapred.LocalJobRunner: Records R/W=38908602/38908078 > reduce\n","2021-11-10 00:06:41,117 INFO streaming.PipeMapRed: R/W/S=39000000/38999545/0 in:10878=39000000/3585 [rec/s] out:10878=38999545/3585 [rec/s]\n","2021-11-10 00:06:41,842 INFO streaming.PipeMapRed: Records R/W=39007107/39006610\n","2021-11-10 00:06:43,722 INFO mapred.LocalJobRunner: Records R/W=39007107/39006610 > reduce\n","2021-11-10 00:06:49,722 INFO mapred.LocalJobRunner: Records R/W=39007107/39006610 > reduce\n","2021-11-10 00:06:50,596 INFO streaming.PipeMapRed: R/W/S=39100000/39099452/0 in:10876=39100000/3595 [rec/s] out:10876=39099452/3595 [rec/s]\n","2021-11-10 00:06:51,843 INFO streaming.PipeMapRed: Records R/W=39112956/39112566\n","2021-11-10 00:06:55,723 INFO mapred.LocalJobRunner: Records R/W=39112956/39112566 > reduce\n","2021-11-10 00:06:59,950 INFO streaming.PipeMapRed: R/W/S=39200000/39199473/0 in:10876=39200000/3604 [rec/s] out:10876=39199473/3604 [rec/s]\n","2021-11-10 00:07:01,723 INFO mapred.LocalJobRunner: Records R/W=39112956/39112566 > reduce\n","2021-11-10 00:07:01,846 INFO streaming.PipeMapRed: Records R/W=39236170/39235543\n","2021-11-10 00:07:07,676 INFO streaming.PipeMapRed: R/W/S=39300000/39299478/0 in:10880=39300000/3612 [rec/s] out:10880=39299478/3612 [rec/s]\n","2021-11-10 00:07:07,724 INFO mapred.LocalJobRunner: Records R/W=39236170/39235543 > reduce\n","2021-11-10 00:07:11,850 INFO streaming.PipeMapRed: Records R/W=39346122/39345648\n","2021-11-10 00:07:13,724 INFO mapred.LocalJobRunner: Records R/W=39346122/39345648 > reduce\n","2021-11-10 00:07:16,633 INFO streaming.PipeMapRed: R/W/S=39400000/39399456/0 in:10880=39400000/3621 [rec/s] out:10880=39399456/3621 [rec/s]\n","2021-11-10 00:07:19,725 INFO mapred.LocalJobRunner: Records R/W=39346122/39345648 > reduce\n","2021-11-10 00:07:21,852 INFO streaming.PipeMapRed: Records R/W=39450406/39449774\n","2021-11-10 00:07:25,725 INFO mapred.LocalJobRunner: Records R/W=39450406/39449774 > reduce\n","2021-11-10 00:07:26,172 INFO streaming.PipeMapRed: R/W/S=39500000/39499206/0 in:10881=39500000/3630 [rec/s] out:10881=39499206/3630 [rec/s]\n","2021-11-10 00:07:31,726 INFO mapred.LocalJobRunner: Records R/W=39450406/39449774 > reduce\n","2021-11-10 00:07:31,853 INFO streaming.PipeMapRed: Records R/W=39585463/39585079\n","2021-11-10 00:07:32,805 INFO streaming.PipeMapRed: R/W/S=39600000/39599335/0 in:10888=39600000/3637 [rec/s] out:10887=39599335/3637 [rec/s]\n","2021-11-10 00:07:37,726 INFO mapred.LocalJobRunner: Records R/W=39585463/39585079 > reduce\n","2021-11-10 00:07:39,497 INFO streaming.PipeMapRed: R/W/S=39700000/39699376/0 in:10897=39700000/3643 [rec/s] out:10897=39699376/3643 [rec/s]\n","2021-11-10 00:07:41,857 INFO streaming.PipeMapRed: Records R/W=39735002/39734488\n","2021-11-10 00:07:43,727 INFO mapred.LocalJobRunner: Records R/W=39735002/39734488 > reduce\n","2021-11-10 00:07:44,935 INFO streaming.PipeMapRed: R/W/S=39800000/39799491/0 in:10907=39800000/3649 [rec/s] out:10906=39799491/3649 [rec/s]\n","2021-11-10 00:07:49,728 INFO mapred.LocalJobRunner: Records R/W=39735002/39734488 > reduce\n","2021-11-10 00:07:51,859 INFO streaming.PipeMapRed: Records R/W=39897414/39896966\n","2021-11-10 00:07:52,071 INFO streaming.PipeMapRed: R/W/S=39900000/39899645/0 in:10913=39900000/3656 [rec/s] out:10913=39899645/3656 [rec/s]\n","2021-11-10 00:07:55,728 INFO mapred.LocalJobRunner: Records R/W=39897414/39896966 > reduce\n","2021-11-10 00:07:59,104 INFO streaming.PipeMapRed: R/W/S=40000000/39999577/0 in:10920=40000000/3663 [rec/s] out:10919=39999577/3663 [rec/s]\n","2021-11-10 00:08:01,729 INFO mapred.LocalJobRunner: Records R/W=39897414/39896966 > reduce\n","2021-11-10 00:08:01,860 INFO streaming.PipeMapRed: Records R/W=40029443/40028979\n","2021-11-10 00:08:07,730 INFO mapred.LocalJobRunner: Records R/W=40029443/40028979 > reduce\n","2021-11-10 00:08:08,760 INFO streaming.PipeMapRed: R/W/S=40100000/40099436/0 in:10917=40100000/3673 [rec/s] out:10917=40099436/3673 [rec/s]\n","2021-11-10 00:08:11,864 INFO streaming.PipeMapRed: Records R/W=40127641/40127121\n","2021-11-10 00:08:13,730 INFO mapred.LocalJobRunner: Records R/W=40127641/40127121 > reduce\n","2021-11-10 00:08:19,731 INFO mapred.LocalJobRunner: Records R/W=40127641/40127121 > reduce\n","2021-11-10 00:08:20,451 INFO streaming.PipeMapRed: R/W/S=40200000/40199422/0 in:10912=40200000/3684 [rec/s] out:10911=40199422/3684 [rec/s]\n","2021-11-10 00:08:21,869 INFO streaming.PipeMapRed: Records R/W=40211073/40210677\n","2021-11-10 00:08:25,731 INFO mapred.LocalJobRunner: Records R/W=40211073/40210677 > reduce\n","2021-11-10 00:08:31,732 INFO mapred.LocalJobRunner: Records R/W=40211073/40210677 > reduce\n","2021-11-10 00:08:31,873 INFO streaming.PipeMapRed: Records R/W=40290479/40289963\n","2021-11-10 00:08:33,123 INFO streaming.PipeMapRed: R/W/S=40300000/40299815/0 in:10900=40300000/3697 [rec/s] out:10900=40299815/3697 [rec/s]\n","2021-11-10 00:08:37,732 INFO mapred.LocalJobRunner: Records R/W=40290479/40289963 > reduce\n","2021-11-10 00:08:39,835 INFO streaming.PipeMapRed: R/W/S=40400000/40399675/0 in:10907=40400000/3704 [rec/s] out:10907=40399675/3704 [rec/s]\n","2021-11-10 00:08:41,879 INFO streaming.PipeMapRed: Records R/W=40415821/40415399\n","2021-11-10 00:08:43,733 INFO mapred.LocalJobRunner: Records R/W=40415821/40415399 > reduce\n","2021-11-10 00:08:44,397 INFO mapreduce.Job:  map 100% reduce 92%\n","2021-11-10 00:08:49,733 INFO mapred.LocalJobRunner: Records R/W=40415821/40415399 > reduce\n","2021-11-10 00:08:51,881 INFO streaming.PipeMapRed: Records R/W=40498110/40497643\n","2021-11-10 00:08:52,083 INFO streaming.PipeMapRed: R/W/S=40500000/40499489/0 in:10898=40500000/3716 [rec/s] out:10898=40499489/3716 [rec/s]\n","2021-11-10 00:08:55,733 INFO mapred.LocalJobRunner: Records R/W=40498110/40497643 > reduce\n","2021-11-10 00:09:01,734 INFO mapred.LocalJobRunner: Records R/W=40498110/40497643 > reduce\n","2021-11-10 00:09:01,887 INFO streaming.PipeMapRed: Records R/W=40585589/40585213\n","2021-11-10 00:09:03,274 INFO streaming.PipeMapRed: R/W/S=40600000/40599423/0 in:10893=40600000/3727 [rec/s] out:10893=40599423/3727 [rec/s]\n","2021-11-10 00:09:07,734 INFO mapred.LocalJobRunner: Records R/W=40585589/40585213 > reduce\n","2021-11-10 00:09:10,536 INFO streaming.PipeMapRed: R/W/S=40700000/40699423/0 in:10899=40700000/3734 [rec/s] out:10899=40699423/3734 [rec/s]\n","2021-11-10 00:09:11,889 INFO streaming.PipeMapRed: Records R/W=40718005/40717338\n","2021-11-10 00:09:13,735 INFO mapred.LocalJobRunner: Records R/W=40718005/40717338 > reduce\n","2021-11-10 00:09:18,003 INFO streaming.PipeMapRed: R/W/S=40800000/40799500/0 in:10903=40800000/3742 [rec/s] out:10903=40799500/3742 [rec/s]\n","2021-11-10 00:09:19,735 INFO mapred.LocalJobRunner: Records R/W=40718005/40717338 > reduce\n","2021-11-10 00:09:21,891 INFO streaming.PipeMapRed: Records R/W=40851773/40851156\n","2021-11-10 00:09:25,478 INFO streaming.PipeMapRed: R/W/S=40900000/40899382/0 in:10909=40900000/3749 [rec/s] out:10909=40899382/3749 [rec/s]\n","2021-11-10 00:09:25,736 INFO mapred.LocalJobRunner: Records R/W=40851773/40851156 > reduce\n","2021-11-10 00:09:31,736 INFO mapred.LocalJobRunner: Records R/W=40851773/40851156 > reduce\n","2021-11-10 00:09:31,892 INFO streaming.PipeMapRed: Records R/W=40986415/40985587\n","2021-11-10 00:09:32,901 INFO streaming.PipeMapRed: R/W/S=41000000/40999168/0 in:10912=41000000/3757 [rec/s] out:10912=40999168/3757 [rec/s]\n","2021-11-10 00:09:37,737 INFO mapred.LocalJobRunner: Records R/W=40986415/40985587 > reduce\n","2021-11-10 00:09:38,496 INFO streaming.PipeMapRed: R/W/S=41100000/41099011/0 in:10925=41100000/3762 [rec/s] out:10924=41099011/3762 [rec/s]\n","2021-11-10 00:09:41,896 INFO streaming.PipeMapRed: Records R/W=41163307/41162850\n","2021-11-10 00:09:43,737 INFO mapred.LocalJobRunner: Records R/W=41163307/41162850 > reduce\n","2021-11-10 00:09:44,567 INFO streaming.PipeMapRed: R/W/S=41200000/41199672/0 in:10934=41200000/3768 [rec/s] out:10934=41199672/3768 [rec/s]\n","2021-11-10 00:09:49,738 INFO mapred.LocalJobRunner: Records R/W=41163307/41162850 > reduce\n","2021-11-10 00:09:51,899 INFO streaming.PipeMapRed: Records R/W=41297337/41296764\n","2021-11-10 00:09:52,124 INFO streaming.PipeMapRed: R/W/S=41300000/41299379/0 in:10937=41300000/3776 [rec/s] out:10937=41299379/3776 [rec/s]\n","2021-11-10 00:09:55,738 INFO mapred.LocalJobRunner: Records R/W=41297337/41296764 > reduce\n","2021-11-10 00:09:59,900 INFO streaming.PipeMapRed: R/W/S=41400000/41399325/0 in:10940=41400000/3784 [rec/s] out:10940=41399325/3784 [rec/s]\n","2021-11-10 00:10:01,739 INFO mapred.LocalJobRunner: Records R/W=41297337/41296764 > reduce\n","2021-11-10 00:10:01,903 INFO streaming.PipeMapRed: Records R/W=41427794/41427005\n","2021-11-10 00:10:07,328 INFO streaming.PipeMapRed: R/W/S=41500000/41499300/0 in:10946=41500000/3791 [rec/s] out:10946=41499300/3791 [rec/s]\n","2021-11-10 00:10:07,739 INFO mapred.LocalJobRunner: Records R/W=41427794/41427005 > reduce\n","2021-11-10 00:10:11,906 INFO streaming.PipeMapRed: Records R/W=41560528/41559927\n","2021-11-10 00:10:13,740 INFO mapred.LocalJobRunner: Records R/W=41560528/41559927 > reduce\n","2021-11-10 00:10:15,916 INFO streaming.PipeMapRed: R/W/S=41600000/41599704/0 in:10947=41600000/3800 [rec/s] out:10947=41599704/3800 [rec/s]\n","2021-11-10 00:10:19,740 INFO mapred.LocalJobRunner: Records R/W=41560528/41559927 > reduce\n","2021-11-10 00:10:21,910 INFO streaming.PipeMapRed: Records R/W=41664965/41664553\n","2021-11-10 00:10:24,925 INFO streaming.PipeMapRed: R/W/S=41700000/41699382/0 in:10947=41700000/3809 [rec/s] out:10947=41699382/3809 [rec/s]\n","2021-11-10 00:10:25,741 INFO mapred.LocalJobRunner: Records R/W=41664965/41664553 > reduce\n","2021-11-10 00:10:31,741 INFO mapred.LocalJobRunner: Records R/W=41664965/41664553 > reduce\n","2021-11-10 00:10:31,915 INFO streaming.PipeMapRed: Records R/W=41773312/41772422\n","2021-11-10 00:10:33,328 INFO streaming.PipeMapRed: R/W/S=41800000/41799569/0 in:10951=41800000/3817 [rec/s] out:10950=41799569/3817 [rec/s]\n","2021-11-10 00:10:37,742 INFO mapred.LocalJobRunner: Records R/W=41773312/41772422 > reduce\n","2021-11-10 00:10:41,916 INFO streaming.PipeMapRed: Records R/W=41878343/41877991\n","2021-11-10 00:10:43,742 INFO mapred.LocalJobRunner: Records R/W=41878343/41877991 > reduce\n","2021-11-10 00:10:44,493 INFO streaming.PipeMapRed: R/W/S=41900000/41899367/0 in:10945=41900000/3828 [rec/s] out:10945=41899367/3828 [rec/s]\n","2021-11-10 00:10:49,742 INFO mapred.LocalJobRunner: Records R/W=41878343/41877991 > reduce\n","2021-11-10 00:10:51,921 INFO streaming.PipeMapRed: Records R/W=41974753/41974356\n","2021-11-10 00:10:55,016 INFO streaming.PipeMapRed: R/W/S=42000000/41999567/0 in:10940=42000000/3839 [rec/s] out:10940=41999567/3839 [rec/s]\n","2021-11-10 00:10:55,743 INFO mapred.LocalJobRunner: Records R/W=41974753/41974356 > reduce\n","2021-11-10 00:11:01,743 INFO mapred.LocalJobRunner: Records R/W=41974753/41974356 > reduce\n","2021-11-10 00:11:01,923 INFO streaming.PipeMapRed: Records R/W=42077825/42077291\n","2021-11-10 00:11:03,734 INFO streaming.PipeMapRed: R/W/S=42100000/42099748/0 in:10940=42100000/3848 [rec/s] out:10940=42099748/3848 [rec/s]\n","2021-11-10 00:11:07,744 INFO mapred.LocalJobRunner: Records R/W=42077825/42077291 > reduce\n","2021-11-10 00:11:11,924 INFO streaming.PipeMapRed: Records R/W=42189373/42188973\n","2021-11-10 00:11:12,870 INFO streaming.PipeMapRed: R/W/S=42200000/42199377/0 in:10941=42200000/3857 [rec/s] out:10940=42199377/3857 [rec/s]\n","2021-11-10 00:11:13,744 INFO mapred.LocalJobRunner: Records R/W=42189373/42188973 > reduce\n","2021-11-10 00:11:14,437 INFO mapreduce.Job:  map 100% reduce 93%\n","2021-11-10 00:11:19,744 INFO mapred.LocalJobRunner: Records R/W=42189373/42188973 > reduce\n","2021-11-10 00:11:21,925 INFO streaming.PipeMapRed: Records R/W=42280591/42279759\n","2021-11-10 00:11:23,906 INFO streaming.PipeMapRed: R/W/S=42300000/42299536/0 in:10935=42300000/3868 [rec/s] out:10935=42299536/3868 [rec/s]\n","2021-11-10 00:11:25,745 INFO mapred.LocalJobRunner: Records R/W=42280591/42279759 > reduce\n","2021-11-10 00:11:31,745 INFO mapred.LocalJobRunner: Records R/W=42280591/42279759 > reduce\n","2021-11-10 00:11:31,928 INFO streaming.PipeMapRed: Records R/W=42382130/42381487\n","2021-11-10 00:11:33,438 INFO streaming.PipeMapRed: R/W/S=42400000/42399700/0 in:10936=42400000/3877 [rec/s] out:10936=42399700/3877 [rec/s]\n","2021-11-10 00:11:37,746 INFO mapred.LocalJobRunner: Records R/W=42382130/42381487 > reduce\n","2021-11-10 00:11:41,239 INFO streaming.PipeMapRed: R/W/S=42500000/42499715/0 in:10939=42500000/3885 [rec/s] out:10939=42499715/3885 [rec/s]\n","2021-11-10 00:11:41,931 INFO streaming.PipeMapRed: Records R/W=42509219/42508759\n","2021-11-10 00:11:43,746 INFO mapred.LocalJobRunner: Records R/W=42509219/42508759 > reduce\n","2021-11-10 00:11:49,223 INFO streaming.PipeMapRed: R/W/S=42600000/42599530/0 in:10942=42600000/3893 [rec/s] out:10942=42599530/3893 [rec/s]\n","2021-11-10 00:11:49,746 INFO mapred.LocalJobRunner: Records R/W=42509219/42508759 > reduce\n","2021-11-10 00:11:51,934 INFO streaming.PipeMapRed: Records R/W=42634332/42633880\n","2021-11-10 00:11:55,747 INFO mapred.LocalJobRunner: Records R/W=42634332/42633880 > reduce\n","2021-11-10 00:11:55,778 INFO streaming.PipeMapRed: R/W/S=42700000/42699321/0 in:10948=42700000/3900 [rec/s] out:10948=42699321/3900 [rec/s]\n","2021-11-10 00:12:01,747 INFO mapred.LocalJobRunner: Records R/W=42634332/42633880 > reduce\n","2021-11-10 00:12:01,937 INFO streaming.PipeMapRed: Records R/W=42776791/42776267\n","2021-11-10 00:12:03,757 INFO streaming.PipeMapRed: R/W/S=42800000/42799354/0 in:10951=42800000/3908 [rec/s] out:10951=42799354/3908 [rec/s]\n","2021-11-10 00:12:07,748 INFO mapred.LocalJobRunner: Records R/W=42776791/42776267 > reduce\n","2021-11-10 00:12:11,837 INFO streaming.PipeMapRed: R/W/S=42900000/42899424/0 in:10955=42900000/3916 [rec/s] out:10954=42899424/3916 [rec/s]\n","2021-11-10 00:12:11,939 INFO streaming.PipeMapRed: Records R/W=42901612/42900734\n","2021-11-10 00:12:13,748 INFO mapred.LocalJobRunner: Records R/W=42901612/42900734 > reduce\n","2021-11-10 00:12:19,749 INFO mapred.LocalJobRunner: Records R/W=42901612/42900734 > reduce\n","2021-11-10 00:12:21,943 INFO streaming.PipeMapRed: Records R/W=42976173/42975922\n","2021-11-10 00:12:23,431 INFO streaming.PipeMapRed: R/W/S=43000000/42999514/0 in:10949=43000000/3927 [rec/s] out:10949=42999514/3927 [rec/s]\n","2021-11-10 00:12:25,749 INFO mapred.LocalJobRunner: Records R/W=42976173/42975922 > reduce\n","2021-11-10 00:12:31,749 INFO mapred.LocalJobRunner: Records R/W=42976173/42975922 > reduce\n","2021-11-10 00:12:31,945 INFO streaming.PipeMapRed: Records R/W=43051810/43051328\n","2021-11-10 00:12:37,563 INFO streaming.PipeMapRed: R/W/S=43100000/43099752/0 in:10936=43100000/3941 [rec/s] out:10936=43099752/3941 [rec/s]\n","2021-11-10 00:12:37,750 INFO mapred.LocalJobRunner: Records R/W=43051810/43051328 > reduce\n","2021-11-10 00:12:41,949 INFO streaming.PipeMapRed: Records R/W=43121791/43121566\n","2021-11-10 00:12:43,750 INFO mapred.LocalJobRunner: Records R/W=43121791/43121566 > reduce\n","2021-11-10 00:12:49,751 INFO mapred.LocalJobRunner: Records R/W=43121791/43121566 > reduce\n","2021-11-10 00:12:51,529 INFO streaming.PipeMapRed: R/W/S=43200000/43199752/0 in:10922=43200000/3955 [rec/s] out:10922=43199752/3955 [rec/s]\n","2021-11-10 00:12:51,951 INFO streaming.PipeMapRed: Records R/W=43207825/43207212\n","2021-11-10 00:12:55,753 INFO mapred.LocalJobRunner: Records R/W=43207825/43207212 > reduce\n","2021-11-10 00:12:57,882 INFO streaming.PipeMapRed: R/W/S=43300000/43299557/0 in:10928=43300000/3962 [rec/s] out:10928=43299557/3962 [rec/s]\n","2021-11-10 00:13:01,753 INFO mapred.LocalJobRunner: Records R/W=43207825/43207212 > reduce\n","2021-11-10 00:13:01,954 INFO streaming.PipeMapRed: Records R/W=43356580/43355836\n","2021-11-10 00:13:05,087 INFO streaming.PipeMapRed: R/W/S=43400000/43399591/0 in:10934=43400000/3969 [rec/s] out:10934=43399591/3969 [rec/s]\n","2021-11-10 00:13:07,754 INFO mapred.LocalJobRunner: Records R/W=43356580/43355836 > reduce\n","2021-11-10 00:13:11,956 INFO streaming.PipeMapRed: Records R/W=43498245/43497765\n","2021-11-10 00:13:12,070 INFO streaming.PipeMapRed: R/W/S=43500000/43499355/0 in:10940=43500000/3976 [rec/s] out:10940=43499355/3976 [rec/s]\n","2021-11-10 00:13:13,754 INFO mapred.LocalJobRunner: Records R/W=43498245/43497765 > reduce\n","2021-11-10 00:13:19,341 INFO streaming.PipeMapRed: R/W/S=43600000/43599608/0 in:10946=43600000/3983 [rec/s] out:10946=43599608/3983 [rec/s]\n","2021-11-10 00:13:19,755 INFO mapred.LocalJobRunner: Records R/W=43498245/43497765 > reduce\n","2021-11-10 00:13:21,958 INFO streaming.PipeMapRed: Records R/W=43678198/43677559\n","2021-11-10 00:13:23,588 INFO streaming.PipeMapRed: R/W/S=43700000/43699639/0 in:10957=43700000/3988 [rec/s] out:10957=43699639/3988 [rec/s]\n","2021-11-10 00:13:25,755 INFO mapred.LocalJobRunner: Records R/W=43678198/43677559 > reduce\n","2021-11-10 00:13:30,619 INFO streaming.PipeMapRed: R/W/S=43800000/43799197/0 in:10963=43800000/3995 [rec/s] out:10963=43799197/3995 [rec/s]\n","2021-11-10 00:13:31,756 INFO mapred.LocalJobRunner: Records R/W=43678198/43677559 > reduce\n","2021-11-10 00:13:31,959 INFO streaming.PipeMapRed: Records R/W=43817435/43816974\n","2021-11-10 00:13:37,761 INFO mapred.LocalJobRunner: Records R/W=43817435/43816974 > reduce\n","2021-11-10 00:13:37,889 INFO streaming.PipeMapRed: R/W/S=43900000/43899623/0 in:10969=43900000/4002 [rec/s] out:10969=43899623/4002 [rec/s]\n","2021-11-10 00:13:38,476 INFO mapreduce.Job:  map 100% reduce 94%\n","2021-11-10 00:13:41,960 INFO streaming.PipeMapRed: Records R/W=43956675/43956154\n","2021-11-10 00:13:43,761 INFO mapred.LocalJobRunner: Records R/W=43956675/43956154 > reduce\n","2021-11-10 00:13:45,141 INFO streaming.PipeMapRed: R/W/S=44000000/43999168/0 in:10975=44000000/4009 [rec/s] out:10975=43999168/4009 [rec/s]\n","2021-11-10 00:13:49,762 INFO mapred.LocalJobRunner: Records R/W=43956675/43956154 > reduce\n","2021-11-10 00:13:51,967 INFO streaming.PipeMapRed: Records R/W=44081752/44081330\n","2021-11-10 00:13:53,697 INFO streaming.PipeMapRed: R/W/S=44100000/44099646/0 in:10975=44100000/4018 [rec/s] out:10975=44099646/4018 [rec/s]\n","2021-11-10 00:13:55,762 INFO mapred.LocalJobRunner: Records R/W=44081752/44081330 > reduce\n","2021-11-10 00:14:01,763 INFO mapred.LocalJobRunner: Records R/W=44081752/44081330 > reduce\n","2021-11-10 00:14:01,970 INFO streaming.PipeMapRed: Records R/W=44163225/44162965\n","2021-11-10 00:14:05,026 INFO streaming.PipeMapRed: R/W/S=44200000/44199738/0 in:10970=44200000/4029 [rec/s] out:10970=44199738/4029 [rec/s]\n","2021-11-10 00:14:07,764 INFO mapred.LocalJobRunner: Records R/W=44163225/44162965 > reduce\n","2021-11-10 00:14:11,980 INFO streaming.PipeMapRed: Records R/W=44241665/44241349\n","2021-11-10 00:14:13,765 INFO mapred.LocalJobRunner: Records R/W=44241665/44241349 > reduce\n","2021-11-10 00:14:16,957 INFO streaming.PipeMapRed: R/W/S=44300000/44299240/0 in:10962=44300000/4041 [rec/s] out:10962=44299240/4041 [rec/s]\n","2021-11-10 00:14:19,765 INFO mapred.LocalJobRunner: Records R/W=44241665/44241349 > reduce\n","2021-11-10 00:14:21,983 INFO streaming.PipeMapRed: Records R/W=44376542/44375911\n","2021-11-10 00:14:23,236 INFO streaming.PipeMapRed: R/W/S=44400000/44399426/0 in:10971=44400000/4047 [rec/s] out:10970=44399426/4047 [rec/s]\n","2021-11-10 00:14:25,765 INFO mapred.LocalJobRunner: Records R/W=44376542/44375911 > reduce\n","2021-11-10 00:14:29,965 INFO streaming.PipeMapRed: R/W/S=44500000/44499313/0 in:10976=44500000/4054 [rec/s] out:10976=44499313/4054 [rec/s]\n","2021-11-10 00:14:31,768 INFO mapred.LocalJobRunner: Records R/W=44376542/44375911 > reduce\n","2021-11-10 00:14:31,984 INFO streaming.PipeMapRed: Records R/W=44531365/44530714\n","2021-11-10 00:14:37,768 INFO mapred.LocalJobRunner: Records R/W=44531365/44530714 > reduce\n","2021-11-10 00:14:41,409 INFO streaming.PipeMapRed: R/W/S=44600000/44599648/0 in:10971=44600000/4065 [rec/s] out:10971=44599648/4065 [rec/s]\n","2021-11-10 00:14:41,993 INFO streaming.PipeMapRed: Records R/W=44604068/44603623\n","2021-11-10 00:14:43,769 INFO mapred.LocalJobRunner: Records R/W=44604068/44603623 > reduce\n","2021-11-10 00:14:49,769 INFO mapred.LocalJobRunner: Records R/W=44604068/44603623 > reduce\n","2021-11-10 00:14:51,998 INFO streaming.PipeMapRed: Records R/W=44689706/44689278\n","2021-11-10 00:14:53,346 INFO streaming.PipeMapRed: R/W/S=44700000/44699452/0 in:10963=44700000/4077 [rec/s] out:10963=44699452/4077 [rec/s]\n","2021-11-10 00:14:55,770 INFO mapred.LocalJobRunner: Records R/W=44689706/44689278 > reduce\n","2021-11-10 00:15:01,770 INFO mapred.LocalJobRunner: Records R/W=44689706/44689278 > reduce\n","2021-11-10 00:15:02,011 INFO streaming.PipeMapRed: Records R/W=44770786/44770448\n","2021-11-10 00:15:05,110 INFO streaming.PipeMapRed: R/W/S=44800000/44799263/0 in:10956=44800000/4089 [rec/s] out:10956=44799263/4089 [rec/s]\n","2021-11-10 00:15:07,771 INFO mapred.LocalJobRunner: Records R/W=44770786/44770448 > reduce\n","2021-11-10 00:15:12,012 INFO streaming.PipeMapRed: Records R/W=44859868/44859362\n","2021-11-10 00:15:13,771 INFO mapred.LocalJobRunner: Records R/W=44859868/44859362 > reduce\n","2021-11-10 00:15:17,283 INFO streaming.PipeMapRed: R/W/S=44900000/44899496/0 in:10948=44900000/4101 [rec/s] out:10948=44899496/4101 [rec/s]\n","2021-11-10 00:15:19,772 INFO mapred.LocalJobRunner: Records R/W=44859868/44859362 > reduce\n","2021-11-10 00:15:22,015 INFO streaming.PipeMapRed: Records R/W=44941604/44941132\n","2021-11-10 00:15:25,772 INFO mapred.LocalJobRunner: Records R/W=44941604/44941132 > reduce\n","2021-11-10 00:15:27,290 INFO streaming.PipeMapRed: R/W/S=45000000/44999493/0 in:10946=45000000/4111 [rec/s] out:10946=44999493/4111 [rec/s]\n","2021-11-10 00:15:31,773 INFO mapred.LocalJobRunner: Records R/W=44941604/44941132 > reduce\n","2021-11-10 00:15:32,018 INFO streaming.PipeMapRed: Records R/W=45050838/45050379\n","2021-11-10 00:15:37,773 INFO mapred.LocalJobRunner: Records R/W=45050838/45050379 > reduce\n","2021-11-10 00:15:38,537 INFO streaming.PipeMapRed: R/W/S=45100000/45099659/0 in:10941=45100000/4122 [rec/s] out:10941=45099659/4122 [rec/s]\n","2021-11-10 00:15:42,019 INFO streaming.PipeMapRed: Records R/W=45119516/45119261\n","2021-11-10 00:15:43,774 INFO mapred.LocalJobRunner: Records R/W=45119516/45119261 > reduce\n","2021-11-10 00:15:46,560 INFO streaming.PipeMapRed: R/W/S=45200000/45199239/0 in:10944=45200000/4130 [rec/s] out:10944=45199239/4130 [rec/s]\n","2021-11-10 00:15:49,774 INFO mapred.LocalJobRunner: Records R/W=45119516/45119261 > reduce\n","2021-11-10 00:15:52,023 INFO streaming.PipeMapRed: Records R/W=45282437/45281809\n","2021-11-10 00:15:53,336 INFO streaming.PipeMapRed: R/W/S=45300000/45299408/0 in:10949=45300000/4137 [rec/s] out:10949=45299408/4137 [rec/s]\n","2021-11-10 00:15:55,775 INFO mapred.LocalJobRunner: Records R/W=45282437/45281809 > reduce\n","2021-11-10 00:16:01,343 INFO streaming.PipeMapRed: R/W/S=45400000/45399735/0 in:10952=45400000/4145 [rec/s] out:10952=45399735/4145 [rec/s]\n","2021-11-10 00:16:01,775 INFO mapred.LocalJobRunner: Records R/W=45282437/45281809 > reduce\n","2021-11-10 00:16:02,025 INFO streaming.PipeMapRed: Records R/W=45407722/45407158\n","2021-11-10 00:16:07,776 INFO mapred.LocalJobRunner: Records R/W=45407722/45407158 > reduce\n","2021-11-10 00:16:08,532 INFO mapreduce.Job:  map 100% reduce 95%\n","2021-11-10 00:16:09,287 INFO streaming.PipeMapRed: R/W/S=45500000/45499710/0 in:10955=45500000/4153 [rec/s] out:10955=45499710/4153 [rec/s]\n","2021-11-10 00:16:12,028 INFO streaming.PipeMapRed: Records R/W=45521977/45521763\n","2021-11-10 00:16:13,776 INFO mapred.LocalJobRunner: Records R/W=45521977/45521763 > reduce\n","2021-11-10 00:16:19,777 INFO mapred.LocalJobRunner: Records R/W=45521977/45521763 > reduce\n","2021-11-10 00:16:22,033 INFO streaming.PipeMapRed: Records R/W=45581566/45581381\n","2021-11-10 00:16:25,044 INFO streaming.PipeMapRed: R/W/S=45600000/45599701/0 in:10937=45600000/4169 [rec/s] out:10937=45599701/4169 [rec/s]\n","2021-11-10 00:16:25,777 INFO mapred.LocalJobRunner: Records R/W=45581566/45581381 > reduce\n","2021-11-10 00:16:31,778 INFO mapred.LocalJobRunner: Records R/W=45581566/45581381 > reduce\n","2021-11-10 00:16:32,043 INFO streaming.PipeMapRed: Records R/W=45639968/45639767\n","2021-11-10 00:16:37,608 INFO streaming.PipeMapRed: R/W/S=45700000/45699592/0 in:10927=45700000/4182 [rec/s] out:10927=45699592/4182 [rec/s]\n","2021-11-10 00:16:37,778 INFO mapred.LocalJobRunner: Records R/W=45639968/45639767 > reduce\n","2021-11-10 00:16:42,044 INFO streaming.PipeMapRed: Records R/W=45727446/45727200\n","2021-11-10 00:16:43,779 INFO mapred.LocalJobRunner: Records R/W=45727446/45727200 > reduce\n","2021-11-10 00:16:49,779 INFO mapred.LocalJobRunner: Records R/W=45727446/45727200 > reduce\n","2021-11-10 00:16:52,045 INFO streaming.PipeMapRed: Records R/W=45784931/45784649\n","2021-11-10 00:16:54,768 INFO streaming.PipeMapRed: R/W/S=45800000/45799766/0 in:10907=45800000/4199 [rec/s] out:10907=45799766/4199 [rec/s]\n","2021-11-10 00:16:55,780 INFO mapred.LocalJobRunner: Records R/W=45784931/45784649 > reduce\n","2021-11-10 00:17:01,780 INFO mapred.LocalJobRunner: Records R/W=45784931/45784649 > reduce\n","2021-11-10 00:17:02,046 INFO streaming.PipeMapRed: Records R/W=45869176/45868863\n","2021-11-10 00:17:04,862 INFO streaming.PipeMapRed: R/W/S=45900000/45899500/0 in:10905=45900000/4209 [rec/s] out:10905=45899500/4209 [rec/s]\n","2021-11-10 00:17:07,780 INFO mapred.LocalJobRunner: Records R/W=45869176/45868863 > reduce\n","2021-11-10 00:17:12,065 INFO streaming.PipeMapRed: Records R/W=45970454/45970112\n","2021-11-10 00:17:13,781 INFO mapred.LocalJobRunner: Records R/W=45970454/45970112 > reduce\n","2021-11-10 00:17:15,230 INFO streaming.PipeMapRed: R/W/S=46000000/45999151/0 in:10903=46000000/4219 [rec/s] out:10902=45999151/4219 [rec/s]\n","2021-11-10 00:17:19,781 INFO mapred.LocalJobRunner: Records R/W=45970454/45970112 > reduce\n","2021-11-10 00:17:22,071 INFO streaming.PipeMapRed: Records R/W=46080558/46080290\n","2021-11-10 00:17:24,422 INFO streaming.PipeMapRed: R/W/S=46100000/46099337/0 in:10903=46100000/4228 [rec/s] out:10903=46099337/4228 [rec/s]\n","2021-11-10 00:17:25,782 INFO mapred.LocalJobRunner: Records R/W=46080558/46080290 > reduce\n","2021-11-10 00:17:31,782 INFO mapred.LocalJobRunner: Records R/W=46080558/46080290 > reduce\n","2021-11-10 00:17:32,078 INFO streaming.PipeMapRed: Records R/W=46185974/46185683\n","2021-11-10 00:17:33,353 INFO streaming.PipeMapRed: R/W/S=46200000/46199685/0 in:10903=46200000/4237 [rec/s] out:10903=46199685/4237 [rec/s]\n","2021-11-10 00:17:37,783 INFO mapred.LocalJobRunner: Records R/W=46185974/46185683 > reduce\n","2021-11-10 00:17:41,077 INFO streaming.PipeMapRed: R/W/S=46300000/46299743/0 in:10906=46300000/4245 [rec/s] out:10906=46299743/4245 [rec/s]\n","2021-11-10 00:17:42,080 INFO streaming.PipeMapRed: Records R/W=46310208/46309871\n","2021-11-10 00:17:43,783 INFO mapred.LocalJobRunner: Records R/W=46310208/46309871 > reduce\n","2021-11-10 00:17:49,784 INFO mapred.LocalJobRunner: Records R/W=46310208/46309871 > reduce\n","2021-11-10 00:17:50,026 INFO streaming.PipeMapRed: R/W/S=46400000/46399660/0 in:10907=46400000/4254 [rec/s] out:10907=46399660/4254 [rec/s]\n","2021-11-10 00:17:52,087 INFO streaming.PipeMapRed: Records R/W=46415461/46414969\n","2021-11-10 00:17:55,784 INFO mapred.LocalJobRunner: Records R/W=46415461/46414969 > reduce\n","2021-11-10 00:18:01,785 INFO mapred.LocalJobRunner: Records R/W=46415461/46414969 > reduce\n","2021-11-10 00:18:02,091 INFO streaming.PipeMapRed: Records R/W=46499558/46499135\n","2021-11-10 00:18:02,154 INFO streaming.PipeMapRed: R/W/S=46500000/46499671/0 in:10900=46500000/4266 [rec/s] out:10900=46499671/4266 [rec/s]\n","2021-11-10 00:18:07,785 INFO mapred.LocalJobRunner: Records R/W=46499558/46499135 > reduce\n","2021-11-10 00:18:12,092 INFO streaming.PipeMapRed: Records R/W=46572103/46571699\n","2021-11-10 00:18:13,786 INFO mapred.LocalJobRunner: Records R/W=46572103/46571699 > reduce\n","2021-11-10 00:18:14,740 INFO streaming.PipeMapRed: R/W/S=46600000/46599579/0 in:10890=46600000/4279 [rec/s] out:10890=46599579/4279 [rec/s]\n","2021-11-10 00:18:19,786 INFO mapred.LocalJobRunner: Records R/W=46572103/46571699 > reduce\n","2021-11-10 00:18:22,100 INFO streaming.PipeMapRed: Records R/W=46654077/46653669\n","2021-11-10 00:18:25,786 INFO mapred.LocalJobRunner: Records R/W=46654077/46653669 > reduce\n","2021-11-10 00:18:28,134 INFO streaming.PipeMapRed: R/W/S=46700000/46699646/0 in:10880=46700000/4292 [rec/s] out:10880=46699646/4292 [rec/s]\n","2021-11-10 00:18:31,787 INFO mapred.LocalJobRunner: Records R/W=46654077/46653669 > reduce\n","2021-11-10 00:18:32,107 INFO streaming.PipeMapRed: Records R/W=46737351/46737033\n","2021-11-10 00:18:37,787 INFO mapred.LocalJobRunner: Records R/W=46737351/46737033 > reduce\n","2021-11-10 00:18:38,312 INFO streaming.PipeMapRed: R/W/S=46800000/46799450/0 in:10878=46800000/4302 [rec/s] out:10878=46799450/4302 [rec/s]\n","2021-11-10 00:18:42,111 INFO streaming.PipeMapRed: Records R/W=46850608/46850325\n","2021-11-10 00:18:43,788 INFO mapred.LocalJobRunner: Records R/W=46850608/46850325 > reduce\n","2021-11-10 00:18:44,582 INFO mapreduce.Job:  map 100% reduce 96%\n","2021-11-10 00:18:46,945 INFO streaming.PipeMapRed: R/W/S=46900000/46899392/0 in:10879=46900000/4311 [rec/s] out:10879=46899392/4311 [rec/s]\n","2021-11-10 00:18:49,788 INFO mapred.LocalJobRunner: Records R/W=46850608/46850325 > reduce\n","2021-11-10 00:18:52,114 INFO streaming.PipeMapRed: Records R/W=46946082/46945703\n","2021-11-10 00:18:55,788 INFO mapred.LocalJobRunner: Records R/W=46946082/46945703 > reduce\n","2021-11-10 00:18:57,184 INFO streaming.PipeMapRed: R/W/S=47000000/46999404/0 in:10877=47000000/4321 [rec/s] out:10876=46999404/4321 [rec/s]\n","2021-11-10 00:19:01,789 INFO mapred.LocalJobRunner: Records R/W=46946082/46945703 > reduce\n","2021-11-10 00:19:02,116 INFO streaming.PipeMapRed: Records R/W=47053733/47053081\n","2021-11-10 00:19:06,848 INFO streaming.PipeMapRed: R/W/S=47100000/47099685/0 in:10875=47100000/4331 [rec/s] out:10875=47099685/4331 [rec/s]\n","2021-11-10 00:19:07,789 INFO mapred.LocalJobRunner: Records R/W=47053733/47053081 > reduce\n","2021-11-10 00:19:12,117 INFO streaming.PipeMapRed: Records R/W=47156179/47155852\n","2021-11-10 00:19:13,790 INFO mapred.LocalJobRunner: Records R/W=47156179/47155852 > reduce\n","2021-11-10 00:19:15,630 INFO streaming.PipeMapRed: R/W/S=47200000/47199619/0 in:10875=47200000/4340 [rec/s] out:10875=47199619/4340 [rec/s]\n","2021-11-10 00:19:19,790 INFO mapred.LocalJobRunner: Records R/W=47156179/47155852 > reduce\n","2021-11-10 00:19:22,121 INFO streaming.PipeMapRed: Records R/W=47273337/47272627\n","2021-11-10 00:19:24,137 INFO streaming.PipeMapRed: R/W/S=47300000/47299636/0 in:10878=47300000/4348 [rec/s] out:10878=47299636/4348 [rec/s]\n","2021-11-10 00:19:25,790 INFO mapred.LocalJobRunner: Records R/W=47273337/47272627 > reduce\n","2021-11-10 00:19:31,539 INFO streaming.PipeMapRed: R/W/S=47400000/47399204/0 in:10884=47400000/4355 [rec/s] out:10883=47399204/4355 [rec/s]\n","2021-11-10 00:19:31,791 INFO mapred.LocalJobRunner: Records R/W=47273337/47272627 > reduce\n","2021-11-10 00:19:32,125 INFO streaming.PipeMapRed: Records R/W=47407643/47406884\n","2021-11-10 00:19:37,595 INFO streaming.PipeMapRed: R/W/S=47500000/47499689/0 in:10889=47500000/4362 [rec/s] out:10889=47499689/4362 [rec/s]\n","2021-11-10 00:19:37,791 INFO mapred.LocalJobRunner: Records R/W=47407643/47406884 > reduce\n","2021-11-10 00:19:42,131 INFO streaming.PipeMapRed: Records R/W=47557717/47557338\n","2021-11-10 00:19:43,792 INFO mapred.LocalJobRunner: Records R/W=47557717/47557338 > reduce\n","2021-11-10 00:19:45,335 INFO streaming.PipeMapRed: R/W/S=47600000/47599554/0 in:10894=47600000/4369 [rec/s] out:10894=47599554/4369 [rec/s]\n","2021-11-10 00:19:49,792 INFO mapred.LocalJobRunner: Records R/W=47557717/47557338 > reduce\n","2021-11-10 00:19:52,139 INFO streaming.PipeMapRed: Records R/W=47674325/47673841\n","2021-11-10 00:19:55,572 INFO streaming.PipeMapRed: R/W/S=47700000/47699214/0 in:10890=47700000/4380 [rec/s] out:10890=47699214/4380 [rec/s]\n","2021-11-10 00:19:55,792 INFO mapred.LocalJobRunner: Records R/W=47674325/47673841 > reduce\n","2021-11-10 00:20:01,793 INFO mapred.LocalJobRunner: Records R/W=47674325/47673841 > reduce\n","2021-11-10 00:20:02,143 INFO streaming.PipeMapRed: Records R/W=47768747/47768382\n","2021-11-10 00:20:05,793 INFO streaming.PipeMapRed: R/W/S=47800000/47799556/0 in:10888=47800000/4390 [rec/s] out:10888=47799556/4390 [rec/s]\n","2021-11-10 00:20:07,793 INFO mapred.LocalJobRunner: Records R/W=47768747/47768382 > reduce\n","2021-11-10 00:20:12,147 INFO streaming.PipeMapRed: Records R/W=47861489/47861152\n","2021-11-10 00:20:13,794 INFO mapred.LocalJobRunner: Records R/W=47861489/47861152 > reduce\n","2021-11-10 00:20:16,669 INFO streaming.PipeMapRed: R/W/S=47900000/47899674/0 in:10883=47900000/4401 [rec/s] out:10883=47899674/4401 [rec/s]\n","2021-11-10 00:20:19,794 INFO mapred.LocalJobRunner: Records R/W=47861489/47861152 > reduce\n","2021-11-10 00:20:22,148 INFO streaming.PipeMapRed: Records R/W=47964643/47964049\n","2021-11-10 00:20:25,795 INFO mapred.LocalJobRunner: Records R/W=47964643/47964049 > reduce\n","2021-11-10 00:20:26,113 INFO streaming.PipeMapRed: R/W/S=48000000/47999576/0 in:10884=48000000/4410 [rec/s] out:10884=47999576/4410 [rec/s]\n","2021-11-10 00:20:31,795 INFO mapred.LocalJobRunner: Records R/W=47964643/47964049 > reduce\n","2021-11-10 00:20:32,149 INFO streaming.PipeMapRed: Records R/W=48059225/48058719\n","2021-11-10 00:20:35,650 INFO streaming.PipeMapRed: R/W/S=48100000/48099502/0 in:10882=48100000/4420 [rec/s] out:10882=48099502/4420 [rec/s]\n","2021-11-10 00:20:37,796 INFO mapred.LocalJobRunner: Records R/W=48059225/48058719 > reduce\n","2021-11-10 00:20:42,150 INFO streaming.PipeMapRed: Records R/W=48158556/48158066\n","2021-11-10 00:20:43,796 INFO mapred.LocalJobRunner: Records R/W=48158556/48158066 > reduce\n","2021-11-10 00:20:45,951 INFO streaming.PipeMapRed: R/W/S=48200000/48199570/0 in:10880=48200000/4430 [rec/s] out:10880=48199570/4430 [rec/s]\n","2021-11-10 00:20:49,797 INFO mapred.LocalJobRunner: Records R/W=48158556/48158066 > reduce\n","2021-11-10 00:20:52,153 INFO streaming.PipeMapRed: Records R/W=48250423/48250031\n","2021-11-10 00:20:55,797 INFO mapred.LocalJobRunner: Records R/W=48250423/48250031 > reduce\n","2021-11-10 00:20:56,197 INFO streaming.PipeMapRed: R/W/S=48300000/48299422/0 in:10878=48300000/4440 [rec/s] out:10878=48299422/4440 [rec/s]\n","2021-11-10 00:21:01,798 INFO mapred.LocalJobRunner: Records R/W=48250423/48250031 > reduce\n","2021-11-10 00:21:02,164 INFO streaming.PipeMapRed: Records R/W=48344171/48343844\n","2021-11-10 00:21:07,799 INFO mapred.LocalJobRunner: Records R/W=48344171/48343844 > reduce\n","2021-11-10 00:21:11,899 INFO streaming.PipeMapRed: R/W/S=48400000/48399420/0 in:10861=48400000/4456 [rec/s] out:10861=48399420/4456 [rec/s]\n","2021-11-10 00:21:12,176 INFO streaming.PipeMapRed: Records R/W=48402283/48401969\n","2021-11-10 00:21:13,799 INFO mapred.LocalJobRunner: Records R/W=48402283/48401969 > reduce\n","2021-11-10 00:21:14,621 INFO mapreduce.Job:  map 100% reduce 97%\n","2021-11-10 00:21:19,800 INFO mapred.LocalJobRunner: Records R/W=48402283/48401969 > reduce\n","2021-11-10 00:21:21,093 INFO streaming.PipeMapRed: R/W/S=48500000/48499644/0 in:10862=48500000/4465 [rec/s] out:10862=48499644/4465 [rec/s]\n","2021-11-10 00:21:22,178 INFO streaming.PipeMapRed: Records R/W=48512957/48512553\n","2021-11-10 00:21:25,800 INFO mapred.LocalJobRunner: Records R/W=48512957/48512553 > reduce\n","2021-11-10 00:21:30,177 INFO streaming.PipeMapRed: R/W/S=48600000/48599495/0 in:10862=48600000/4474 [rec/s] out:10862=48599495/4474 [rec/s]\n","2021-11-10 00:21:31,801 INFO mapred.LocalJobRunner: Records R/W=48512957/48512553 > reduce\n","2021-11-10 00:21:32,184 INFO streaming.PipeMapRed: Records R/W=48617503/48616991\n","2021-11-10 00:21:37,801 INFO mapred.LocalJobRunner: Records R/W=48617503/48616991 > reduce\n","2021-11-10 00:21:40,880 INFO streaming.PipeMapRed: R/W/S=48700000/48699553/0 in:10858=48700000/4485 [rec/s] out:10858=48699553/4485 [rec/s]\n","2021-11-10 00:21:42,190 INFO streaming.PipeMapRed: Records R/W=48711673/48711262\n","2021-11-10 00:21:43,802 INFO mapred.LocalJobRunner: Records R/W=48711673/48711262 > reduce\n","2021-11-10 00:21:49,802 INFO mapred.LocalJobRunner: Records R/W=48711673/48711262 > reduce\n","2021-11-10 00:21:51,554 INFO streaming.PipeMapRed: R/W/S=48800000/48799533/0 in:10856=48800000/4495 [rec/s] out:10856=48799533/4495 [rec/s]\n","2021-11-10 00:21:52,192 INFO streaming.PipeMapRed: Records R/W=48805553/48804949\n","2021-11-10 00:21:55,802 INFO mapred.LocalJobRunner: Records R/W=48805553/48804949 > reduce\n","2021-11-10 00:22:01,803 INFO mapred.LocalJobRunner: Records R/W=48805553/48804949 > reduce\n","2021-11-10 00:22:02,196 INFO streaming.PipeMapRed: Records R/W=48894619/48894095\n","2021-11-10 00:22:02,835 INFO streaming.PipeMapRed: R/W/S=48900000/48899546/0 in:10849=48900000/4507 [rec/s] out:10849=48899546/4507 [rec/s]\n","2021-11-10 00:22:07,803 INFO mapred.LocalJobRunner: Records R/W=48894619/48894095 > reduce\n","2021-11-10 00:22:12,198 INFO streaming.PipeMapRed: Records R/W=48993736/48993381\n","2021-11-10 00:22:12,895 INFO streaming.PipeMapRed: R/W/S=49000000/48999596/0 in:10847=49000000/4517 [rec/s] out:10847=48999596/4517 [rec/s]\n","2021-11-10 00:22:13,804 INFO mapred.LocalJobRunner: Records R/W=48993736/48993381 > reduce\n","2021-11-10 00:22:19,804 INFO mapred.LocalJobRunner: Records R/W=48993736/48993381 > reduce\n","2021-11-10 00:22:21,991 INFO streaming.PipeMapRed: R/W/S=49100000/49099276/0 in:10848=49100000/4526 [rec/s] out:10848=49099276/4526 [rec/s]\n","2021-11-10 00:22:22,201 INFO streaming.PipeMapRed: Records R/W=49103421/49102688\n","2021-11-10 00:22:25,805 INFO mapred.LocalJobRunner: Records R/W=49103421/49102688 > reduce\n","2021-11-10 00:22:27,841 INFO streaming.PipeMapRed: R/W/S=49200000/49199198/0 in:10856=49200000/4532 [rec/s] out:10855=49199198/4532 [rec/s]\n","2021-11-10 00:22:31,805 INFO mapred.LocalJobRunner: Records R/W=49103421/49102688 > reduce\n","2021-11-10 00:22:32,204 INFO streaming.PipeMapRed: Records R/W=49255496/49255087\n","2021-11-10 00:22:35,562 INFO streaming.PipeMapRed: R/W/S=49300000/49299425/0 in:10861=49300000/4539 [rec/s] out:10861=49299425/4539 [rec/s]\n","2021-11-10 00:22:37,806 INFO mapred.LocalJobRunner: Records R/W=49255496/49255087 > reduce\n","2021-11-10 00:22:42,206 INFO streaming.PipeMapRed: Records R/W=49371368/49370980\n","2021-11-10 00:22:43,806 INFO mapred.LocalJobRunner: Records R/W=49371368/49370980 > reduce\n","2021-11-10 00:22:44,534 INFO streaming.PipeMapRed: R/W/S=49400000/49399691/0 in:10861=49400000/4548 [rec/s] out:10861=49399691/4548 [rec/s]\n","2021-11-10 00:22:49,807 INFO mapred.LocalJobRunner: Records R/W=49371368/49370980 > reduce\n","2021-11-10 00:22:52,211 INFO streaming.PipeMapRed: Records R/W=49495596/49494965\n","2021-11-10 00:22:52,576 INFO streaming.PipeMapRed: R/W/S=49500000/49499574/0 in:10862=49500000/4557 [rec/s] out:10862=49499574/4557 [rec/s]\n","2021-11-10 00:22:55,807 INFO mapred.LocalJobRunner: Records R/W=49495596/49494965 > reduce\n","2021-11-10 00:23:01,629 INFO streaming.PipeMapRed: R/W/S=49600000/49599220/0 in:10862=49600000/4566 [rec/s] out:10862=49599220/4566 [rec/s]\n","2021-11-10 00:23:01,808 INFO mapred.LocalJobRunner: Records R/W=49495596/49494965 > reduce\n","2021-11-10 00:23:02,214 INFO streaming.PipeMapRed: Records R/W=49607508/49606800\n","2021-11-10 00:23:07,808 INFO mapred.LocalJobRunner: Records R/W=49607508/49606800 > reduce\n","2021-11-10 00:23:08,367 INFO streaming.PipeMapRed: R/W/S=49700000/49698928/0 in:10870=49700000/4572 [rec/s] out:10870=49698928/4572 [rec/s]\n","2021-11-10 00:23:12,219 INFO streaming.PipeMapRed: Records R/W=49749098/49748609\n","2021-11-10 00:23:13,808 INFO mapred.LocalJobRunner: Records R/W=49749098/49748609 > reduce\n","2021-11-10 00:23:16,227 INFO streaming.PipeMapRed: R/W/S=49800000/49799681/0 in:10873=49800000/4580 [rec/s] out:10873=49799681/4580 [rec/s]\n","2021-11-10 00:23:19,809 INFO mapred.LocalJobRunner: Records R/W=49749098/49748609 > reduce\n","2021-11-10 00:23:22,220 INFO streaming.PipeMapRed: Records R/W=49879262/49878684\n","2021-11-10 00:23:24,135 INFO streaming.PipeMapRed: R/W/S=49900000/49899316/0 in:10876=49900000/4588 [rec/s] out:10876=49899316/4588 [rec/s]\n","2021-11-10 00:23:25,809 INFO mapred.LocalJobRunner: Records R/W=49879262/49878684 > reduce\n","2021-11-10 00:23:31,810 INFO mapred.LocalJobRunner: Records R/W=49879262/49878684 > reduce\n","2021-11-10 00:23:32,221 INFO streaming.PipeMapRed: Records R/W=49986391/49986000\n","2021-11-10 00:23:33,445 INFO streaming.PipeMapRed: R/W/S=50000000/49999346/0 in:10876=50000000/4597 [rec/s] out:10876=49999346/4597 [rec/s]\n","2021-11-10 00:23:37,810 INFO mapred.LocalJobRunner: Records R/W=49986391/49986000 > reduce\n","2021-11-10 00:23:38,687 INFO mapreduce.Job:  map 100% reduce 98%\n","2021-11-10 00:23:42,222 INFO streaming.PipeMapRed: Records R/W=50093232/50092826\n","2021-11-10 00:23:42,845 INFO streaming.PipeMapRed: R/W/S=50100000/50099626/0 in:10874=50100000/4607 [rec/s] out:10874=50099626/4607 [rec/s]\n","2021-11-10 00:23:43,811 INFO mapred.LocalJobRunner: Records R/W=50093232/50092826 > reduce\n","2021-11-10 00:23:49,260 INFO streaming.PipeMapRed: R/W/S=50200000/50199604/0 in:10882=50200000/4613 [rec/s] out:10882=50199604/4613 [rec/s]\n","2021-11-10 00:23:49,811 INFO mapred.LocalJobRunner: Records R/W=50093232/50092826 > reduce\n","2021-11-10 00:23:52,227 INFO streaming.PipeMapRed: Records R/W=50232298/50231714\n","2021-11-10 00:23:55,812 INFO mapred.LocalJobRunner: Records R/W=50232298/50231714 > reduce\n","2021-11-10 00:23:58,707 INFO streaming.PipeMapRed: R/W/S=50300000/50299424/0 in:10880=50300000/4623 [rec/s] out:10880=50299424/4623 [rec/s]\n","2021-11-10 00:24:01,812 INFO mapred.LocalJobRunner: Records R/W=50232298/50231714 > reduce\n","2021-11-10 00:24:02,229 INFO streaming.PipeMapRed: Records R/W=50337425/50336791\n","2021-11-10 00:24:07,813 INFO mapred.LocalJobRunner: Records R/W=50337425/50336791 > reduce\n","2021-11-10 00:24:08,304 INFO streaming.PipeMapRed: R/W/S=50400000/50399647/0 in:10880=50400000/4632 [rec/s] out:10880=50399647/4632 [rec/s]\n","2021-11-10 00:24:12,232 INFO streaming.PipeMapRed: Records R/W=50441794/50441099\n","2021-11-10 00:24:13,813 INFO mapred.LocalJobRunner: Records R/W=50441794/50441099 > reduce\n","2021-11-10 00:24:19,815 INFO mapred.LocalJobRunner: Records R/W=50441794/50441099 > reduce\n","2021-11-10 00:24:21,846 INFO streaming.PipeMapRed: R/W/S=50500000/50499616/0 in:10869=50500000/4646 [rec/s] out:10869=50499616/4646 [rec/s]\n","2021-11-10 00:24:22,251 INFO streaming.PipeMapRed: Records R/W=50500890/50500430\n","2021-11-10 00:24:25,815 INFO mapred.LocalJobRunner: Records R/W=50500890/50500430 > reduce\n","2021-11-10 00:24:31,816 INFO mapred.LocalJobRunner: Records R/W=50500890/50500430 > reduce\n","2021-11-10 00:24:32,255 INFO streaming.PipeMapRed: Records R/W=50598223/50597716\n","2021-11-10 00:24:32,436 INFO streaming.PipeMapRed: R/W/S=50600000/50599710/0 in:10867=50600000/4656 [rec/s] out:10867=50599710/4656 [rec/s]\n","2021-11-10 00:24:37,816 INFO mapred.LocalJobRunner: Records R/W=50598223/50597716 > reduce\n","2021-11-10 00:24:40,776 INFO streaming.PipeMapRed: R/W/S=50700000/50699319/0 in:10868=50700000/4665 [rec/s] out:10868=50699319/4665 [rec/s]\n","2021-11-10 00:24:42,260 INFO streaming.PipeMapRed: Records R/W=50717991/50717617\n","2021-11-10 00:24:43,817 INFO mapred.LocalJobRunner: Records R/W=50717991/50717617 > reduce\n","2021-11-10 00:24:48,264 INFO streaming.PipeMapRed: R/W/S=50800000/50799387/0 in:10873=50800000/4672 [rec/s] out:10873=50799387/4672 [rec/s]\n","2021-11-10 00:24:49,817 INFO mapred.LocalJobRunner: Records R/W=50717991/50717617 > reduce\n","2021-11-10 00:24:52,263 INFO streaming.PipeMapRed: Records R/W=50859379/50858847\n","2021-11-10 00:24:55,436 INFO streaming.PipeMapRed: R/W/S=50900000/50899353/0 in:10878=50900000/4679 [rec/s] out:10878=50899353/4679 [rec/s]\n","2021-11-10 00:24:55,817 INFO mapred.LocalJobRunner: Records R/W=50859379/50858847 > reduce\n","2021-11-10 00:25:01,820 INFO mapred.LocalJobRunner: Records R/W=50859379/50858847 > reduce\n","2021-11-10 00:25:02,267 INFO streaming.PipeMapRed: Records R/W=50952074/50951744\n","2021-11-10 00:25:06,412 INFO streaming.PipeMapRed: R/W/S=51000000/50999447/0 in:10874=51000000/4690 [rec/s] out:10874=50999447/4690 [rec/s]\n","2021-11-10 00:25:07,821 INFO mapred.LocalJobRunner: Records R/W=50952074/50951744 > reduce\n","2021-11-10 00:25:12,269 INFO streaming.PipeMapRed: Records R/W=51046836/51046470\n","2021-11-10 00:25:13,821 INFO mapred.LocalJobRunner: Records R/W=51046836/51046470 > reduce\n","2021-11-10 00:25:15,854 INFO streaming.PipeMapRed: R/W/S=51100000/51099286/0 in:10872=51100000/4700 [rec/s] out:10872=51099286/4700 [rec/s]\n","2021-11-10 00:25:19,822 INFO mapred.LocalJobRunner: Records R/W=51046836/51046470 > reduce\n","2021-11-10 00:25:22,274 INFO streaming.PipeMapRed: Records R/W=51160128/51159766\n","2021-11-10 00:25:25,822 INFO mapred.LocalJobRunner: Records R/W=51160128/51159766 > reduce\n","2021-11-10 00:25:26,636 INFO streaming.PipeMapRed: R/W/S=51200000/51199732/0 in:10868=51200000/4711 [rec/s] out:10868=51199732/4711 [rec/s]\n","2021-11-10 00:25:31,823 INFO mapred.LocalJobRunner: Records R/W=51160128/51159766 > reduce\n","2021-11-10 00:25:32,281 INFO streaming.PipeMapRed: Records R/W=51251427/51251065\n","2021-11-10 00:25:37,684 INFO streaming.PipeMapRed: R/W/S=51300000/51299566/0 in:10864=51300000/4722 [rec/s] out:10863=51299566/4722 [rec/s]\n","2021-11-10 00:25:37,823 INFO mapred.LocalJobRunner: Records R/W=51251427/51251065 > reduce\n","2021-11-10 00:25:42,283 INFO streaming.PipeMapRed: Records R/W=51337935/51337468\n","2021-11-10 00:25:43,823 INFO mapred.LocalJobRunner: Records R/W=51337935/51337468 > reduce\n","2021-11-10 00:25:49,303 INFO streaming.PipeMapRed: R/W/S=51400000/51399547/0 in:10859=51400000/4733 [rec/s] out:10859=51399547/4733 [rec/s]\n","2021-11-10 00:25:49,824 INFO mapred.LocalJobRunner: Records R/W=51337935/51337468 > reduce\n","2021-11-10 00:25:52,285 INFO streaming.PipeMapRed: Records R/W=51435617/51435236\n","2021-11-10 00:25:55,824 INFO mapred.LocalJobRunner: Records R/W=51435617/51435236 > reduce\n","2021-11-10 00:25:57,674 INFO streaming.PipeMapRed: R/W/S=51500000/51499484/0 in:10860=51500000/4742 [rec/s] out:10860=51499484/4742 [rec/s]\n","2021-11-10 00:26:01,826 INFO mapred.LocalJobRunner: Records R/W=51435617/51435236 > reduce\n","2021-11-10 00:26:02,288 INFO streaming.PipeMapRed: Records R/W=51554202/51553867\n","2021-11-10 00:26:06,229 INFO streaming.PipeMapRed: R/W/S=51600000/51599497/0 in:10863=51600000/4750 [rec/s] out:10863=51599497/4750 [rec/s]\n","2021-11-10 00:26:07,827 INFO mapred.LocalJobRunner: Records R/W=51554202/51553867 > reduce\n","2021-11-10 00:26:08,743 INFO mapreduce.Job:  map 100% reduce 99%\n","2021-11-10 00:26:11,826 INFO streaming.PipeMapRed: R/W/S=51700000/51699422/0 in:10870=51700000/4756 [rec/s] out:10870=51699422/4756 [rec/s]\n","2021-11-10 00:26:12,290 INFO streaming.PipeMapRed: Records R/W=51705228/51704669\n","2021-11-10 00:26:13,828 INFO mapred.LocalJobRunner: Records R/W=51705228/51704669 > reduce\n","2021-11-10 00:26:19,828 INFO mapred.LocalJobRunner: Records R/W=51705228/51704669 > reduce\n","2021-11-10 00:26:20,168 INFO streaming.PipeMapRed: R/W/S=51800000/51799563/0 in:10873=51800000/4764 [rec/s] out:10873=51799563/4764 [rec/s]\n","2021-11-10 00:26:22,293 INFO streaming.PipeMapRed: Records R/W=51825031/51824415\n","2021-11-10 00:26:25,829 INFO mapred.LocalJobRunner: Records R/W=51825031/51824415 > reduce\n","2021-11-10 00:26:28,645 INFO streaming.PipeMapRed: R/W/S=51900000/51899703/0 in:10873=51900000/4773 [rec/s] out:10873=51899703/4773 [rec/s]\n","2021-11-10 00:26:31,830 INFO mapred.LocalJobRunner: Records R/W=51825031/51824415 > reduce\n","2021-11-10 00:26:32,298 INFO streaming.PipeMapRed: Records R/W=51941847/51941603\n","2021-11-10 00:26:37,830 INFO mapred.LocalJobRunner: Records R/W=51941847/51941603 > reduce\n","2021-11-10 00:26:38,332 INFO streaming.PipeMapRed: R/W/S=52000000/51999502/0 in:10874=52000000/4782 [rec/s] out:10874=51999502/4782 [rec/s]\n","2021-11-10 00:26:42,306 INFO streaming.PipeMapRed: Records R/W=52039430/52038815\n","2021-11-10 00:26:43,831 INFO mapred.LocalJobRunner: Records R/W=52039430/52038815 > reduce\n","2021-11-10 00:26:48,168 INFO streaming.PipeMapRed: R/W/S=52100000/52099773/0 in:10872=52100000/4792 [rec/s] out:10872=52099773/4792 [rec/s]\n","2021-11-10 00:26:49,831 INFO mapred.LocalJobRunner: Records R/W=52039430/52038815 > reduce\n","2021-11-10 00:26:52,308 INFO streaming.PipeMapRed: Records R/W=52132960/52132538\n","2021-11-10 00:26:55,832 INFO mapred.LocalJobRunner: Records R/W=52132960/52132538 > reduce\n","2021-11-10 00:26:59,596 INFO streaming.PipeMapRed: R/W/S=52200000/52199563/0 in:10865=52200000/4804 [rec/s] out:10865=52199563/4804 [rec/s]\n","2021-11-10 00:27:01,832 INFO mapred.LocalJobRunner: Records R/W=52132960/52132538 > reduce\n","2021-11-10 00:27:02,310 INFO streaming.PipeMapRed: Records R/W=52237340/52237053\n","2021-11-10 00:27:07,833 INFO mapred.LocalJobRunner: Records R/W=52237340/52237053 > reduce\n","2021-11-10 00:27:09,282 INFO streaming.PipeMapRed: R/W/S=52300000/52299668/0 in:10866=52300000/4813 [rec/s] out:10866=52299668/4813 [rec/s]\n","2021-11-10 00:27:12,321 INFO streaming.PipeMapRed: Records R/W=52326754/52326235\n","2021-11-10 00:27:13,836 INFO mapred.LocalJobRunner: Records R/W=52326754/52326235 > reduce\n","2021-11-10 00:27:19,836 INFO mapred.LocalJobRunner: Records R/W=52326754/52326235 > reduce\n","2021-11-10 00:27:20,420 INFO streaming.PipeMapRed: R/W/S=52400000/52399295/0 in:10862=52400000/4824 [rec/s] out:10862=52399295/4824 [rec/s]\n","2021-11-10 00:27:22,324 INFO streaming.PipeMapRed: Records R/W=52420986/52420423\n","2021-11-10 00:27:25,837 INFO mapred.LocalJobRunner: Records R/W=52420986/52420423 > reduce\n","2021-11-10 00:27:30,182 INFO streaming.PipeMapRed: R/W/S=52500000/52499314/0 in:10860=52500000/4834 [rec/s] out:10860=52499314/4834 [rec/s]\n","2021-11-10 00:27:31,840 INFO mapred.LocalJobRunner: Records R/W=52420986/52420423 > reduce\n","2021-11-10 00:27:32,326 INFO streaming.PipeMapRed: Records R/W=52522129/52521388\n","2021-11-10 00:27:37,840 INFO mapred.LocalJobRunner: Records R/W=52522129/52521388 > reduce\n","2021-11-10 00:27:41,434 INFO streaming.PipeMapRed: R/W/S=52600000/52599887/0 in:10856=52600000/4845 [rec/s] out:10856=52599887/4845 [rec/s]\n","2021-11-10 00:27:42,328 INFO streaming.PipeMapRed: Records R/W=52606417/52605749\n","2021-11-10 00:27:43,841 INFO mapred.LocalJobRunner: Records R/W=52606417/52605749 > reduce\n","2021-11-10 00:27:49,841 INFO mapred.LocalJobRunner: Records R/W=52606417/52605749 > reduce\n","2021-11-10 00:27:52,333 INFO streaming.PipeMapRed: Records R/W=52694075/52693714\n","2021-11-10 00:27:53,049 INFO streaming.PipeMapRed: R/W/S=52700000/52699651/0 in:10850=52700000/4857 [rec/s] out:10850=52699651/4857 [rec/s]\n","2021-11-10 00:27:55,843 INFO mapred.LocalJobRunner: Records R/W=52694075/52693714 > reduce\n","2021-11-10 00:28:01,843 INFO mapred.LocalJobRunner: Records R/W=52694075/52693714 > reduce\n","2021-11-10 00:28:02,347 INFO streaming.PipeMapRed: Records R/W=52781964/52781397\n","2021-11-10 00:28:04,479 INFO streaming.PipeMapRed: R/W/S=52800000/52799855/0 in:10846=52800000/4868 [rec/s] out:10846=52799855/4868 [rec/s]\n","2021-11-10 00:28:07,846 INFO mapred.LocalJobRunner: Records R/W=52781964/52781397 > reduce\n","2021-11-10 00:28:12,353 INFO streaming.PipeMapRed: Records R/W=52870563/52870059\n","2021-11-10 00:28:13,849 INFO mapred.LocalJobRunner: Records R/W=52870563/52870059 > reduce\n","2021-11-10 00:28:15,727 INFO streaming.PipeMapRed: R/W/S=52900000/52899675/0 in:10840=52900000/4880 [rec/s] out:10840=52899675/4880 [rec/s]\n","2021-11-10 00:28:19,849 INFO mapred.LocalJobRunner: Records R/W=52870563/52870059 > reduce\n","2021-11-10 00:28:20,735 INFO streaming.PipeMapRed: R/W/S=53000000/52999622/0 in:10849=53000000/4885 [rec/s] out:10849=52999622/4885 [rec/s]\n","2021-11-10 00:28:22,356 INFO streaming.PipeMapRed: Records R/W=53014014/53013789\n","2021-11-10 00:28:25,850 INFO mapred.LocalJobRunner: Records R/W=53014014/53013789 > reduce\n","2021-11-10 00:28:31,851 INFO mapred.LocalJobRunner: Records R/W=53014014/53013789 > reduce\n","2021-11-10 00:28:32,069 INFO streaming.PipeMapRed: R/W/S=53100000/53099797/0 in:10845=53100000/4896 [rec/s] out:10845=53099797/4896 [rec/s]\n","2021-11-10 00:28:32,359 INFO streaming.PipeMapRed: Records R/W=53102512/53102227\n","2021-11-10 00:28:37,851 INFO mapred.LocalJobRunner: Records R/W=53102512/53102227 > reduce\n","2021-11-10 00:28:38,795 INFO mapreduce.Job:  map 100% reduce 100%\n","2021-11-10 00:28:42,363 INFO streaming.PipeMapRed: Records R/W=53194302/53193723\n","2021-11-10 00:28:42,997 INFO streaming.PipeMapRed: R/W/S=53200000/53199541/0 in:10841=53200000/4907 [rec/s] out:10841=53199541/4907 [rec/s]\n","2021-11-10 00:28:43,852 INFO mapred.LocalJobRunner: Records R/W=53194302/53193723 > reduce\n","2021-11-10 00:28:49,852 INFO mapred.LocalJobRunner: Records R/W=53194302/53193723 > reduce\n","2021-11-10 00:28:52,364 INFO streaming.PipeMapRed: Records R/W=53282271/53281916\n","2021-11-10 00:28:54,446 INFO streaming.PipeMapRed: R/W/S=53300000/53299629/0 in:10837=53300000/4918 [rec/s] out:10837=53299629/4918 [rec/s]\n","2021-11-10 00:28:55,853 INFO mapred.LocalJobRunner: Records R/W=53282271/53281916 > reduce\n","2021-11-10 00:29:01,853 INFO mapred.LocalJobRunner: Records R/W=53282271/53281916 > reduce\n","2021-11-10 00:29:02,369 INFO streaming.PipeMapRed: Records R/W=53377680/53377221\n","2021-11-10 00:29:04,837 INFO streaming.PipeMapRed: R/W/S=53400000/53399459/0 in:10833=53400000/4929 [rec/s] out:10833=53399459/4929 [rec/s]\n","2021-11-10 00:29:07,854 INFO mapred.LocalJobRunner: Records R/W=53377680/53377221 > reduce\n","2021-11-10 00:29:12,371 INFO streaming.PipeMapRed: Records R/W=53474783/53474192\n","2021-11-10 00:29:13,854 INFO mapred.LocalJobRunner: Records R/W=53474783/53474192 > reduce\n","2021-11-10 00:29:15,246 INFO streaming.PipeMapRed: R/W/S=53500000/53499592/0 in:10832=53500000/4939 [rec/s] out:10832=53499592/4939 [rec/s]\n","2021-11-10 00:29:19,855 INFO mapred.LocalJobRunner: Records R/W=53474783/53474192 > reduce\n","2021-11-10 00:29:22,373 INFO streaming.PipeMapRed: Records R/W=53554625/53553767\n","2021-11-10 00:29:25,855 INFO mapred.LocalJobRunner: Records R/W=53554625/53553767 > reduce\n","2021-11-10 00:29:26,049 INFO streaming.PipeMapRed: R/W/S=53600000/53599485/0 in:10828=53600000/4950 [rec/s] out:10828=53599485/4950 [rec/s]\n","2021-11-10 00:29:31,855 INFO mapred.LocalJobRunner: Records R/W=53554625/53553767 > reduce\n","2021-11-10 00:29:32,374 INFO streaming.PipeMapRed: Records R/W=53676541/53676201\n","2021-11-10 00:29:34,470 INFO streaming.PipeMapRed: R/W/S=53700000/53699711/0 in:10830=53700000/4958 [rec/s] out:10830=53699711/4958 [rec/s]\n","2021-11-10 00:29:37,856 INFO mapred.LocalJobRunner: Records R/W=53676541/53676201 > reduce\n","2021-11-10 00:29:42,376 INFO streaming.PipeMapRed: Records R/W=53768846/53768219\n","2021-11-10 00:29:43,856 INFO mapred.LocalJobRunner: Records R/W=53768846/53768219 > reduce\n","2021-11-10 00:29:45,160 INFO streaming.PipeMapRed: R/W/S=53800000/53799649/0 in:10827=53800000/4969 [rec/s] out:10827=53799649/4969 [rec/s]\n","2021-11-10 00:29:49,857 INFO mapred.LocalJobRunner: Records R/W=53768846/53768219 > reduce\n","2021-11-10 00:29:51,036 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 00:29:51,036 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 00:29:51,067 INFO mapred.Task: Task:attempt_local1236956871_0001_r_000000_0 is done. And is in the process of committing\n","2021-11-10 00:29:51,070 INFO mapred.LocalJobRunner: Records R/W=53768846/53768219 > reduce\n","2021-11-10 00:29:51,070 INFO mapred.Task: Task attempt_local1236956871_0001_r_000000_0 is allowed to commit now\n","2021-11-10 00:29:51,085 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1236956871_0001_r_000000_0' to hdfs://localhost:9000/user/job/clean_data\n","2021-11-10 00:29:51,085 INFO mapred.LocalJobRunner: Records R/W=53768846/53768219 > reduce\n","2021-11-10 00:29:51,086 INFO mapred.Task: Task 'attempt_local1236956871_0001_r_000000_0' done.\n","2021-11-10 00:29:51,086 INFO mapred.Task: Final Counters for attempt_local1236956871_0001_r_000000_0: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=51313864454\n","\t\tFILE: Number of bytes written=51314379152\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5461995128\n","\t\tHDFS: Number of bytes written=7868645707\n","\t\tHDFS: Number of read operations=90\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=3\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=50088439\n","\t\tReduce shuffle bytes=17104583368\n","\t\tReduce input records=53851542\n","\t\tReduce output records=53851542\n","\t\tSpilled Records=53851542\n","\t\tShuffled Maps =41\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=41\n","\t\tGC time elapsed (ms)=166\n","\t\tTotal committed heap usage (bytes)=2951741440\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=7868645707\n","2021-11-10 00:29:51,086 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236956871_0001_r_000000_0\n","2021-11-10 00:29:51,086 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2021-11-10 00:29:51,838 INFO mapreduce.Job: Job job_local1236956871_0001 completed successfully\n","2021-11-10 00:29:51,872 INFO mapreduce.Job: Counters: 36\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=413869619296\n","\t\tFILE: Number of bytes written=776445510702\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=121182686448\n","\t\tHDFS: Number of bytes written=7868645707\n","\t\tHDFS: Number of read operations=1935\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=44\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=53851542\n","\t\tMap output records=53851542\n","\t\tMap output bytes=16922011069\n","\t\tMap output materialized bytes=17104583368\n","\t\tInput split bytes=4633\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=50088439\n","\t\tReduce shuffle bytes=17104583368\n","\t\tReduce input records=53851542\n","\t\tReduce output records=53851542\n","\t\tSpilled Records=161554626\n","\t\tShuffled Maps =41\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=41\n","\t\tGC time elapsed (ms)=497\n","\t\tTotal committed heap usage (bytes)=20234371072\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=5461995128\n","\tFile Output Format Counters \n","\t\tBytes Written=7868645707\n","2021-11-10 00:29:51,872 INFO streaming.StreamJob: Output directory: /user//job/clean_data\n"]}]},{"cell_type":"code","metadata":{"id":"9QWVt5jrQJrR"},"source":["# move the clean data to drive\n","!${hdfs} dfs  -copyToLocal  /user/$USER/job/clean_data  /content/drive/MyDrive/mp1-bigdata/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4bwkit4DOPV","executionInfo":{"status":"ok","timestamp":1636553929452,"user_tz":-120,"elapsed":1122435,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"639015fa-5533-4647-a86b-2b7be7ca2b33"},"source":["# create a zipped version of the clean data and also move to drive\n","%%bash\n","sudo apt install bzip2\n","bzip2 -z --keep /content/drive/MyDrive/mp1-bigdata/clean_data/input/clean_data"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","bzip2 is already the newest version (1.0.6-8.1ubuntu0.2).\n","0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"]},{"output_type":"stream","name":"stderr","text":["\n","WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"jTWgkcUtuoVq"},"source":["##set up the HDFS, transfer data into it"]},{"cell_type":"code","metadata":{"id":"biN-2nB0Xqr-"},"source":["#set up the directories in the HDFS\n","%%bash\n","#/usr/local/hadoop-3.3.0/bin/hdfs dfs -mkdir /user\n","/usr/local/hadoop-3.3.0/bin/hdfs  dfs -mkdir /user/$USER\n","/usr/local/hadoop-3.3.0/bin/hdfs  dfs -mkdir /user/$USER/job\n","/usr/local/hadoop-3.3.0/bin/hdfs  dfs -mkdir /user/$USER/job/req1\n","/usr/local/hadoop-3.3.0/bin/hdfs  dfs -mkdir /user/$USER/job/req2\n","/usr/local/hadoop-3.3.0/bin/hdfs  dfs -mkdir /user/$USER/job/req2-posts\n","/usr/local/hadoop-3.3.0/bin/hdfs  dfs -mkdir /user/$USER/job/req3\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5DwbxpLMt2aN"},"source":["## loading a data sample into HDFS"]},{"cell_type":"code","metadata":{"id":"gRorNQwK3ZpR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636746791634,"user_tz":-120,"elapsed":2564,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"6bef5e72-3fb4-4eb5-aa12-8eb316d183a1"},"source":["# sample a bunch of data out\n","!rm -r input\n","!mkdir input\n","!head -n 1000000 /content/drive/MyDrive/mp1-bigdata/raw_data/RC_2015-01 >> input/sample.txt\n","# check the sample encoding \n","#! chardetect /content/input/sample.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'input': No such file or directory\n"]}]},{"cell_type":"code","metadata":{"id":"WrjFVNnwtO3l"},"source":["# remove a previous input directory if exists\n","#!/usr/local/hadoop-3.3.0/bin/hdfs  dfs  -rm  -r /user/$USER/job/input\n","\n","# put data into HDFS\n","!/usr/local/hadoop-3.3.0/bin/hdfs  dfs -put /content/input/ /user/$USER/job\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S9wrPfkkKAF4"},"source":["# remove the local input directory to save space \n","! rm -r input"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iiefrk-guRC9"},"source":["## transfer entire dataset (clean data) into HDFS"]},{"cell_type":"code","metadata":{"id":"1iemDIccZrJ0"},"source":["# transfer data to HDFS\n","!/usr/local/hadoop-3.3.0/bin/hdfs  dfs -put /content/drive/MyDrive/mp1-bigdata/clean_data/compressed/input/ /user/$USER/job\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iURNbF4dYRgp","executionInfo":{"status":"ok","timestamp":1636593352956,"user_tz":-120,"elapsed":1845,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"36344299-5d5c-48b1-fe5c-08ba47607216"},"source":["#  Check the HDFS content\n","!/usr/local/hadoop-3.3.0/bin/hdfs dfs -ls -h -R /user"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drwxr-xr-x   - root supergroup          0 2021-11-11 01:15 /user/job\n","drwxr-xr-x   - root supergroup          0 2021-11-11 01:15 /user/job/input\n","-rw-r--r--   1 root supergroup    254.0 M 2021-11-11 01:15 /user/job/input/clean_data.bz2\n","drwxr-xr-x   - root supergroup          0 2021-11-11 01:15 /user/job/req1\n","drwxr-xr-x   - root supergroup          0 2021-11-11 01:15 /user/job/req2\n","drwxr-xr-x   - root supergroup          0 2021-11-11 01:15 /user/job/req3\n"]}]},{"cell_type":"markdown","metadata":{"id":"_iSdGlvVAUUc"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"SLc08BSmvx81"},"source":["## Run mapreduce Jobs for REQUIREMENT 1:\n","\n","3 jobs chained"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xC9CAPyrHb0o","executionInfo":{"status":"ok","timestamp":1636594842791,"user_tz":-120,"elapsed":3886,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"23875b8d-acb0-4d7b-b91a-676342ad1ad4"},"source":["# Run Job 1\n","!${hadoop_stream}  -input /user/$USER/job/input/ -output /user/$USER/job/req1/output -file /content/drive/MyDrive/mp1-bigdata/req1/mapper.py  -file /content/drive/MyDrive/mp1-bigdata/req1/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-11-11 01:40:38,598 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","packageJobJar: [/content/drive/MyDrive/mp1-bigdata/req1/mapper.py, /content/drive/MyDrive/mp1-bigdata/req1/reducer.py] [] /tmp/streamjob9141056926424092505.jar tmpDir=null\n","2021-11-11 01:40:40,365 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2021-11-11 01:40:40,510 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2021-11-11 01:40:40,510 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2021-11-11 01:40:40,530 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2021-11-11 01:40:40,652 ERROR streaming.StreamJob: Error Launching job : Output directory hdfs://localhost:9000/user/job/req1/output already exists\n","Streaming Command Failed!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qgMOsRzqqL6","executionInfo":{"status":"ok","timestamp":1636594899715,"user_tz":-120,"elapsed":36382,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"5067230e-b82b-4117-b3fb-a5ffeacc22f9"},"source":["#Run job2\n","!${hadoop_stream}  -input /user/$USER/job/req1/output/ -output /user/$USER/job/req1/output2 -file /content/drive/MyDrive/mp1-bigdata/req1/mapper2.py  -file /content/drive/MyDrive/mp1-bigdata/req1/reducer2.py  -mapper 'python mapper2.py'  -reducer 'python reducer2.py'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-11-11 01:41:03,110 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","packageJobJar: [/content/drive/MyDrive/mp1-bigdata/req1/mapper2.py, /content/drive/MyDrive/mp1-bigdata/req1/reducer2.py] [] /tmp/streamjob16221223788816649769.jar tmpDir=null\n","2021-11-11 01:41:04,493 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2021-11-11 01:41:04,635 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2021-11-11 01:41:04,635 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2021-11-11 01:41:04,651 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2021-11-11 01:41:04,981 INFO mapred.FileInputFormat: Total input files to process : 1\n","2021-11-11 01:41:05,006 INFO mapreduce.JobSubmitter: number of splits:5\n","2021-11-11 01:41:05,268 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local126579158_0001\n","2021-11-11 01:41:05,268 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2021-11-11 01:41:05,587 INFO mapred.LocalDistributedCacheManager: Localized file:/content/drive/MyDrive/mp1-bigdata/req1/mapper2.py as file:/tmp/hadoop-root/mapred/local/job_local126579158_0001_9b0a58a9-2d83-4ae0-964a-32fa27154553/mapper2.py\n","2021-11-11 01:41:05,618 INFO mapred.LocalDistributedCacheManager: Localized file:/content/drive/MyDrive/mp1-bigdata/req1/reducer2.py as file:/tmp/hadoop-root/mapred/local/job_local126579158_0001_26f99c14-2afc-48e1-a485-dd60493d97bc/reducer2.py\n","2021-11-11 01:41:05,687 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2021-11-11 01:41:05,689 INFO mapreduce.Job: Running job: job_local126579158_0001\n","2021-11-11 01:41:05,694 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2021-11-11 01:41:05,696 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2021-11-11 01:41:05,700 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:41:05,700 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:41:05,755 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2021-11-11 01:41:05,758 INFO mapred.LocalJobRunner: Starting task: attempt_local126579158_0001_m_000000_0\n","2021-11-11 01:41:05,785 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:41:05,787 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:41:05,818 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:41:05,829 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/req1/output/part-00000:0+134217728\n","2021-11-11 01:41:05,863 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-11 01:41:05,913 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-11 01:41:05,913 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-11 01:41:05,913 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-11 01:41:05,913 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-11 01:41:05,913 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-11 01:41:05,917 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-11 01:41:05,928 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n","2021-11-11 01:41:05,935 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2021-11-11 01:41:05,936 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2021-11-11 01:41:05,937 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2021-11-11 01:41:05,937 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2021-11-11 01:41:05,938 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2021-11-11 01:41:05,938 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2021-11-11 01:41:05,939 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2021-11-11 01:41:05,939 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2021-11-11 01:41:05,939 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2021-11-11 01:41:05,940 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2021-11-11 01:41:05,940 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2021-11-11 01:41:05,940 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2021-11-11 01:41:06,087 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:06,087 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:06,109 INFO streaming.PipeMapRed: Records R/W=84/1\n","2021-11-11 01:41:06,113 INFO streaming.PipeMapRed: R/W/S=100/37/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:06,516 INFO streaming.PipeMapRed: R/W/S=1000/891/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:06,693 INFO mapreduce.Job: Job job_local126579158_0001 running in uber mode : false\n","2021-11-11 01:41:06,694 INFO mapreduce.Job:  map 0% reduce 0%\n","2021-11-11 01:41:08,427 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:41:08,427 INFO mapred.MapTask: bufstart = 0; bufend = 83831112; bufvoid = 104857600\n","2021-11-11 01:41:08,427 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199464(104797856); length = 14933/6553600\n","2021-11-11 01:41:08,427 INFO mapred.MapTask: (EQUATOR) 83846040 kvi 20961504(83846016)\n","2021-11-11 01:41:08,789 INFO mapred.MapTask: Finished spill 0\n","2021-11-11 01:41:08,789 INFO mapred.MapTask: (RESET) equator 83846040 kv 20961504(83846016) kvi 20957784(83831136)\n","2021-11-11 01:41:09,764 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:41:09,765 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:41:09,768 INFO mapred.LocalJobRunner: \n","2021-11-11 01:41:09,768 INFO mapred.MapTask: Starting flush of map output\n","2021-11-11 01:41:09,769 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:41:09,769 INFO mapred.MapTask: bufstart = 83846040; bufend = 32790181; bufvoid = 101471639\n","2021-11-11 01:41:09,769 INFO mapred.MapTask: kvstart = 20961504(83846016); kvend = 20943484(83773936); length = 18021/6553600\n","2021-11-11 01:41:09,858 INFO mapred.MapTask: Finished spill 1\n","2021-11-11 01:41:09,878 INFO mapred.Merger: Merging 2 sorted segments\n","2021-11-11 01:41:09,889 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 134106620 bytes\n","2021-11-11 01:41:10,570 INFO mapred.Task: Task:attempt_local126579158_0001_m_000000_0 is done. And is in the process of committing\n","2021-11-11 01:41:10,578 INFO mapred.LocalJobRunner: Records R/W=84/1 > sort\n","2021-11-11 01:41:10,579 INFO mapred.Task: Task 'attempt_local126579158_0001_m_000000_0' done.\n","2021-11-11 01:41:10,586 INFO mapred.Task: Final Counters for attempt_local126579158_0001_m_000000_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=134273338\n","\t\tFILE: Number of bytes written=269158843\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=134234112\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=5\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=8240\n","\t\tMap output records=8240\n","\t\tMap output bytes=134246892\n","\t\tMap output materialized bytes=134271643\n","\t\tInput split bytes=105\n","\t\tCombine input records=0\n","\t\tSpilled Records=16480\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=75\n","\t\tTotal committed heap usage (bytes)=482344960\n","\tFile Input Format Counters \n","\t\tBytes Read=134234112\n","2021-11-11 01:41:10,587 INFO mapred.LocalJobRunner: Finishing task: attempt_local126579158_0001_m_000000_0\n","2021-11-11 01:41:10,587 INFO mapred.LocalJobRunner: Starting task: attempt_local126579158_0001_m_000001_0\n","2021-11-11 01:41:10,589 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:41:10,590 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:41:10,590 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:41:10,591 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/req1/output/part-00000:134217728+134217728\n","2021-11-11 01:41:10,612 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-11 01:41:10,634 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-11 01:41:10,634 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-11 01:41:10,634 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-11 01:41:10,634 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-11 01:41:10,634 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-11 01:41:10,635 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-11 01:41:10,646 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n","2021-11-11 01:41:10,681 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:10,682 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:10,698 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-11 01:41:10,761 INFO streaming.PipeMapRed: Records R/W=72/1\n","2021-11-11 01:41:10,765 INFO streaming.PipeMapRed: R/W/S=100/76/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:10,862 INFO streaming.PipeMapRed: R/W/S=1000/942/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:12,696 INFO streaming.PipeMapRed: R/W/S=10000/9978/0 in:5000=10000/2 [rec/s] out:4989=9978/2 [rec/s]\n","2021-11-11 01:41:12,707 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:41:12,707 INFO mapred.MapTask: bufstart = 0; bufend = 83729781; bufvoid = 104857600\n","2021-11-11 01:41:12,707 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26173688(104694752); length = 40709/6553600\n","2021-11-11 01:41:12,708 INFO mapred.MapTask: (EQUATOR) 83770469 kvi 20942612(83770448)\n","2021-11-11 01:41:13,065 INFO mapred.MapTask: Finished spill 0\n","2021-11-11 01:41:13,087 INFO mapred.MapTask: (RESET) equator 83770469 kv 20942612(83770448) kvi 20935464(83741856)\n","2021-11-11 01:41:14,380 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:41:14,381 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:41:14,382 INFO mapred.LocalJobRunner: \n","2021-11-11 01:41:14,382 INFO mapred.MapTask: Starting flush of map output\n","2021-11-11 01:41:14,382 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:41:14,382 INFO mapred.MapTask: bufstart = 83770469; bufend = 29560558; bufvoid = 104844905\n","2021-11-11 01:41:14,382 INFO mapred.MapTask: kvstart = 20942612(83770448); kvend = 20921348(83685392); length = 21265/6553600\n","2021-11-11 01:41:14,470 INFO mapred.MapTask: Finished spill 1\n","2021-11-11 01:41:14,473 INFO mapred.Merger: Merging 2 sorted segments\n","2021-11-11 01:41:14,474 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 134381473 bytes\n","2021-11-11 01:41:14,701 INFO mapreduce.Job:  map 20% reduce 0%\n","2021-11-11 01:41:15,108 INFO mapred.Task: Task:attempt_local126579158_0001_m_000001_0 is done. And is in the process of committing\n","2021-11-11 01:41:15,113 INFO mapred.LocalJobRunner: Records R/W=72/1 > sort\n","2021-11-11 01:41:15,113 INFO mapred.Task: Task 'attempt_local126579158_0001_m_000001_0' done.\n","2021-11-11 01:41:15,114 INFO mapred.Task: Final Counters for attempt_local126579158_0001_m_000001_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=268685159\n","\t\tFILE: Number of bytes written=537981415\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=268582912\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=7\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=15495\n","\t\tMap output records=15495\n","\t\tMap output bytes=134364775\n","\t\tMap output materialized bytes=134411267\n","\t\tInput split bytes=105\n","\t\tCombine input records=0\n","\t\tSpilled Records=30990\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=12\n","\t\tTotal committed heap usage (bytes)=588251136\n","\tFile Input Format Counters \n","\t\tBytes Read=134348800\n","2021-11-11 01:41:15,114 INFO mapred.LocalJobRunner: Finishing task: attempt_local126579158_0001_m_000001_0\n","2021-11-11 01:41:15,114 INFO mapred.LocalJobRunner: Starting task: attempt_local126579158_0001_m_000002_0\n","2021-11-11 01:41:15,117 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:41:15,117 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:41:15,117 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:41:15,118 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/req1/output/part-00000:268435456+134217728\n","2021-11-11 01:41:15,140 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-11 01:41:15,153 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-11 01:41:15,154 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-11 01:41:15,154 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-11 01:41:15,154 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-11 01:41:15,154 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-11 01:41:15,156 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-11 01:41:15,163 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n","2021-11-11 01:41:15,190 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:15,190 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:15,263 INFO streaming.PipeMapRed: Records R/W=46/1\n","2021-11-11 01:41:15,275 INFO streaming.PipeMapRed: R/W/S=100/93/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:15,327 INFO streaming.PipeMapRed: R/W/S=1000/954/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:15,702 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-11 01:41:17,220 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:41:17,220 INFO mapred.MapTask: bufstart = 0; bufend = 83855831; bufvoid = 104857600\n","2021-11-11 01:41:17,220 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26190840(104763360); length = 23557/6553600\n","2021-11-11 01:41:17,221 INFO mapred.MapTask: (EQUATOR) 83879287 kvi 20969816(83879264)\n","2021-11-11 01:41:17,502 INFO mapred.MapTask: Finished spill 0\n","2021-11-11 01:41:17,504 INFO mapred.MapTask: (RESET) equator 83879287 kv 20969816(83879264) kvi 20966932(83867728)\n","2021-11-11 01:41:18,401 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:41:18,402 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:41:18,402 INFO mapred.LocalJobRunner: \n","2021-11-11 01:41:18,403 INFO mapred.MapTask: Starting flush of map output\n","2021-11-11 01:41:18,403 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:41:18,403 INFO mapred.MapTask: bufstart = 83879287; bufend = 32676687; bufvoid = 101888149\n","2021-11-11 01:41:18,403 INFO mapred.MapTask: kvstart = 20969816(83879264); kvend = 20955412(83821648); length = 14405/6553600\n","2021-11-11 01:41:18,481 INFO mapred.MapTask: Finished spill 1\n","2021-11-11 01:41:18,483 INFO mapred.Merger: Merging 2 sorted segments\n","2021-11-11 01:41:18,484 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 134538243 bytes\n","2021-11-11 01:41:18,704 INFO mapreduce.Job:  map 40% reduce 0%\n","2021-11-11 01:41:19,027 INFO mapred.Task: Task:attempt_local126579158_0001_m_000002_0 is done. And is in the process of committing\n","2021-11-11 01:41:19,031 INFO mapred.LocalJobRunner: Records R/W=46/1 > sort\n","2021-11-11 01:41:19,032 INFO mapred.Task: Task 'attempt_local126579158_0001_m_000002_0' done.\n","2021-11-11 01:41:19,032 INFO mapred.Task: Final Counters for attempt_local126579158_0001_m_000002_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=403255909\n","\t\tFILE: Number of bytes written=807121845\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=403234816\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=9\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=9492\n","\t\tMap output records=9492\n","\t\tMap output bytes=134541380\n","\t\tMap output materialized bytes=134570196\n","\t\tInput split bytes=105\n","\t\tCombine input records=0\n","\t\tSpilled Records=18984\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=18\n","\t\tTotal committed heap usage (bytes)=588251136\n","\tFile Input Format Counters \n","\t\tBytes Read=134651904\n","2021-11-11 01:41:19,032 INFO mapred.LocalJobRunner: Finishing task: attempt_local126579158_0001_m_000002_0\n","2021-11-11 01:41:19,033 INFO mapred.LocalJobRunner: Starting task: attempt_local126579158_0001_m_000003_0\n","2021-11-11 01:41:19,040 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:41:19,040 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:41:19,041 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:41:19,043 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/req1/output/part-00000:402653184+134217728\n","2021-11-11 01:41:19,055 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-11 01:41:19,070 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-11 01:41:19,071 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-11 01:41:19,071 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-11 01:41:19,071 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-11 01:41:19,071 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-11 01:41:19,071 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-11 01:41:19,080 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n","2021-11-11 01:41:19,126 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:19,198 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:19,200 INFO streaming.PipeMapRed: Records R/W=11/1\n","2021-11-11 01:41:19,206 INFO streaming.PipeMapRed: R/W/S=100/69/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:19,541 INFO streaming.PipeMapRed: R/W/S=1000/966/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:19,705 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-11 01:41:20,385 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:41:20,385 INFO mapred.MapTask: bufstart = 0; bufend = 83813153; bufvoid = 104857600\n","2021-11-11 01:41:20,385 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26196124(104784496); length = 18273/6553600\n","2021-11-11 01:41:20,385 INFO mapred.MapTask: (EQUATOR) 83831425 kvi 20957852(83831408)\n","2021-11-11 01:41:20,590 INFO mapred.MapTask: Finished spill 0\n","2021-11-11 01:41:20,590 INFO mapred.MapTask: (RESET) equator 83831425 kv 20957852(83831408) kvi 20953296(83813184)\n","2021-11-11 01:41:20,848 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:41:20,849 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:41:20,850 INFO mapred.LocalJobRunner: \n","2021-11-11 01:41:20,851 INFO mapred.MapTask: Starting flush of map output\n","2021-11-11 01:41:20,851 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:41:20,851 INFO mapred.MapTask: bufstart = 83831425; bufend = 29068334; bufvoid = 104757018\n","2021-11-11 01:41:20,851 INFO mapred.MapTask: kvstart = 20957852(83831408); kvend = 20944868(83779472); length = 12985/6553600\n","2021-11-11 01:41:20,969 INFO mapred.MapTask: Finished spill 1\n","2021-11-11 01:41:20,971 INFO mapred.Merger: Merging 2 sorted segments\n","2021-11-11 01:41:20,972 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 133674904 bytes\n","2021-11-11 01:41:21,418 INFO mapred.Task: Task:attempt_local126579158_0001_m_000003_0 is done. And is in the process of committing\n","2021-11-11 01:41:21,423 INFO mapred.LocalJobRunner: Records R/W=11/1 > sort\n","2021-11-11 01:41:21,423 INFO mapred.Task: Task 'attempt_local126579158_0001_m_000003_0' done.\n","2021-11-11 01:41:21,424 INFO mapred.Task: Final Counters for attempt_local126579158_0001_m_000003_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=537087290\n","\t\tFILE: Number of bytes written=1074783537\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=537460736\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=11\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=7816\n","\t\tMap output records=7816\n","\t\tMap output bytes=133807080\n","\t\tMap output materialized bytes=133830827\n","\t\tInput split bytes=105\n","\t\tCombine input records=0\n","\t\tSpilled Records=15632\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=12\n","\t\tTotal committed heap usage (bytes)=588251136\n","\tFile Input Format Counters \n","\t\tBytes Read=134225920\n","2021-11-11 01:41:21,424 INFO mapred.LocalJobRunner: Finishing task: attempt_local126579158_0001_m_000003_0\n","2021-11-11 01:41:21,424 INFO mapred.LocalJobRunner: Starting task: attempt_local126579158_0001_m_000004_0\n","2021-11-11 01:41:21,427 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:41:21,427 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:41:21,428 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:41:21,429 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/req1/output/part-00000:536870912+34961237\n","2021-11-11 01:41:21,445 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-11 01:41:21,463 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-11 01:41:21,463 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-11 01:41:21,463 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-11 01:41:21,463 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-11 01:41:21,463 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-11 01:41:21,467 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-11 01:41:21,479 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n","2021-11-11 01:41:21,505 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:21,505 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:21,568 INFO streaming.PipeMapRed: Records R/W=37/1\n","2021-11-11 01:41:21,583 INFO streaming.PipeMapRed: R/W/S=100/36/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:21,659 INFO streaming.PipeMapRed: R/W/S=1000/983/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:21,784 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:41:21,785 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:41:21,786 INFO mapred.LocalJobRunner: \n","2021-11-11 01:41:21,786 INFO mapred.MapTask: Starting flush of map output\n","2021-11-11 01:41:21,786 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:41:21,786 INFO mapred.MapTask: bufstart = 0; bufend = 34958582; bufvoid = 104857600\n","2021-11-11 01:41:21,786 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206632(104826528); length = 7765/6553600\n","2021-11-11 01:41:21,823 INFO mapred.MapTask: Finished spill 0\n","2021-11-11 01:41:21,825 INFO mapred.Task: Task:attempt_local126579158_0001_m_000004_0 is done. And is in the process of committing\n","2021-11-11 01:41:21,831 INFO mapred.LocalJobRunner: Records R/W=37/1\n","2021-11-11 01:41:21,831 INFO mapred.Task: Task 'attempt_local126579158_0001_m_000004_0' done.\n","2021-11-11 01:41:21,832 INFO mapred.Task: Final Counters for attempt_local126579158_0001_m_000004_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=537087838\n","\t\tFILE: Number of bytes written=1109748050\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=572421973\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=13\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1942\n","\t\tMap output records=1942\n","\t\tMap output bytes=34958582\n","\t\tMap output materialized bytes=34964481\n","\t\tInput split bytes=105\n","\t\tCombine input records=0\n","\t\tSpilled Records=1942\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=588251136\n","\tFile Input Format Counters \n","\t\tBytes Read=34961237\n","2021-11-11 01:41:21,832 INFO mapred.LocalJobRunner: Finishing task: attempt_local126579158_0001_m_000004_0\n","2021-11-11 01:41:21,834 INFO mapred.LocalJobRunner: map task executor complete.\n","2021-11-11 01:41:21,838 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2021-11-11 01:41:21,840 INFO mapred.LocalJobRunner: Starting task: attempt_local126579158_0001_r_000000_0\n","2021-11-11 01:41:21,849 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:41:21,849 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:41:21,849 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:41:21,852 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@60db6725\n","2021-11-11 01:41:21,854 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2021-11-11 01:41:21,879 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2021-11-11 01:41:21,882 INFO reduce.EventFetcher: attempt_local126579158_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2021-11-11 01:41:21,924 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local126579158_0001_m_000004_0 decomp: 34964477 len: 34964481 to MEMORY\n","2021-11-11 01:41:21,956 INFO reduce.InMemoryMapOutput: Read 34964477 bytes from map-output for attempt_local126579158_0001_m_000004_0\n","2021-11-11 01:41:21,958 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 34964477, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->34964477\n","2021-11-11 01:41:21,980 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local126579158_0001_m_000000_0 decomp: 134271639 len: 134271643 to MEMORY\n","2021-11-11 01:41:22,075 INFO reduce.InMemoryMapOutput: Read 134271639 bytes from map-output for attempt_local126579158_0001_m_000000_0\n","2021-11-11 01:41:22,075 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 134271639, inMemoryMapOutputs.size() -> 2, commitMemory -> 34964477, usedMemory ->169236116\n","2021-11-11 01:41:22,098 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local126579158_0001_m_000003_0 decomp: 133830823 len: 133830827 to MEMORY\n","2021-11-11 01:41:22,199 INFO reduce.InMemoryMapOutput: Read 133830823 bytes from map-output for attempt_local126579158_0001_m_000003_0\n","2021-11-11 01:41:22,199 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 133830823, inMemoryMapOutputs.size() -> 3, commitMemory -> 169236116, usedMemory ->303066939\n","2021-11-11 01:41:22,361 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local126579158_0001_m_000002_0 decomp: 134570192 len: 134570196 to MEMORY\n","2021-11-11 01:41:22,455 INFO reduce.InMemoryMapOutput: Read 134570192 bytes from map-output for attempt_local126579158_0001_m_000002_0\n","2021-11-11 01:41:22,455 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 134570192, inMemoryMapOutputs.size() -> 4, commitMemory -> 303066939, usedMemory ->437637131\n","2021-11-11 01:41:22,615 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local126579158_0001_m_000001_0 decomp: 134411263 len: 134411267 to MEMORY\n","2021-11-11 01:41:22,708 INFO reduce.InMemoryMapOutput: Read 134411263 bytes from map-output for attempt_local126579158_0001_m_000001_0\n","2021-11-11 01:41:22,708 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 134411263, inMemoryMapOutputs.size() -> 5, commitMemory -> 437637131, usedMemory ->572048394\n","2021-11-11 01:41:22,708 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2021-11-11 01:41:22,709 INFO mapred.LocalJobRunner: 5 / 5 copied.\n","2021-11-11 01:41:22,709 INFO reduce.MergeManagerImpl: finalMerge called with 5 in-memory map-outputs and 0 on-disk map-outputs\n","2021-11-11 01:41:22,711 INFO mapred.Merger: Merging 5 sorted segments\n","2021-11-11 01:41:22,712 INFO mapred.Merger: Down to the last merge-pass, with 5 segments left of total size: 570661912 bytes\n","2021-11-11 01:41:24,861 INFO reduce.MergeManagerImpl: Merged 5 segments, 572048394 bytes to disk to satisfy reduce memory limit\n","2021-11-11 01:41:24,866 INFO reduce.MergeManagerImpl: Merging 1 files, 572048390 bytes from disk\n","2021-11-11 01:41:24,867 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2021-11-11 01:41:24,867 INFO mapred.Merger: Merging 1 sorted segments\n","2021-11-11 01:41:24,868 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 571897456 bytes\n","2021-11-11 01:41:24,868 INFO mapred.LocalJobRunner: 5 / 5 copied.\n","2021-11-11 01:41:24,874 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n","2021-11-11 01:41:24,879 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2021-11-11 01:41:24,880 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2021-11-11 01:41:25,350 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:25,394 INFO streaming.PipeMapRed: Records R/W=9/1\n","2021-11-11 01:41:25,397 INFO streaming.PipeMapRed: R/W/S=10/2/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:25,448 INFO streaming.PipeMapRed: R/W/S=100/73/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:25,714 INFO streaming.PipeMapRed: R/W/S=1000/492/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:41:27,215 INFO streaming.PipeMapRed: R/W/S=10000/7536/0 in:5000=10000/2 [rec/s] out:3768=7536/2 [rec/s]\n","2021-11-11 01:41:33,869 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n","2021-11-11 01:41:34,714 INFO mapreduce.Job:  map 100% reduce 92%\n","2021-11-11 01:41:36,864 INFO streaming.PipeMapRed: Records R/W=40464/40448\n","2021-11-11 01:41:37,170 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:41:37,173 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:41:37,193 INFO mapred.Task: Task:attempt_local126579158_0001_r_000000_0 is done. And is in the process of committing\n","2021-11-11 01:41:37,196 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n","2021-11-11 01:41:37,199 INFO mapred.Task: Task attempt_local126579158_0001_r_000000_0 is allowed to commit now\n","2021-11-11 01:41:37,215 INFO output.FileOutputCommitter: Saved output of task 'attempt_local126579158_0001_r_000000_0' to hdfs://localhost:9000/user/job/req1/output2\n","2021-11-11 01:41:37,219 INFO mapred.LocalJobRunner: Records R/W=40464/40448 > reduce\n","2021-11-11 01:41:37,219 INFO mapred.Task: Task 'attempt_local126579158_0001_r_000000_0' done.\n","2021-11-11 01:41:37,220 INFO mapred.Task: Final Counters for attempt_local126579158_0001_r_000000_0: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=1681184802\n","\t\tFILE: Number of bytes written=1681796440\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=572421973\n","\t\tHDFS: Number of bytes written=571832149\n","\t\tHDFS: Number of read operations=18\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=3\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=42985\n","\t\tReduce shuffle bytes=572048414\n","\t\tReduce input records=42985\n","\t\tReduce output records=42985\n","\t\tSpilled Records=42985\n","\t\tShuffled Maps =5\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=5\n","\t\tGC time elapsed (ms)=21\n","\t\tTotal committed heap usage (bytes)=858783744\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=571832149\n","2021-11-11 01:41:37,220 INFO mapred.LocalJobRunner: Finishing task: attempt_local126579158_0001_r_000000_0\n","2021-11-11 01:41:37,221 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2021-11-11 01:41:37,716 INFO mapreduce.Job:  map 100% reduce 100%\n","2021-11-11 01:41:37,716 INFO mapreduce.Job: Job job_local126579158_0001 completed successfully\n","2021-11-11 01:41:37,735 INFO mapreduce.Job: Counters: 36\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=3561574336\n","\t\tFILE: Number of bytes written=5480590130\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2488356522\n","\t\tHDFS: Number of bytes written=571832149\n","\t\tHDFS: Number of read operations=63\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=8\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=42985\n","\t\tMap output records=42985\n","\t\tMap output bytes=571918709\n","\t\tMap output materialized bytes=572048414\n","\t\tInput split bytes=525\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=42985\n","\t\tReduce shuffle bytes=572048414\n","\t\tReduce input records=42985\n","\t\tReduce output records=42985\n","\t\tSpilled Records=127013\n","\t\tShuffled Maps =5\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=5\n","\t\tGC time elapsed (ms)=141\n","\t\tTotal committed heap usage (bytes)=3694133248\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=572421973\n","\tFile Output Format Counters \n","\t\tBytes Written=571832149\n","2021-11-11 01:41:37,735 INFO streaming.StreamJob: Output directory: /user//job/req1/output2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gNyqV9To42AB","executionInfo":{"status":"ok","timestamp":1636595469799,"user_tz":-120,"elapsed":345448,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"ea60cbcf-c7c1-45f0-fc98-99b864061215"},"source":["# Run job 3\n","!${hadoop_stream}  -input /user/$USER/job/req1/output2/ -output /user/$USER/job/req1/output3 -file /content/drive/MyDrive/mp1-bigdata/req1/mapper3.py  -file /content/drive/MyDrive/mp1-bigdata/req1/reducer3.py  -mapper 'python mapper3.py'  -reducer 'python reducer3.py'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-11-11 01:45:24,039 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","packageJobJar: [/content/drive/MyDrive/mp1-bigdata/req1/mapper3.py, /content/drive/MyDrive/mp1-bigdata/req1/reducer3.py] [] /tmp/streamjob7165269480440162145.jar tmpDir=null\n","2021-11-11 01:45:25,431 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2021-11-11 01:45:25,585 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2021-11-11 01:45:25,585 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2021-11-11 01:45:25,607 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2021-11-11 01:45:25,944 INFO mapred.FileInputFormat: Total input files to process : 1\n","2021-11-11 01:45:25,962 INFO mapreduce.JobSubmitter: number of splits:5\n","2021-11-11 01:45:26,207 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local761059046_0001\n","2021-11-11 01:45:26,207 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2021-11-11 01:45:26,475 INFO mapred.LocalDistributedCacheManager: Localized file:/content/drive/MyDrive/mp1-bigdata/req1/mapper3.py as file:/tmp/hadoop-root/mapred/local/job_local761059046_0001_b3636731-e384-4019-aafc-5d653360fe04/mapper3.py\n","2021-11-11 01:45:26,528 INFO mapred.LocalDistributedCacheManager: Localized file:/content/drive/MyDrive/mp1-bigdata/req1/reducer3.py as file:/tmp/hadoop-root/mapred/local/job_local761059046_0001_19a9fb31-f58c-4b94-a08a-c82273a26cdb/reducer3.py\n","2021-11-11 01:45:26,659 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2021-11-11 01:45:26,661 INFO mapreduce.Job: Running job: job_local761059046_0001\n","2021-11-11 01:45:26,666 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2021-11-11 01:45:26,668 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2021-11-11 01:45:26,675 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:45:26,675 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:45:26,744 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2021-11-11 01:45:26,747 INFO mapred.LocalJobRunner: Starting task: attempt_local761059046_0001_m_000000_0\n","2021-11-11 01:45:26,773 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:45:26,775 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:45:26,813 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:45:26,823 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/req1/output2/part-00000:0+134217728\n","2021-11-11 01:45:26,859 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-11 01:45:26,908 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-11 01:45:26,909 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-11 01:45:26,909 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-11 01:45:26,909 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-11 01:45:26,909 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-11 01:45:26,923 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-11 01:45:26,936 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper3.py]\n","2021-11-11 01:45:26,946 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2021-11-11 01:45:26,946 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2021-11-11 01:45:26,947 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2021-11-11 01:45:26,947 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2021-11-11 01:45:26,948 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2021-11-11 01:45:26,948 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2021-11-11 01:45:26,948 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2021-11-11 01:45:26,948 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2021-11-11 01:45:26,949 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2021-11-11 01:45:26,949 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2021-11-11 01:45:26,950 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2021-11-11 01:45:26,950 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2021-11-11 01:45:27,101 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:27,192 INFO streaming.PipeMapRed: Records R/W=3/1\n","2021-11-11 01:45:27,664 INFO mapreduce.Job: Job job_local761059046_0001 running in uber mode : false\n","2021-11-11 01:45:27,666 INFO mapreduce.Job:  map 0% reduce 0%\n","2021-11-11 01:45:27,799 INFO streaming.PipeMapRed: R/W/S=10/20964/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:28,511 INFO streaming.PipeMapRed: R/W/S=100/58883/0 in:100=100/1 [rec/s] out:58883=58883/1 [rec/s]\n","2021-11-11 01:45:34,508 INFO streaming.PipeMapRed: R/W/S=1000/350894/0 in:142=1000/7 [rec/s] out:50127=350894/7 [rec/s]\n","2021-11-11 01:45:34,904 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:45:34,905 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:45:34,907 INFO mapred.LocalJobRunner: \n","2021-11-11 01:45:34,907 INFO mapred.MapTask: Starting flush of map output\n","2021-11-11 01:45:34,907 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:45:34,907 INFO mapred.MapTask: bufstart = 0; bufend = 33196723; bufvoid = 104857600\n","2021-11-11 01:45:34,907 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24810332(99241328); length = 1404065/6553600\n","2021-11-11 01:45:35,629 INFO mapred.MapTask: Finished spill 0\n","2021-11-11 01:45:35,641 INFO mapred.Task: Task:attempt_local761059046_0001_m_000000_0 is done. And is in the process of committing\n","2021-11-11 01:45:35,651 INFO mapred.LocalJobRunner: Records R/W=3/1\n","2021-11-11 01:45:35,652 INFO mapred.Task: Task 'attempt_local761059046_0001_m_000000_0' done.\n","2021-11-11 01:45:35,658 INFO mapred.Task: Final Counters for attempt_local761059046_0001_m_000000_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=3899\n","\t\tFILE: Number of bytes written=34543886\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=134942720\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=5\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=6701\n","\t\tMap output records=351017\n","\t\tMap output bytes=33196723\n","\t\tMap output materialized bytes=33926107\n","\t\tInput split bytes=106\n","\t\tCombine input records=0\n","\t\tSpilled Records=351017\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=16\n","\t\tTotal committed heap usage (bytes)=313524224\n","\tFile Input Format Counters \n","\t\tBytes Read=134942720\n","2021-11-11 01:45:35,659 INFO mapred.LocalJobRunner: Finishing task: attempt_local761059046_0001_m_000000_0\n","2021-11-11 01:45:35,659 INFO mapred.LocalJobRunner: Starting task: attempt_local761059046_0001_m_000001_0\n","2021-11-11 01:45:35,662 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:45:35,663 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:45:35,663 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:45:35,665 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/req1/output2/part-00000:134217728+134217728\n","2021-11-11 01:45:35,672 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-11 01:45:35,707 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-11 01:45:35,729 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-11 01:45:35,729 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-11 01:45:35,729 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-11 01:45:35,729 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-11 01:45:35,729 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-11 01:45:35,730 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-11 01:45:35,741 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper3.py]\n","2021-11-11 01:45:35,772 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:35,923 INFO streaming.PipeMapRed: Records R/W=7/1\n","2021-11-11 01:45:35,985 INFO streaming.PipeMapRed: R/W/S=10/8689/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:37,228 INFO streaming.PipeMapRed: R/W/S=100/74557/0 in:100=100/1 [rec/s] out:74557=74557/1 [rec/s]\n","2021-11-11 01:45:39,480 INFO streaming.PipeMapRed: R/W/S=1000/186562/0 in:333=1000/3 [rec/s] out:62187=186562/3 [rec/s]\n","2021-11-11 01:45:39,494 INFO streaming.PipeMapRed: R/W/S=10000/186562/0 in:3333=10000/3 [rec/s] out:62187=186562/3 [rec/s]\n","2021-11-11 01:45:39,895 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:45:39,896 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:45:39,897 INFO mapred.LocalJobRunner: \n","2021-11-11 01:45:39,897 INFO mapred.MapTask: Starting flush of map output\n","2021-11-11 01:45:39,897 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:45:39,897 INFO mapred.MapTask: bufstart = 0; bufend = 15642301; bufvoid = 104857600\n","2021-11-11 01:45:39,897 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25467732(101870928); length = 746665/6553600\n","2021-11-11 01:45:40,141 INFO mapred.MapTask: Finished spill 0\n","2021-11-11 01:45:40,147 INFO mapred.Task: Task:attempt_local761059046_0001_m_000001_0 is done. And is in the process of committing\n","2021-11-11 01:45:40,156 INFO mapred.LocalJobRunner: Records R/W=7/1\n","2021-11-11 01:45:40,156 INFO mapred.Task: Task 'attempt_local761059046_0001_m_000001_0' done.\n","2021-11-11 01:45:40,158 INFO mapred.Task: Final Counters for attempt_local761059046_0001_m_000001_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=4452\n","\t\tFILE: Number of bytes written=50571758\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=269303808\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=7\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=19188\n","\t\tMap output records=186667\n","\t\tMap output bytes=15642301\n","\t\tMap output materialized bytes=16027840\n","\t\tInput split bytes=106\n","\t\tCombine input records=0\n","\t\tSpilled Records=186667\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=11\n","\t\tTotal committed heap usage (bytes)=387973120\n","\tFile Input Format Counters \n","\t\tBytes Read=134361088\n","2021-11-11 01:45:40,158 INFO mapred.LocalJobRunner: Finishing task: attempt_local761059046_0001_m_000001_0\n","2021-11-11 01:45:40,158 INFO mapred.LocalJobRunner: Starting task: attempt_local761059046_0001_m_000002_0\n","2021-11-11 01:45:40,160 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:45:40,160 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:45:40,161 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:45:40,162 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/req1/output2/part-00000:268435456+134217728\n","2021-11-11 01:45:40,177 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-11 01:45:40,204 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-11 01:45:40,215 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-11 01:45:40,215 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-11 01:45:40,215 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-11 01:45:40,215 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-11 01:45:40,222 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-11 01:45:40,227 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper3.py]\n","2021-11-11 01:45:40,261 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:40,261 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:40,262 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:40,373 INFO streaming.PipeMapRed: Records R/W=165/1\n","2021-11-11 01:45:46,611 INFO streaming.PipeMapRed: R/W/S=1000/231221/0 in:166=1000/6 [rec/s] out:38536=231221/6 [rec/s]\n","2021-11-11 01:45:46,836 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:45:46,837 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:45:46,837 INFO mapred.LocalJobRunner: \n","2021-11-11 01:45:46,837 INFO mapred.MapTask: Starting flush of map output\n","2021-11-11 01:45:46,837 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:45:46,837 INFO mapred.MapTask: bufstart = 0; bufend = 26183929; bufvoid = 104857600\n","2021-11-11 01:45:46,837 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25289312(101157248); length = 925085/6553600\n","2021-11-11 01:45:47,093 INFO mapred.MapTask: Finished spill 0\n","2021-11-11 01:45:47,096 INFO mapred.Task: Task:attempt_local761059046_0001_m_000002_0 is done. And is in the process of committing\n","2021-11-11 01:45:47,100 INFO mapred.LocalJobRunner: Records R/W=165/1\n","2021-11-11 01:45:47,100 INFO mapred.Task: Task 'attempt_local761059046_0001_m_000002_0' done.\n","2021-11-11 01:45:47,103 INFO mapred.Task: Final Counters for attempt_local761059046_0001_m_000002_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=5005\n","\t\tFILE: Number of bytes written=77243757\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=409325568\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=9\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=5406\n","\t\tMap output records=231272\n","\t\tMap output bytes=26183929\n","\t\tMap output materialized bytes=26671967\n","\t\tInput split bytes=106\n","\t\tCombine input records=0\n","\t\tSpilled Records=231272\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=48\n","\t\tTotal committed heap usage (bytes)=387973120\n","\tFile Input Format Counters \n","\t\tBytes Read=140021760\n","2021-11-11 01:45:47,103 INFO mapred.LocalJobRunner: Finishing task: attempt_local761059046_0001_m_000002_0\n","2021-11-11 01:45:47,103 INFO mapred.LocalJobRunner: Starting task: attempt_local761059046_0001_m_000003_0\n","2021-11-11 01:45:47,106 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:45:47,106 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:45:47,107 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:45:47,108 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/req1/output2/part-00000:402653184+134217728\n","2021-11-11 01:45:47,132 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-11 01:45:47,166 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-11 01:45:47,169 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-11 01:45:47,169 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-11 01:45:47,169 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-11 01:45:47,169 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-11 01:45:47,170 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-11 01:45:47,188 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper3.py]\n","2021-11-11 01:45:47,213 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:47,271 INFO streaming.PipeMapRed: Records R/W=4/1\n","2021-11-11 01:45:47,359 INFO streaming.PipeMapRed: R/W/S=10/5298/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:48,050 INFO streaming.PipeMapRed: R/W/S=100/40624/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:48,095 INFO streaming.PipeMapRed: R/W/S=1000/43719/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:48,460 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:45:48,461 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:45:48,463 INFO mapred.LocalJobRunner: \n","2021-11-11 01:45:48,463 INFO mapred.MapTask: Starting flush of map output\n","2021-11-11 01:45:48,463 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:45:48,463 INFO mapred.MapTask: bufstart = 0; bufend = 3764882; bufvoid = 104857600\n","2021-11-11 01:45:48,463 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26033404(104133616); length = 180993/6553600\n","2021-11-11 01:45:48,506 INFO mapred.MapTask: Finished spill 0\n","2021-11-11 01:45:48,511 INFO mapred.Task: Task:attempt_local761059046_0001_m_000003_0 is done. And is in the process of committing\n","2021-11-11 01:45:48,518 INFO mapred.LocalJobRunner: Records R/W=4/1\n","2021-11-11 01:45:48,518 INFO mapred.Task: Task 'attempt_local761059046_0001_m_000003_0' done.\n","2021-11-11 01:45:48,519 INFO mapred.Task: Final Counters for attempt_local761059046_0001_m_000003_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=5558\n","\t\tFILE: Number of bytes written=81102166\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=543645696\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=11\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=8919\n","\t\tMap output records=45249\n","\t\tMap output bytes=3764882\n","\t\tMap output materialized bytes=3858377\n","\t\tInput split bytes=106\n","\t\tCombine input records=0\n","\t\tSpilled Records=45249\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=6\n","\t\tTotal committed heap usage (bytes)=387973120\n","\tFile Input Format Counters \n","\t\tBytes Read=134320128\n","2021-11-11 01:45:48,520 INFO mapred.LocalJobRunner: Finishing task: attempt_local761059046_0001_m_000003_0\n","2021-11-11 01:45:48,520 INFO mapred.LocalJobRunner: Starting task: attempt_local761059046_0001_m_000004_0\n","2021-11-11 01:45:48,525 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:45:48,525 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:45:48,525 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:45:48,526 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/req1/output2/part-00000:536870912+34961237\n","2021-11-11 01:45:48,538 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-11 01:45:48,555 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-11 01:45:48,555 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-11 01:45:48,555 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-11 01:45:48,555 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-11 01:45:48,555 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-11 01:45:48,558 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-11 01:45:48,572 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper3.py]\n","2021-11-11 01:45:48,597 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:48,684 INFO streaming.PipeMapRed: Records R/W=7/1\n","2021-11-11 01:45:48,741 INFO streaming.PipeMapRed: R/W/S=10/1918/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:49,073 INFO streaming.PipeMapRed: R/W/S=100/22681/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:53,694 INFO streaming.PipeMapRed: R/W/S=1000/225468/0 in:200=1000/5 [rec/s] out:45093=225468/5 [rec/s]\n","2021-11-11 01:45:53,753 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:45:53,756 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:45:53,756 INFO mapred.LocalJobRunner: \n","2021-11-11 01:45:53,756 INFO mapred.MapTask: Starting flush of map output\n","2021-11-11 01:45:53,756 INFO mapred.MapTask: Spilling map output\n","2021-11-11 01:45:53,756 INFO mapred.MapTask: bufstart = 0; bufend = 25239230; bufvoid = 104857600\n","2021-11-11 01:45:53,756 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25312436(101249744); length = 901961/6553600\n","2021-11-11 01:45:53,965 INFO mapred.MapTask: Finished spill 0\n","2021-11-11 01:45:53,968 INFO mapred.Task: Task:attempt_local761059046_0001_m_000004_0 is done. And is in the process of committing\n","2021-11-11 01:45:53,971 INFO mapred.LocalJobRunner: Records R/W=7/1\n","2021-11-11 01:45:53,971 INFO mapred.Task: Task 'attempt_local761059046_0001_m_000004_0' done.\n","2021-11-11 01:45:53,972 INFO mapred.Task: Final Counters for attempt_local761059046_0001_m_000004_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=6111\n","\t\tFILE: Number of bytes written=106817354\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=578606933\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=13\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=2771\n","\t\tMap output records=225491\n","\t\tMap output bytes=25239230\n","\t\tMap output materialized bytes=25715156\n","\t\tInput split bytes=106\n","\t\tCombine input records=0\n","\t\tSpilled Records=225491\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=387973120\n","\tFile Input Format Counters \n","\t\tBytes Read=34961237\n","2021-11-11 01:45:53,972 INFO mapred.LocalJobRunner: Finishing task: attempt_local761059046_0001_m_000004_0\n","2021-11-11 01:45:53,972 INFO mapred.LocalJobRunner: map task executor complete.\n","2021-11-11 01:45:53,979 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2021-11-11 01:45:53,979 INFO mapred.LocalJobRunner: Starting task: attempt_local761059046_0001_r_000000_0\n","2021-11-11 01:45:53,998 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-11 01:45:53,998 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-11 01:45:53,998 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-11 01:45:54,001 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@57067460\n","2021-11-11 01:45:54,002 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2021-11-11 01:45:54,021 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2021-11-11 01:45:54,034 INFO reduce.EventFetcher: attempt_local761059046_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2021-11-11 01:45:54,067 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local761059046_0001_m_000003_0 decomp: 3858373 len: 3858377 to MEMORY\n","2021-11-11 01:45:54,079 INFO reduce.InMemoryMapOutput: Read 3858373 bytes from map-output for attempt_local761059046_0001_m_000003_0\n","2021-11-11 01:45:54,081 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 3858373, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3858373\n","2021-11-11 01:45:54,096 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local761059046_0001_m_000000_0 decomp: 33926103 len: 33926107 to MEMORY\n","2021-11-11 01:45:54,124 INFO reduce.InMemoryMapOutput: Read 33926103 bytes from map-output for attempt_local761059046_0001_m_000000_0\n","2021-11-11 01:45:54,124 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 33926103, inMemoryMapOutputs.size() -> 2, commitMemory -> 3858373, usedMemory ->37784476\n","2021-11-11 01:45:54,144 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local761059046_0001_m_000002_0 decomp: 26671963 len: 26671967 to MEMORY\n","2021-11-11 01:45:54,167 INFO reduce.InMemoryMapOutput: Read 26671963 bytes from map-output for attempt_local761059046_0001_m_000002_0\n","2021-11-11 01:45:54,167 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26671963, inMemoryMapOutputs.size() -> 3, commitMemory -> 37784476, usedMemory ->64456439\n","2021-11-11 01:45:54,172 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local761059046_0001_m_000004_0 decomp: 25715152 len: 25715156 to MEMORY\n","2021-11-11 01:45:54,194 INFO reduce.InMemoryMapOutput: Read 25715152 bytes from map-output for attempt_local761059046_0001_m_000004_0\n","2021-11-11 01:45:54,194 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 25715152, inMemoryMapOutputs.size() -> 4, commitMemory -> 64456439, usedMemory ->90171591\n","2021-11-11 01:45:54,202 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local761059046_0001_m_000001_0 decomp: 16027836 len: 16027840 to MEMORY\n","2021-11-11 01:45:54,220 INFO reduce.InMemoryMapOutput: Read 16027836 bytes from map-output for attempt_local761059046_0001_m_000001_0\n","2021-11-11 01:45:54,220 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 16027836, inMemoryMapOutputs.size() -> 5, commitMemory -> 90171591, usedMemory ->106199427\n","2021-11-11 01:45:54,227 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2021-11-11 01:45:54,227 INFO mapred.LocalJobRunner: 5 / 5 copied.\n","2021-11-11 01:45:54,228 INFO reduce.MergeManagerImpl: finalMerge called with 5 in-memory map-outputs and 0 on-disk map-outputs\n","2021-11-11 01:45:54,233 INFO mapred.Merger: Merging 5 sorted segments\n","2021-11-11 01:45:54,234 INFO mapred.Merger: Down to the last merge-pass, with 5 segments left of total size: 106199064 bytes\n","2021-11-11 01:45:54,903 INFO reduce.MergeManagerImpl: Merged 5 segments, 106199427 bytes to disk to satisfy reduce memory limit\n","2021-11-11 01:45:54,904 INFO reduce.MergeManagerImpl: Merging 1 files, 106199423 bytes from disk\n","2021-11-11 01:45:54,906 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2021-11-11 01:45:54,906 INFO mapred.Merger: Merging 1 sorted segments\n","2021-11-11 01:45:54,908 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 106199381 bytes\n","2021-11-11 01:45:54,909 INFO mapred.LocalJobRunner: 5 / 5 copied.\n","2021-11-11 01:45:54,917 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer3.py]\n","2021-11-11 01:45:54,919 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2021-11-11 01:45:54,920 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2021-11-11 01:45:55,476 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:55,476 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:55,485 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:55,493 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:55,562 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-11 01:45:56,278 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:100000=100000/1 [rec/s] out:0=0/1 [rec/s]\n","2021-11-11 01:45:56,819 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n","2021-11-11 01:45:57,335 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:150000=300000/2 [rec/s] out:0=0/2 [rec/s]\n","2021-11-11 01:45:57,777 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n","2021-11-11 01:45:58,302 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:166666=500000/3 [rec/s] out:0=0/3 [rec/s]\n","2021-11-11 01:45:58,864 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:200000=600000/3 [rec/s] out:0=0/3 [rec/s]\n","2021-11-11 01:45:59,433 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:175000=700000/4 [rec/s] out:0=0/4 [rec/s]\n","2021-11-11 01:45:59,874 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:200000=800000/4 [rec/s] out:0=0/4 [rec/s]\n","2021-11-11 01:46:00,482 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:180000=900000/5 [rec/s] out:0=0/5 [rec/s]\n","2021-11-11 01:46:00,928 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:166666=1000000/6 [rec/s] out:0=0/6 [rec/s]\n","2021-11-11 01:46:06,002 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:46:06,692 INFO mapreduce.Job:  map 100% reduce 87%\n","2021-11-11 01:46:12,003 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:46:12,696 INFO mapreduce.Job:  map 100% reduce 88%\n","2021-11-11 01:46:18,004 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:46:24,005 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:46:24,706 INFO mapreduce.Job:  map 100% reduce 89%\n","2021-11-11 01:46:30,007 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:46:36,008 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:46:36,732 INFO mapreduce.Job:  map 100% reduce 90%\n","2021-11-11 01:46:42,009 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:46:48,010 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:46:54,013 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:46:54,742 INFO mapreduce.Job:  map 100% reduce 92%\n","2021-11-11 01:47:00,015 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:47:54,017 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:48:00,018 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:48:00,783 INFO mapreduce.Job:  map 100% reduce 93%\n","2021-11-11 01:48:06,023 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:48:12,027 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:48:12,789 INFO mapreduce.Job:  map 100% reduce 94%\n","2021-11-11 01:48:18,029 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:48:24,032 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:48:24,794 INFO mapreduce.Job:  map 100% reduce 95%\n","2021-11-11 01:48:30,033 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:48:36,034 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:48:42,035 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:48:42,803 INFO mapreduce.Job:  map 100% reduce 96%\n","2021-11-11 01:48:48,037 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:49:00,039 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:49:00,811 INFO mapreduce.Job:  map 100% reduce 97%\n","2021-11-11 01:50:00,042 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:50:00,843 INFO mapreduce.Job:  map 100% reduce 98%\n","2021-11-11 01:50:06,046 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:50:12,047 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:50:18,049 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:50:18,850 INFO mapreduce.Job:  map 100% reduce 99%\n","2021-11-11 01:50:24,050 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:50:48,052 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:50:48,865 INFO mapreduce.Job:  map 100% reduce 100%\n","2021-11-11 01:51:00,053 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:51:01,640 INFO streaming.PipeMapRed: Records R/W=1039696/1\n","2021-11-11 01:51:05,207 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-11 01:51:05,208 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-11 01:51:05,400 INFO mapred.Task: Task:attempt_local761059046_0001_r_000000_0 is done. And is in the process of committing\n","2021-11-11 01:51:05,403 INFO mapred.LocalJobRunner: reduce > reduce\n","2021-11-11 01:51:05,403 INFO mapred.Task: Task attempt_local761059046_0001_r_000000_0 is allowed to commit now\n","2021-11-11 01:51:05,421 INFO output.FileOutputCommitter: Saved output of task 'attempt_local761059046_0001_r_000000_0' to hdfs://localhost:9000/user/job/req1/output3\n","2021-11-11 01:51:05,422 INFO mapred.LocalJobRunner: Records R/W=1039696/1 > reduce\n","2021-11-11 01:51:05,422 INFO mapred.Task: Task 'attempt_local761059046_0001_r_000000_0' done.\n","2021-11-11 01:51:05,423 INFO mapred.Task: Final Counters for attempt_local761059046_0001_r_000000_0: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=212405141\n","\t\tFILE: Number of bytes written=213016777\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=578606933\n","\t\tHDFS: Number of bytes written=137874659\n","\t\tHDFS: Number of read operations=18\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=3\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1037798\n","\t\tReduce shuffle bytes=106199447\n","\t\tReduce input records=1039696\n","\t\tReduce output records=690972\n","\t\tSpilled Records=1039696\n","\t\tShuffled Maps =5\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=5\n","\t\tGC time elapsed (ms)=19\n","\t\tTotal committed heap usage (bytes)=387973120\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=137874659\n","2021-11-11 01:51:05,423 INFO mapred.LocalJobRunner: Finishing task: attempt_local761059046_0001_r_000000_0\n","2021-11-11 01:51:05,423 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2021-11-11 01:51:05,872 INFO mapreduce.Job: Job job_local761059046_0001 completed successfully\n","2021-11-11 01:51:05,890 INFO mapreduce.Job: Counters: 36\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=212430166\n","\t\tFILE: Number of bytes written=563295698\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2514431658\n","\t\tHDFS: Number of bytes written=137874659\n","\t\tHDFS: Number of read operations=63\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=8\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=42985\n","\t\tMap output records=1039696\n","\t\tMap output bytes=104027065\n","\t\tMap output materialized bytes=106199447\n","\t\tInput split bytes=530\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1037798\n","\t\tReduce shuffle bytes=106199447\n","\t\tReduce input records=1039696\n","\t\tReduce output records=690972\n","\t\tSpilled Records=2079392\n","\t\tShuffled Maps =5\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=5\n","\t\tGC time elapsed (ms)=103\n","\t\tTotal committed heap usage (bytes)=2253389824\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=578606933\n","\tFile Output Format Counters \n","\t\tBytes Written=137874659\n","2021-11-11 01:51:05,890 INFO streaming.StreamJob: Output directory: /user//job/req1/output3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zoHwS0x6KXi","executionInfo":{"status":"ok","timestamp":1636432652754,"user_tz":-120,"elapsed":3878,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"d097dd01-3a5b-4036-e0ed-1ef0f1d3f6e1"},"source":["# view the outputs\n","!${hdfs} dfs -ls -h -R /user"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drwxr-xr-x   - root supergroup          0 2021-11-09 04:36 /user/job\n","drwxr-xr-x   - root supergroup          0 2021-11-09 04:37 /user/job/req1\n","drwxr-xr-x   - root supergroup          0 2021-11-09 04:36 /user/job/req1/input\n","-rw-r--r--   1 root supergroup     54.6 M 2021-11-09 04:36 /user/job/req1/input/sample.txt\n","drwxr-xr-x   - root supergroup          0 2021-11-09 04:36 /user/job/req1/output\n","-rw-r--r--   1 root supergroup          0 2021-11-09 04:36 /user/job/req1/output/_SUCCESS\n","-rw-r--r--   1 root supergroup      1.9 M 2021-11-09 04:36 /user/job/req1/output/part-00000\n","drwxr-xr-x   - root supergroup          0 2021-11-09 04:37 /user/job/req1/output2\n","-rw-r--r--   1 root supergroup          0 2021-11-09 04:37 /user/job/req1/output2/_SUCCESS\n","-rw-r--r--   1 root supergroup    513.8 K 2021-11-09 04:37 /user/job/req1/output2/part-00000\n","drwxr-xr-x   - root supergroup          0 2021-11-09 04:37 /user/job/req1/output3\n","-rw-r--r--   1 root supergroup          0 2021-11-09 04:37 /user/job/req1/output3/_SUCCESS\n","-rw-r--r--   1 root supergroup    822.1 K 2021-11-09 04:37 /user/job/req1/output3/part-00000\n"]}]},{"cell_type":"code","metadata":{"id":"HHarouDNsqKT"},"source":["!${hdfs} dfs  -copyToLocal  /user/$USER/job/req1/output3  /content/drive/MyDrive/mp1-bigdata/req1\n","!${hdfs} dfs  -copyToLocal  /user/$USER/job/req1/output2  /content/drive/MyDrive/mp1-bigdata/req1\n","!${hdfs} dfs  -copyToLocal  /user/$USER/job/req1/output  /content/drive/MyDrive/mp1-bigdata/req1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZwBlTKQl3zD9","executionInfo":{"status":"ok","timestamp":1636591434562,"user_tz":-120,"elapsed":6385,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"c846eb5b-9099-4b84-9fab-81e6515d4d6b"},"source":["#!/usr/local/hadoop-3.3.0/bin/hdfs  dfs  -rm  -r /user/$USER/job/req1/input/.ipynb_checkpoints\n","!/usr/local/hadoop-3.3.0/bin/hdfs  dfs  -rm  -r /user/$USER/job/req1/output2\n","!/usr/local/hadoop-3.3.0/bin/hdfs  dfs  -rm  -r /user/$USER/job/req1/output\n","!/usr/local/hadoop-3.3.0/bin/hdfs  dfs  -rm  -r /user/$USER/job/req1/output3\n","\n","#!rm -r /content/drive/MyDrive/mp1-bigdata/req1/output3"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted /user/job/req1/output2\n","rm: `/user//job/req1/output3': No such file or directory\n"]}]},{"cell_type":"markdown","metadata":{"id":"mSNCx9O1I57c"},"source":["## run mapreduce job for REQUIREMENT 2 "]},{"cell_type":"markdown","metadata":{"id":"b6j7vBXrrCV-"},"source":["### contaversiality per reply to comment rate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g5-G17N4Mh1c","executionInfo":{"status":"ok","timestamp":1636551169015,"user_tz":-120,"elapsed":87193,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"885d9d79-5f40-4ae0-b20a-bc6abae1c1b9"},"source":["!${hadoop_stream}  -input /user/$USER/job/input/ -output /user/$USER/job/req2/output -file /content/drive/MyDrive/mp1-bigdata/req2/mapper.py  -file /content/drive/MyDrive/mp1-bigdata/req2/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-11-10 13:22:11,331 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","packageJobJar: [/content/drive/MyDrive/mp1-bigdata/req2/mapper.py, /content/drive/MyDrive/mp1-bigdata/req2/reducer.py] [] /tmp/streamjob3127149342338541294.jar tmpDir=null\n","2021-11-10 13:22:14,593 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2021-11-10 13:22:14,764 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2021-11-10 13:22:14,765 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2021-11-10 13:22:14,813 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2021-11-10 13:22:15,406 INFO mapred.FileInputFormat: Total input files to process : 1\n","2021-11-10 13:22:15,448 INFO mapreduce.JobSubmitter: number of splits:59\n","2021-11-10 13:22:15,867 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1867248162_0001\n","2021-11-10 13:22:15,867 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2021-11-10 13:22:16,392 INFO mapred.LocalDistributedCacheManager: Localized file:/content/drive/MyDrive/mp1-bigdata/req2/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1867248162_0001_d4b15047-0abb-4410-8a3e-fa301c8d3725/mapper.py\n","2021-11-10 13:22:16,429 INFO mapred.LocalDistributedCacheManager: Localized file:/content/drive/MyDrive/mp1-bigdata/req2/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1867248162_0001_5b0fcaf0-ee16-43ff-9c31-3b12055ae131/reducer.py\n","2021-11-10 13:22:16,577 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2021-11-10 13:22:16,579 INFO mapreduce.Job: Running job: job_local1867248162_0001\n","2021-11-10 13:22:16,589 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2021-11-10 13:22:16,592 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2021-11-10 13:22:16,598 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:22:16,598 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:22:16,776 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2021-11-10 13:22:16,782 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000000_0\n","2021-11-10 13:22:16,831 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:22:16,832 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:22:16,872 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:22:16,890 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:0+134217728\n","2021-11-10 13:22:16,985 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:22:17,153 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:22:17,154 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:22:17,154 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:22:17,154 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:22:17,154 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:22:17,159 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:22:17,197 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:22:17,207 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2021-11-10 13:22:17,208 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2021-11-10 13:22:17,209 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2021-11-10 13:22:17,209 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2021-11-10 13:22:17,210 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2021-11-10 13:22:17,210 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2021-11-10 13:22:17,211 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2021-11-10 13:22:17,211 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2021-11-10 13:22:17,212 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2021-11-10 13:22:17,213 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2021-11-10 13:22:17,213 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2021-11-10 13:22:17,213 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2021-11-10 13:22:17,587 INFO mapreduce.Job: Job job_local1867248162_0001 running in uber mode : false\n","2021-11-10 13:22:17,589 INFO mapreduce.Job:  map 0% reduce 0%\n","2021-11-10 13:22:17,667 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:17,668 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:17,672 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:17,691 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:17,723 INFO streaming.PipeMapRed: Records R/W=1800/1\n","2021-11-10 13:22:17,932 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:18,906 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:100000=100000/1 [rec/s] out:98328=98328/1 [rec/s]\n","2021-11-10 13:22:19,848 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:100000=200000/2 [rec/s] out:99352=198704/2 [rec/s]\n","2021-11-10 13:22:20,697 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:100000=300000/3 [rec/s] out:99352=298056/3 [rec/s]\n","2021-11-10 13:22:21,475 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:100000=400000/4 [rec/s] out:99608=398433/4 [rec/s]\n","2021-11-10 13:22:22,239 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:100000=500000/5 [rec/s] out:99761=498809/5 [rec/s]\n","2021-11-10 13:22:22,966 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:120000=600000/5 [rec/s] out:119632=598162/5 [rec/s]\n","2021-11-10 13:22:23,686 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:116666=700000/6 [rec/s] out:116423=698538/6 [rec/s]\n","2021-11-10 13:22:24,465 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:114285=800000/7 [rec/s] out:114130=798915/7 [rec/s]\n","2021-11-10 13:22:25,198 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:128571=900000/7 [rec/s] out:128323=898267/7 [rec/s]\n","2021-11-10 13:22:25,354 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:22:25,354 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:22:25,360 INFO mapred.LocalJobRunner: \n","2021-11-10 13:22:25,360 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:22:25,360 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:22:25,360 INFO mapred.MapTask: bufstart = 0; bufend = 8256978; bufvoid = 104857600\n","2021-11-10 13:22:25,360 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22544632(90178528); length = 3669765/6553600\n","2021-11-10 13:22:25,834 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:22:25,853 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000000_0 is done. And is in the process of committing\n","2021-11-10 13:22:25,868 INFO mapred.LocalJobRunner: Records R/W=1800/1\n","2021-11-10 13:22:25,868 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000000_0' done.\n","2021-11-10 13:22:25,877 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000000_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=6631\n","\t\tFILE: Number of bytes written=10717075\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=134221824\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=5\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=917442\n","\t\tMap output records=917442\n","\t\tMap output bytes=8256978\n","\t\tMap output materialized bytes=10091868\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=917442\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=154\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:22:25,877 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000000_0\n","2021-11-10 13:22:25,877 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000001_0\n","2021-11-10 13:22:25,881 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:22:25,881 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:22:25,881 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:22:25,885 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:134217728+134217728\n","2021-11-10 13:22:25,910 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:22:25,997 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:22:25,997 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:22:25,997 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:22:25,997 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:22:25,997 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:22:25,999 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:22:26,019 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:22:26,060 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:26,060 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:26,062 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:26,164 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:26,171 INFO streaming.PipeMapRed: Records R/W=1753/1\n","2021-11-10 13:22:26,261 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:26,618 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:22:27,055 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:100000=100000/1 [rec/s] out:98328=98328/1 [rec/s]\n","2021-11-10 13:22:27,823 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:22:28,571 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:22:29,340 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:22:30,122 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:125000=500000/4 [rec/s] out:124702=498809/4 [rec/s]\n","2021-11-10 13:22:30,934 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:22:31,681 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:22:31,901 INFO mapred.LocalJobRunner: Records R/W=1753/1 > map\n","2021-11-10 13:22:32,412 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:22:32,623 INFO mapreduce.Job:  map 3% reduce 0%\n","2021-11-10 13:22:33,125 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:128571=900000/7 [rec/s] out:128323=898267/7 [rec/s]\n","2021-11-10 13:22:33,195 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:22:33,195 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:22:33,196 INFO mapred.LocalJobRunner: Records R/W=1753/1 > map\n","2021-11-10 13:22:33,196 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:22:33,196 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:22:33,196 INFO mapred.MapTask: bufstart = 0; bufend = 8156583; bufvoid = 104857600\n","2021-11-10 13:22:33,197 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22589252(90357008); length = 3625145/6553600\n","2021-11-10 13:22:33,633 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:22:33,637 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000001_0 is done. And is in the process of committing\n","2021-11-10 13:22:33,641 INFO mapred.LocalJobRunner: Records R/W=1753/1\n","2021-11-10 13:22:33,641 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000001_0' done.\n","2021-11-10 13:22:33,642 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000001_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=10783\n","\t\tFILE: Number of bytes written=20686270\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=268443648\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=7\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=906287\n","\t\tMap output records=906287\n","\t\tMap output bytes=8156583\n","\t\tMap output materialized bytes=9969163\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=906287\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:22:33,642 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000001_0\n","2021-11-10 13:22:33,642 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000002_0\n","2021-11-10 13:22:33,648 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:22:33,648 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:22:33,649 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:22:33,650 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:268435456+134217728\n","2021-11-10 13:22:33,663 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:22:33,685 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:22:33,685 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:22:33,685 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:22:33,686 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:22:33,686 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:22:33,696 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:22:33,705 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:22:33,726 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:33,727 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:33,727 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:33,875 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:33,880 INFO streaming.PipeMapRed: Records R/W=1791/1\n","2021-11-10 13:22:33,958 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:34,579 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:34,624 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:22:35,343 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:22:36,084 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:22:36,831 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:22:37,576 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:22:38,383 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:22:39,105 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:22:39,829 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:22:40,729 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:128571=900000/7 [rec/s] out:128323=898267/7 [rec/s]\n","2021-11-10 13:22:40,878 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:22:40,880 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:22:40,881 INFO mapred.LocalJobRunner: \n","2021-11-10 13:22:40,881 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:22:40,881 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:22:40,881 INFO mapred.MapTask: bufstart = 0; bufend = 8258814; bufvoid = 104857600\n","2021-11-10 13:22:40,881 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22543816(90175264); length = 3670581/6553600\n","2021-11-10 13:22:41,192 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:22:41,196 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000002_0 is done. And is in the process of committing\n","2021-11-10 13:22:41,203 INFO mapred.LocalJobRunner: Records R/W=1791/1\n","2021-11-10 13:22:41,204 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000002_0' done.\n","2021-11-10 13:22:41,210 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000002_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=14935\n","\t\tFILE: Number of bytes written=30780414\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=402665472\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=9\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=917646\n","\t\tMap output records=917646\n","\t\tMap output bytes=8258814\n","\t\tMap output materialized bytes=10094112\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=917646\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:22:41,210 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000002_0\n","2021-11-10 13:22:41,210 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000003_0\n","2021-11-10 13:22:41,212 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:22:41,212 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:22:41,212 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:22:41,214 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:402653184+134217728\n","2021-11-10 13:22:41,228 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:22:41,260 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:22:41,260 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:22:41,260 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:22:41,260 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:22:41,260 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:22:41,262 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:22:41,269 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:22:41,299 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:41,299 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:41,299 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:41,449 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:41,458 INFO streaming.PipeMapRed: Records R/W=1784/1\n","2021-11-10 13:22:41,521 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:42,223 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:42,984 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:22:43,753 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:22:44,466 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:22:45,211 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:22:45,950 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:22:46,712 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:22:47,451 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:22:48,159 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:22:48,282 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:22:48,288 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:22:48,289 INFO mapred.LocalJobRunner: \n","2021-11-10 13:22:48,289 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:22:48,289 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:22:48,289 INFO mapred.MapTask: bufstart = 0; bufend = 8221032; bufvoid = 104857600\n","2021-11-10 13:22:48,289 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22560608(90242432); length = 3653789/6553600\n","2021-11-10 13:22:48,625 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:22:48,631 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000003_0 is done. And is in the process of committing\n","2021-11-10 13:22:48,635 INFO mapreduce.Job:  map 5% reduce 0%\n","2021-11-10 13:22:48,640 INFO mapred.LocalJobRunner: Records R/W=1784/1\n","2021-11-10 13:22:48,640 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000003_0' done.\n","2021-11-10 13:22:48,640 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000003_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=19087\n","\t\tFILE: Number of bytes written=40828380\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=536887296\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=11\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=913448\n","\t\tMap output records=913448\n","\t\tMap output bytes=8221032\n","\t\tMap output materialized bytes=10047934\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=913448\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=8\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:22:48,641 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000003_0\n","2021-11-10 13:22:48,641 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000004_0\n","2021-11-10 13:22:48,644 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:22:48,644 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:22:48,644 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:22:48,646 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:536870912+134217728\n","2021-11-10 13:22:48,665 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:22:48,698 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:22:48,698 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:22:48,698 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:22:48,698 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:22:48,698 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:22:48,699 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:22:48,708 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:22:48,735 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:48,735 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:48,735 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:48,842 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:48,847 INFO streaming.PipeMapRed: Records R/W=1777/1\n","2021-11-10 13:22:48,920 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:49,582 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:49,636 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:22:50,350 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:22:51,105 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:22:51,846 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:22:52,580 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:22:53,313 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:22:54,046 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:22:54,781 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:22:55,484 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:22:55,666 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:22:55,666 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:22:55,667 INFO mapred.LocalJobRunner: \n","2021-11-10 13:22:55,667 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:22:55,667 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:22:55,667 INFO mapred.MapTask: bufstart = 0; bufend = 8298405; bufvoid = 104857600\n","2021-11-10 13:22:55,667 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22526220(90104880); length = 3688177/6553600\n","2021-11-10 13:22:55,990 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:22:55,995 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000004_0 is done. And is in the process of committing\n","2021-11-10 13:22:56,001 INFO mapred.LocalJobRunner: Records R/W=1777/1\n","2021-11-10 13:22:56,001 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000004_0' done.\n","2021-11-10 13:22:56,004 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000004_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=23239\n","\t\tFILE: Number of bytes written=50970913\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=671109120\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=13\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=922045\n","\t\tMap output records=922045\n","\t\tMap output bytes=8298405\n","\t\tMap output materialized bytes=10142501\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=922045\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:22:56,004 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000004_0\n","2021-11-10 13:22:56,004 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000005_0\n","2021-11-10 13:22:56,006 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:22:56,006 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:22:56,007 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:22:56,014 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:671088640+134217728\n","2021-11-10 13:22:56,036 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:22:56,060 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:22:56,060 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:22:56,060 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:22:56,060 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:22:56,061 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:22:56,065 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:22:56,073 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:22:56,107 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:56,107 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:56,107 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:56,219 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:56,230 INFO streaming.PipeMapRed: Records R/W=1847/1\n","2021-11-10 13:22:56,284 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:56,918 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:22:57,656 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:22:58,388 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:22:59,130 INFO streaming.PipeMapRed: R/W/S=400000/399457/0 in:133333=400000/3 [rec/s] out:133152=399457/3 [rec/s]\n","2021-11-10 13:22:59,861 INFO streaming.PipeMapRed: R/W/S=500000/498605/0 in:166666=500000/3 [rec/s] out:166209=498628/3 [rec/s]\n","2021-11-10 13:23:00,600 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:23:01,326 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:23:02,053 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:160000=800000/5 [rec/s] out:159783=798915/5 [rec/s]\n","2021-11-10 13:23:02,758 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:23:02,924 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:23:02,925 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:23:02,925 INFO mapred.LocalJobRunner: \n","2021-11-10 13:23:02,926 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:23:02,926 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:23:02,926 INFO mapred.MapTask: bufstart = 0; bufend = 8286867; bufvoid = 104857600\n","2021-11-10 13:23:02,926 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22531348(90125392); length = 3683049/6553600\n","2021-11-10 13:23:03,230 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:23:03,234 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000005_0 is done. And is in the process of committing\n","2021-11-10 13:23:03,237 INFO mapred.LocalJobRunner: Records R/W=1847/1\n","2021-11-10 13:23:03,237 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000005_0' done.\n","2021-11-10 13:23:03,238 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000005_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=27391\n","\t\tFILE: Number of bytes written=61099344\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=805330944\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=15\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=920763\n","\t\tMap output records=920763\n","\t\tMap output bytes=8286867\n","\t\tMap output materialized bytes=10128399\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=920763\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=6\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:23:03,238 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000005_0\n","2021-11-10 13:23:03,238 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000006_0\n","2021-11-10 13:23:03,247 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:23:03,247 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:23:03,248 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:23:03,251 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:805306368+134217728\n","2021-11-10 13:23:03,266 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:23:03,283 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:23:03,283 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:23:03,284 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:23:03,284 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:23:03,284 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:23:03,284 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:23:03,292 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:23:03,302 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:03,302 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:03,302 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:03,398 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:03,405 INFO streaming.PipeMapRed: Records R/W=1775/1\n","2021-11-10 13:23:03,462 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:04,095 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:04,827 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:23:05,594 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:23:06,325 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:23:07,071 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:23:07,798 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:23:08,563 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:23:09,303 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:23:10,054 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:23:10,198 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:23:10,199 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:23:10,200 INFO mapred.LocalJobRunner: \n","2021-11-10 13:23:10,200 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:23:10,200 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:23:10,200 INFO mapred.MapTask: bufstart = 0; bufend = 8267760; bufvoid = 104857600\n","2021-11-10 13:23:10,200 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22539840(90159360); length = 3674557/6553600\n","2021-11-10 13:23:10,545 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:23:10,550 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000006_0 is done. And is in the process of committing\n","2021-11-10 13:23:10,554 INFO mapred.LocalJobRunner: Records R/W=1775/1\n","2021-11-10 13:23:10,554 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000006_0' done.\n","2021-11-10 13:23:10,556 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000006_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=31543\n","\t\tFILE: Number of bytes written=71204422\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=939552768\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=17\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=918640\n","\t\tMap output records=918640\n","\t\tMap output bytes=8267760\n","\t\tMap output materialized bytes=10105046\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=918640\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=10\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:23:10,556 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000006_0\n","2021-11-10 13:23:10,556 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000007_0\n","2021-11-10 13:23:10,558 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:23:10,560 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:23:10,560 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:23:10,562 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:939524096+134217728\n","2021-11-10 13:23:10,583 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:23:10,600 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:23:10,600 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:23:10,600 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:23:10,600 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:23:10,600 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:23:10,601 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:23:10,607 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:23:10,617 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:10,617 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:10,618 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:10,712 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:10,720 INFO streaming.PipeMapRed: Records R/W=1765/1\n","2021-11-10 13:23:10,781 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:11,415 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:12,151 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:23:12,910 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:23:13,687 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:23:14,431 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:23:15,176 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:23:15,900 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:23:16,624 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:23:17,346 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:23:17,488 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:23:17,489 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:23:17,489 INFO mapred.LocalJobRunner: \n","2021-11-10 13:23:17,489 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:23:17,489 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:23:17,489 INFO mapred.MapTask: bufstart = 0; bufend = 8264340; bufvoid = 104857600\n","2021-11-10 13:23:17,489 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22541360(90165440); length = 3673037/6553600\n","2021-11-10 13:23:17,657 INFO mapreduce.Job:  map 12% reduce 0%\n","2021-11-10 13:23:17,809 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:23:17,812 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000007_0 is done. And is in the process of committing\n","2021-11-10 13:23:17,823 INFO mapred.LocalJobRunner: Records R/W=1765/1\n","2021-11-10 13:23:17,823 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000007_0' done.\n","2021-11-10 13:23:17,824 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000007_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=35695\n","\t\tFILE: Number of bytes written=81305320\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=1073774592\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=19\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=918260\n","\t\tMap output records=918260\n","\t\tMap output bytes=8264340\n","\t\tMap output materialized bytes=10100866\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=918260\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:23:17,824 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000007_0\n","2021-11-10 13:23:17,824 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000008_0\n","2021-11-10 13:23:17,826 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:23:17,826 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:23:17,826 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:23:17,827 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:1073741824+134217728\n","2021-11-10 13:23:17,836 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:23:17,858 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:23:17,858 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:23:17,858 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:23:17,858 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:23:17,858 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:23:17,859 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:23:17,869 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:23:17,894 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:17,894 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:17,894 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:17,988 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:18,001 INFO streaming.PipeMapRed: Records R/W=1853/1\n","2021-11-10 13:23:18,056 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:18,658 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:23:18,692 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:19,479 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:23:20,241 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:23:20,993 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:23:21,699 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:23:22,437 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:23:23,168 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:23:23,890 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:23:24,611 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:23:24,861 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:23:24,862 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:23:24,863 INFO mapred.LocalJobRunner: \n","2021-11-10 13:23:24,863 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:23:24,863 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:23:24,863 INFO mapred.MapTask: bufstart = 0; bufend = 8395713; bufvoid = 104857600\n","2021-11-10 13:23:24,863 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22482972(89931888); length = 3731425/6553600\n","2021-11-10 13:23:25,212 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:23:25,216 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000008_0 is done. And is in the process of committing\n","2021-11-10 13:23:25,220 INFO mapred.LocalJobRunner: Records R/W=1853/1\n","2021-11-10 13:23:25,220 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000008_0' done.\n","2021-11-10 13:23:25,221 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000008_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=39847\n","\t\tFILE: Number of bytes written=91566785\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=1207996416\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=21\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=932857\n","\t\tMap output records=932857\n","\t\tMap output bytes=8395713\n","\t\tMap output materialized bytes=10261433\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=932857\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=4\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:23:25,221 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000008_0\n","2021-11-10 13:23:25,221 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000009_0\n","2021-11-10 13:23:25,238 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:23:25,239 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:23:25,239 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:23:25,240 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:1207959552+134217728\n","2021-11-10 13:23:25,256 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:23:25,286 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:23:25,286 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:23:25,286 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:23:25,286 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:23:25,286 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:23:25,287 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:23:25,295 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:23:25,309 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:25,310 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:25,310 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:25,410 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:25,415 INFO streaming.PipeMapRed: Records R/W=1718/1\n","2021-11-10 13:23:25,481 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:26,152 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:26,909 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:23:27,677 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:23:28,427 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:23:29,189 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:23:29,898 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:23:30,612 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:23:31,345 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:23:32,088 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:23:32,090 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:23:32,091 INFO mapred.LocalJobRunner: \n","2021-11-10 13:23:32,091 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:23:32,091 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:23:32,091 INFO mapred.MapTask: bufstart = 0; bufend = 8080515; bufvoid = 104857600\n","2021-11-10 13:23:32,091 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22623060(90492240); length = 3591337/6553600\n","2021-11-10 13:23:32,408 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:23:32,411 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000009_0 is done. And is in the process of committing\n","2021-11-10 13:23:32,415 INFO mapred.LocalJobRunner: Records R/W=1718/1\n","2021-11-10 13:23:32,415 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000009_0' done.\n","2021-11-10 13:23:32,416 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000009_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=43999\n","\t\tFILE: Number of bytes written=101443008\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=1342218240\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=23\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=897835\n","\t\tMap output records=897835\n","\t\tMap output bytes=8080515\n","\t\tMap output materialized bytes=9876191\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=897835\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=11\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:23:32,416 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000009_0\n","2021-11-10 13:23:32,416 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000010_0\n","2021-11-10 13:23:32,418 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:23:32,418 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:23:32,418 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:23:32,419 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:1342177280+134217728\n","2021-11-10 13:23:32,452 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:23:32,469 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:23:32,469 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:23:32,469 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:23:32,469 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:23:32,470 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:23:32,472 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:23:32,478 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:23:32,487 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:32,487 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:32,487 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:32,572 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:32,577 INFO streaming.PipeMapRed: Records R/W=1697/1\n","2021-11-10 13:23:32,632 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:33,323 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:34,053 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:23:34,776 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:23:35,469 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:200000=400000/2 [rec/s] out:199216=398433/2 [rec/s]\n","2021-11-10 13:23:36,179 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:23:36,899 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:23:37,633 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:23:38,352 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:160000=800000/5 [rec/s] out:159783=798915/5 [rec/s]\n","2021-11-10 13:23:38,429 INFO mapred.LocalJobRunner: Records R/W=1697/1 > map\n","2021-11-10 13:23:38,674 INFO mapreduce.Job:  map 18% reduce 0%\n","2021-11-10 13:23:39,048 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:23:39,135 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:23:39,135 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:23:39,136 INFO mapred.LocalJobRunner: Records R/W=1697/1 > map\n","2021-11-10 13:23:39,137 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:23:39,137 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:23:39,137 INFO mapred.MapTask: bufstart = 0; bufend = 8188371; bufvoid = 104857600\n","2021-11-10 13:23:39,137 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22575124(90300496); length = 3639273/6553600\n","2021-11-10 13:23:39,447 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:23:39,450 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000010_0 is done. And is in the process of committing\n","2021-11-10 13:23:39,455 INFO mapred.LocalJobRunner: Records R/W=1697/1\n","2021-11-10 13:23:39,455 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000010_0' done.\n","2021-11-10 13:23:39,456 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000010_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=48151\n","\t\tFILE: Number of bytes written=111451055\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=1476440064\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=25\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=909819\n","\t\tMap output records=909819\n","\t\tMap output bytes=8188371\n","\t\tMap output materialized bytes=10008015\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=909819\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:23:39,456 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000010_0\n","2021-11-10 13:23:39,456 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000011_0\n","2021-11-10 13:23:39,458 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:23:39,458 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:23:39,458 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:23:39,459 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:1476395008+134217728\n","2021-11-10 13:23:39,478 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:23:39,514 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:23:39,514 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:23:39,514 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:23:39,514 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:23:39,514 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:23:39,515 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:23:39,526 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:23:39,549 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:39,549 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:39,549 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:39,656 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:39,662 INFO streaming.PipeMapRed: Records R/W=1806/1\n","2021-11-10 13:23:39,675 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:23:39,727 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:40,439 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:41,181 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:23:41,946 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:23:42,704 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:23:43,518 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:23:44,248 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:23:44,999 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:23:45,466 INFO mapred.LocalJobRunner: Records R/W=1806/1 > map\n","2021-11-10 13:23:45,687 INFO mapreduce.Job:  map 20% reduce 0%\n","2021-11-10 13:23:45,710 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:23:46,428 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:23:46,491 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:23:46,492 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:23:46,495 INFO mapred.LocalJobRunner: Records R/W=1806/1 > map\n","2021-11-10 13:23:46,495 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:23:46,495 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:23:46,495 INFO mapred.MapTask: bufstart = 0; bufend = 8139159; bufvoid = 104857600\n","2021-11-10 13:23:46,495 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22596996(90387984); length = 3617401/6553600\n","2021-11-10 13:23:46,813 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:23:46,816 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000011_0 is done. And is in the process of committing\n","2021-11-10 13:23:46,821 INFO mapred.LocalJobRunner: Records R/W=1806/1\n","2021-11-10 13:23:46,821 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000011_0' done.\n","2021-11-10 13:23:46,822 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000011_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=52303\n","\t\tFILE: Number of bytes written=121398954\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=1610661888\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=27\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=904351\n","\t\tMap output records=904351\n","\t\tMap output bytes=8139159\n","\t\tMap output materialized bytes=9947867\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=904351\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:23:46,822 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000011_0\n","2021-11-10 13:23:46,822 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000012_0\n","2021-11-10 13:23:46,824 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:23:46,824 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:23:46,824 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:23:46,826 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:1610612736+134217728\n","2021-11-10 13:23:46,850 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:23:46,881 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:23:46,881 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:23:46,881 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:23:46,881 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:23:46,881 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:23:46,884 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:23:46,893 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:23:46,905 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:46,905 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:46,905 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:47,000 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:47,006 INFO streaming.PipeMapRed: Records R/W=1835/1\n","2021-11-10 13:23:47,061 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:47,694 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:23:47,760 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:48,491 INFO streaming.PipeMapRed: R/W/S=200000/197680/0 in:200000=200000/1 [rec/s] out:197680=197680/1 [rec/s]\n","2021-11-10 13:23:49,198 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:23:49,932 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:23:50,641 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:23:51,365 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:23:52,110 INFO streaming.PipeMapRed: R/W/S=700000/699562/0 in:140000=700000/5 [rec/s] out:139912=699562/5 [rec/s]\n","2021-11-10 13:23:52,832 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:160000=800000/5 [rec/s] out:159578=797890/5 [rec/s]\n","2021-11-10 13:23:53,567 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:23:53,789 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:23:53,789 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:23:53,790 INFO mapred.LocalJobRunner: \n","2021-11-10 13:23:53,790 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:23:53,790 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:23:53,790 INFO mapred.MapTask: bufstart = 0; bufend = 8377884; bufvoid = 104857600\n","2021-11-10 13:23:53,790 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22490896(89963584); length = 3723501/6553600\n","2021-11-10 13:23:54,104 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:23:54,108 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000012_0 is done. And is in the process of committing\n","2021-11-10 13:23:54,113 INFO mapred.LocalJobRunner: Records R/W=1835/1\n","2021-11-10 13:23:54,114 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000012_0' done.\n","2021-11-10 13:23:54,114 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000012_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=56455\n","\t\tFILE: Number of bytes written=131638628\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=1744883712\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=29\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=930876\n","\t\tMap output records=930876\n","\t\tMap output bytes=8377884\n","\t\tMap output materialized bytes=10239642\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=930876\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=9\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:23:54,114 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000012_0\n","2021-11-10 13:23:54,115 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000013_0\n","2021-11-10 13:23:54,117 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:23:54,117 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:23:54,117 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:23:54,118 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:1744830464+134217728\n","2021-11-10 13:23:54,147 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:23:54,164 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:23:54,164 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:23:54,164 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:23:54,164 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:23:54,164 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:23:54,165 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:23:54,173 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:23:54,180 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:54,180 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:54,180 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:54,271 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:54,276 INFO streaming.PipeMapRed: Records R/W=1837/1\n","2021-11-10 13:23:54,330 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:54,988 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:23:55,732 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:23:56,459 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:23:57,184 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:23:57,911 INFO streaming.PipeMapRed: R/W/S=500000/497785/0 in:166666=500000/3 [rec/s] out:165928=497785/3 [rec/s]\n","2021-11-10 13:23:58,636 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:23:59,383 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:24:00,122 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:160000=800000/5 [rec/s] out:159783=798915/5 [rec/s]\n","2021-11-10 13:24:00,830 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:24:01,034 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:24:01,035 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:24:01,037 INFO mapred.LocalJobRunner: \n","2021-11-10 13:24:01,037 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:24:01,037 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:24:01,037 INFO mapred.MapTask: bufstart = 0; bufend = 8345088; bufvoid = 104857600\n","2021-11-10 13:24:01,037 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22505472(90021888); length = 3708925/6553600\n","2021-11-10 13:24:01,359 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:24:01,362 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000013_0 is done. And is in the process of committing\n","2021-11-10 13:24:01,367 INFO mapred.LocalJobRunner: Records R/W=1837/1\n","2021-11-10 13:24:01,367 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000013_0' done.\n","2021-11-10 13:24:01,368 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000013_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=60607\n","\t\tFILE: Number of bytes written=141838218\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=1879105536\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=31\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=927232\n","\t\tMap output records=927232\n","\t\tMap output bytes=8345088\n","\t\tMap output materialized bytes=10199558\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=927232\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:24:01,368 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000013_0\n","2021-11-10 13:24:01,368 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000014_0\n","2021-11-10 13:24:01,370 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:24:01,370 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:24:01,370 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:24:01,371 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:1879048192+134217728\n","2021-11-10 13:24:01,392 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:24:01,414 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:24:01,414 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:24:01,414 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:24:01,414 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:24:01,414 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:24:01,420 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:24:01,429 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:24:01,464 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:01,464 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:01,464 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:01,555 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:01,561 INFO streaming.PipeMapRed: Records R/W=1795/1\n","2021-11-10 13:24:01,620 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:02,278 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:03,018 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:24:03,780 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:24:04,518 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:24:05,300 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:24:06,060 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:24:06,784 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:24:07,386 INFO mapred.LocalJobRunner: Records R/W=1795/1 > map\n","2021-11-10 13:24:07,603 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:24:07,712 INFO mapreduce.Job:  map 25% reduce 0%\n","2021-11-10 13:24:08,351 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:24:08,405 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:24:08,406 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:24:08,408 INFO mapred.LocalJobRunner: Records R/W=1795/1 > map\n","2021-11-10 13:24:08,408 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:24:08,408 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:24:08,408 INFO mapred.MapTask: bufstart = 0; bufend = 8148591; bufvoid = 104857600\n","2021-11-10 13:24:08,408 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22592804(90371216); length = 3621593/6553600\n","2021-11-10 13:24:08,746 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:24:08,749 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000014_0 is done. And is in the process of committing\n","2021-11-10 13:24:08,753 INFO mapred.LocalJobRunner: Records R/W=1795/1\n","2021-11-10 13:24:08,753 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000014_0' done.\n","2021-11-10 13:24:08,754 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000014_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=64759\n","\t\tFILE: Number of bytes written=151797645\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2013327360\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=33\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=905399\n","\t\tMap output records=905399\n","\t\tMap output bytes=8148591\n","\t\tMap output materialized bytes=9959395\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=905399\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:24:08,754 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000014_0\n","2021-11-10 13:24:08,754 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000015_0\n","2021-11-10 13:24:08,756 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:24:08,756 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:24:08,757 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:24:08,759 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:2013265920+134217728\n","2021-11-10 13:24:08,784 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:24:08,816 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:24:08,816 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:24:08,816 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:24:08,816 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:24:08,816 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:24:08,817 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:24:08,826 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:24:08,838 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:08,838 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:08,838 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:08,954 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:08,959 INFO streaming.PipeMapRed: Records R/W=1668/1\n","2021-11-10 13:24:09,020 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:09,697 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:09,713 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:24:10,532 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:24:11,281 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:24:12,080 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:24:12,811 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:24:13,559 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:24:14,318 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:24:15,121 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:24:15,831 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:24:15,914 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:24:15,916 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:24:15,916 INFO mapred.LocalJobRunner: \n","2021-11-10 13:24:15,916 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:24:15,916 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:24:15,916 INFO mapred.MapTask: bufstart = 0; bufend = 8180037; bufvoid = 104857600\n","2021-11-10 13:24:15,916 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22578828(90315312); length = 3635569/6553600\n","2021-11-10 13:24:16,271 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:24:16,276 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000015_0 is done. And is in the process of committing\n","2021-11-10 13:24:16,281 INFO mapred.LocalJobRunner: Records R/W=1668/1\n","2021-11-10 13:24:16,281 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000015_0' done.\n","2021-11-10 13:24:16,281 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000015_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=68911\n","\t\tFILE: Number of bytes written=161795506\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2147549184\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=35\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=908893\n","\t\tMap output records=908893\n","\t\tMap output bytes=8180037\n","\t\tMap output materialized bytes=9997829\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=908893\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=13\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:24:16,281 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000015_0\n","2021-11-10 13:24:16,281 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000016_0\n","2021-11-10 13:24:16,284 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:24:16,284 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:24:16,285 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:24:16,290 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:2147483648+134217728\n","2021-11-10 13:24:16,319 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:24:16,350 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:24:16,350 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:24:16,350 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:24:16,350 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:24:16,350 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:24:16,351 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:24:16,359 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:24:16,367 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:16,367 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:16,367 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:16,472 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:16,477 INFO streaming.PipeMapRed: Records R/W=1849/1\n","2021-11-10 13:24:16,542 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:17,250 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:18,027 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:24:18,784 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:24:19,554 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:24:20,349 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:24:21,099 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:24:21,890 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:24:22,681 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:24:23,451 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:128571=900000/7 [rec/s] out:128323=898267/7 [rec/s]\n","2021-11-10 13:24:23,537 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:24:23,537 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:24:23,538 INFO mapred.LocalJobRunner: \n","2021-11-10 13:24:23,539 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:24:23,539 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:24:23,539 INFO mapred.MapTask: bufstart = 0; bufend = 8183727; bufvoid = 104857600\n","2021-11-10 13:24:23,539 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22577188(90308752); length = 3637209/6553600\n","2021-11-10 13:24:23,725 INFO mapreduce.Job:  map 27% reduce 0%\n","2021-11-10 13:24:23,877 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:24:23,882 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000016_0 is done. And is in the process of committing\n","2021-11-10 13:24:23,886 INFO mapred.LocalJobRunner: Records R/W=1849/1\n","2021-11-10 13:24:23,886 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000016_0' done.\n","2021-11-10 13:24:23,887 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000016_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=73063\n","\t\tFILE: Number of bytes written=171797877\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2281771008\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=37\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=909303\n","\t\tMap output records=909303\n","\t\tMap output bytes=8183727\n","\t\tMap output materialized bytes=10002339\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=909303\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:24:23,887 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000016_0\n","2021-11-10 13:24:23,887 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000017_0\n","2021-11-10 13:24:23,889 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:24:23,889 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:24:23,890 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:24:23,890 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:2281701376+134217728\n","2021-11-10 13:24:23,912 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:24:23,949 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:24:23,949 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:24:23,949 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:24:23,949 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:24:23,949 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:24:23,951 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:24:23,977 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:24:24,018 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:24,018 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:24,018 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:24,120 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:24,128 INFO streaming.PipeMapRed: Records R/W=1725/1\n","2021-11-10 13:24:24,190 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:24,726 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:24:24,882 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:25,671 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:24:26,435 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:24:27,211 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:24:27,986 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:24:28,742 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:24:29,477 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:24:30,249 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:24:30,971 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:24:31,128 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:24:31,129 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:24:31,129 INFO mapred.LocalJobRunner: \n","2021-11-10 13:24:31,129 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:24:31,129 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:24:31,129 INFO mapred.MapTask: bufstart = 0; bufend = 8281224; bufvoid = 104857600\n","2021-11-10 13:24:31,129 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22533856(90135424); length = 3680541/6553600\n","2021-11-10 13:24:31,463 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:24:31,467 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000017_0 is done. And is in the process of committing\n","2021-11-10 13:24:31,472 INFO mapred.LocalJobRunner: Records R/W=1725/1\n","2021-11-10 13:24:31,472 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000017_0' done.\n","2021-11-10 13:24:31,472 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000017_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=77215\n","\t\tFILE: Number of bytes written=181919411\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2415992832\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=39\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=920136\n","\t\tMap output records=920136\n","\t\tMap output bytes=8281224\n","\t\tMap output materialized bytes=10121502\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=920136\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:24:31,472 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000017_0\n","2021-11-10 13:24:31,473 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000018_0\n","2021-11-10 13:24:31,474 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:24:31,474 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:24:31,475 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:24:31,475 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:2415919104+134217728\n","2021-11-10 13:24:31,512 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:24:31,542 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:24:31,542 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:24:31,542 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:24:31,542 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:24:31,542 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:24:31,544 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:24:31,551 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:24:31,559 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:31,559 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:31,559 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:31,655 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:31,661 INFO streaming.PipeMapRed: Records R/W=1699/1\n","2021-11-10 13:24:31,730 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:32,425 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:33,142 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:24:33,912 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:24:34,671 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:24:35,401 INFO streaming.PipeMapRed: R/W/S=500000/497785/0 in:166666=500000/3 [rec/s] out:165928=497785/3 [rec/s]\n","2021-11-10 13:24:36,205 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:24:36,963 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:24:37,757 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:24:38,501 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:24:38,657 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:24:38,658 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:24:38,658 INFO mapred.LocalJobRunner: \n","2021-11-10 13:24:38,658 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:24:38,658 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:24:38,658 INFO mapred.MapTask: bufstart = 0; bufend = 8255493; bufvoid = 104857600\n","2021-11-10 13:24:38,658 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22545292(90181168); length = 3669105/6553600\n","2021-11-10 13:24:38,736 INFO mapreduce.Job:  map 31% reduce 0%\n","2021-11-10 13:24:39,010 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:24:39,014 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000018_0 is done. And is in the process of committing\n","2021-11-10 13:24:39,017 INFO mapred.LocalJobRunner: Records R/W=1699/1\n","2021-11-10 13:24:39,017 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000018_0' done.\n","2021-11-10 13:24:39,018 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000018_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=81367\n","\t\tFILE: Number of bytes written=192009496\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2550214656\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=41\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=917277\n","\t\tMap output records=917277\n","\t\tMap output bytes=8255493\n","\t\tMap output materialized bytes=10090053\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=917277\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=19\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:24:39,018 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000018_0\n","2021-11-10 13:24:39,018 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000019_0\n","2021-11-10 13:24:39,026 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:24:39,026 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:24:39,030 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:24:39,033 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:2550136832+134217728\n","2021-11-10 13:24:39,056 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:24:39,090 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:24:39,090 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:24:39,090 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:24:39,090 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:24:39,090 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:24:39,091 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:24:39,100 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:24:39,108 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:39,108 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:39,108 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:39,208 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:39,213 INFO streaming.PipeMapRed: Records R/W=1841/1\n","2021-11-10 13:24:39,276 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:39,736 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:24:39,988 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:40,763 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:24:41,509 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:24:42,277 INFO streaming.PipeMapRed: R/W/S=400000/399457/0 in:133333=400000/3 [rec/s] out:133152=399457/3 [rec/s]\n","2021-11-10 13:24:43,015 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:24:43,782 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:24:44,552 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:24:45,256 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:24:46,083 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:24:46,272 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:24:46,273 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:24:46,273 INFO mapred.LocalJobRunner: \n","2021-11-10 13:24:46,273 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:24:46,274 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:24:46,274 INFO mapred.MapTask: bufstart = 0; bufend = 8316729; bufvoid = 104857600\n","2021-11-10 13:24:46,274 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22518076(90072304); length = 3696321/6553600\n","2021-11-10 13:24:46,628 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:24:46,632 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000019_0 is done. And is in the process of committing\n","2021-11-10 13:24:46,637 INFO mapred.LocalJobRunner: Records R/W=1841/1\n","2021-11-10 13:24:46,637 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000019_0' done.\n","2021-11-10 13:24:46,637 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000019_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=85519\n","\t\tFILE: Number of bytes written=202174425\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2684436480\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=43\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=924081\n","\t\tMap output records=924081\n","\t\tMap output bytes=8316729\n","\t\tMap output materialized bytes=10164897\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=924081\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:24:46,637 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000019_0\n","2021-11-10 13:24:46,637 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000020_0\n","2021-11-10 13:24:46,639 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:24:46,639 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:24:46,639 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:24:46,641 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:2684354560+134217728\n","2021-11-10 13:24:46,656 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:24:46,693 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:24:46,693 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:24:46,693 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:24:46,693 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:24:46,693 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:24:46,702 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:24:46,713 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:24:46,740 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:46,740 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:46,740 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:46,864 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:46,870 INFO streaming.PipeMapRed: Records R/W=1790/1\n","2021-11-10 13:24:46,932 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:47,594 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:48,385 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:24:49,148 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:24:49,900 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:24:50,672 INFO streaming.PipeMapRed: R/W/S=500000/497785/0 in:166666=500000/3 [rec/s] out:165928=497785/3 [rec/s]\n","2021-11-10 13:24:51,424 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:24:52,163 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:24:52,892 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:24:53,593 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:24:53,641 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:24:53,645 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:24:53,646 INFO mapred.LocalJobRunner: \n","2021-11-10 13:24:53,646 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:24:53,646 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:24:53,646 INFO mapred.MapTask: bufstart = 0; bufend = 8132886; bufvoid = 104857600\n","2021-11-10 13:24:53,646 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22599784(90399136); length = 3614613/6553600\n","2021-11-10 13:24:53,749 INFO mapreduce.Job:  map 34% reduce 0%\n","2021-11-10 13:24:53,964 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:24:53,966 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000020_0 is done. And is in the process of committing\n","2021-11-10 13:24:53,970 INFO mapred.LocalJobRunner: Records R/W=1790/1\n","2021-11-10 13:24:53,970 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000020_0' done.\n","2021-11-10 13:24:53,973 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000020_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=89671\n","\t\tFILE: Number of bytes written=212114657\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2818658304\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=45\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=903654\n","\t\tMap output records=903654\n","\t\tMap output bytes=8132886\n","\t\tMap output materialized bytes=9940200\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=903654\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:24:53,973 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000020_0\n","2021-11-10 13:24:53,973 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000021_0\n","2021-11-10 13:24:53,975 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:24:53,975 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:24:53,975 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:24:53,976 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:2818572288+134217728\n","2021-11-10 13:24:53,994 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:24:54,014 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:24:54,014 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:24:54,014 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:24:54,014 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:24:54,014 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:24:54,016 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:24:54,025 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:24:54,032 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:54,032 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:54,032 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:54,130 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:54,135 INFO streaming.PipeMapRed: Records R/W=1799/1\n","2021-11-10 13:24:54,199 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:54,749 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:24:54,866 INFO streaming.PipeMapRed: R/W/S=100000/98820/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:24:55,608 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:24:56,345 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:24:57,094 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:24:57,834 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:24:58,595 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:24:59,340 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:24:59,980 INFO mapred.LocalJobRunner: Records R/W=1799/1 > map\n","2021-11-10 13:25:00,125 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:25:00,753 INFO mapreduce.Job:  map 37% reduce 0%\n","2021-11-10 13:25:00,826 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:25:00,827 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:25:00,827 INFO mapred.LocalJobRunner: Records R/W=1799/1 > map\n","2021-11-10 13:25:00,828 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:25:00,828 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:25:00,828 INFO mapred.MapTask: bufstart = 0; bufend = 8064414; bufvoid = 104857600\n","2021-11-10 13:25:00,828 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22630216(90520864); length = 3584181/6553600\n","2021-11-10 13:25:01,154 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:25:01,160 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000021_0 is done. And is in the process of committing\n","2021-11-10 13:25:01,166 INFO mapred.LocalJobRunner: Records R/W=1799/1\n","2021-11-10 13:25:01,166 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000021_0' done.\n","2021-11-10 13:25:01,167 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000021_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=93527\n","\t\tFILE: Number of bytes written=221971201\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2952880128\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=47\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=896046\n","\t\tMap output records=896046\n","\t\tMap output bytes=8064414\n","\t\tMap output materialized bytes=9856512\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=896046\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=8\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:25:01,167 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000021_0\n","2021-11-10 13:25:01,167 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000022_0\n","2021-11-10 13:25:01,168 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:25:01,168 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:25:01,168 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:25:01,169 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:2952790016+134217728\n","2021-11-10 13:25:01,185 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:25:01,202 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:25:01,202 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:25:01,202 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:25:01,202 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:25:01,202 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:25:01,203 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:25:01,211 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:25:01,222 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:01,222 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:01,222 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:01,311 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:01,316 INFO streaming.PipeMapRed: Records R/W=1858/1\n","2021-11-10 13:25:01,375 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:01,753 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:25:02,011 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:02,741 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:25:03,488 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:25:04,248 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:25:05,008 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:25:05,795 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:25:06,553 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:25:07,322 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:25:08,036 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:25:08,152 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:25:08,153 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:25:08,154 INFO mapred.LocalJobRunner: \n","2021-11-10 13:25:08,154 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:25:08,154 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:25:08,154 INFO mapred.MapTask: bufstart = 0; bufend = 8211168; bufvoid = 104857600\n","2021-11-10 13:25:08,154 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22564992(90259968); length = 3649405/6553600\n","2021-11-10 13:25:08,495 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:25:08,501 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000022_0 is done. And is in the process of committing\n","2021-11-10 13:25:08,507 INFO mapred.LocalJobRunner: Records R/W=1858/1\n","2021-11-10 13:25:08,507 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000022_0' done.\n","2021-11-10 13:25:08,508 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000022_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=97383\n","\t\tFILE: Number of bytes written=232007111\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3087101952\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=49\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=912352\n","\t\tMap output records=912352\n","\t\tMap output bytes=8211168\n","\t\tMap output materialized bytes=10035878\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=912352\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:25:08,508 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000022_0\n","2021-11-10 13:25:08,508 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000023_0\n","2021-11-10 13:25:08,519 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:25:08,519 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:25:08,520 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:25:08,521 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:3087007744+134217728\n","2021-11-10 13:25:08,549 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:25:08,585 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:25:08,585 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:25:08,585 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:25:08,585 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:25:08,585 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:25:08,586 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:25:08,597 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:25:08,641 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:08,641 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:08,641 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:08,748 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:08,754 INFO streaming.PipeMapRed: Records R/W=1872/1\n","2021-11-10 13:25:08,822 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:09,477 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:10,231 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:25:10,974 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:25:11,735 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:25:12,474 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:25:13,232 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:25:14,011 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:25:14,750 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:25:15,460 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:25:15,490 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:25:15,490 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:25:15,491 INFO mapred.LocalJobRunner: \n","2021-11-10 13:25:15,491 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:25:15,491 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:25:15,491 INFO mapred.MapTask: bufstart = 0; bufend = 8112204; bufvoid = 104857600\n","2021-11-10 13:25:15,491 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22608976(90435904); length = 3605421/6553600\n","2021-11-10 13:25:15,761 INFO mapreduce.Job:  map 39% reduce 0%\n","2021-11-10 13:25:15,827 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:25:15,832 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000023_0 is done. And is in the process of committing\n","2021-11-10 13:25:15,836 INFO mapred.LocalJobRunner: Records R/W=1872/1\n","2021-11-10 13:25:15,836 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000023_0' done.\n","2021-11-10 13:25:15,836 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000023_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=101239\n","\t\tFILE: Number of bytes written=241922065\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3221323776\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=51\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=901356\n","\t\tMap output records=901356\n","\t\tMap output bytes=8112204\n","\t\tMap output materialized bytes=9914922\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=901356\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:25:15,836 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000023_0\n","2021-11-10 13:25:15,837 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000024_0\n","2021-11-10 13:25:15,838 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:25:15,838 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:25:15,838 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:25:15,839 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:3221225472+134217728\n","2021-11-10 13:25:15,857 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:25:15,889 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:25:15,889 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:25:15,889 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:25:15,889 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:25:15,889 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:25:15,890 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:25:15,902 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:25:15,917 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:15,917 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:15,917 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:16,028 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:16,033 INFO streaming.PipeMapRed: Records R/W=1785/1\n","2021-11-10 13:25:16,093 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:16,762 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:25:16,831 INFO streaming.PipeMapRed: R/W/S=100000/98510/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:17,592 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:25:18,310 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:25:19,090 INFO streaming.PipeMapRed: R/W/S=400000/399457/0 in:133333=400000/3 [rec/s] out:133152=399457/3 [rec/s]\n","2021-11-10 13:25:19,866 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:25:20,621 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:25:21,381 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:25:22,155 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:25:22,877 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:25:22,898 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:25:22,899 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:25:22,900 INFO mapred.LocalJobRunner: \n","2021-11-10 13:25:22,900 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:25:22,901 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:25:22,901 INFO mapred.MapTask: bufstart = 0; bufend = 8103717; bufvoid = 104857600\n","2021-11-10 13:25:22,901 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22612748(90450992); length = 3601649/6553600\n","2021-11-10 13:25:23,265 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:25:23,270 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000024_0 is done. And is in the process of committing\n","2021-11-10 13:25:23,274 INFO mapred.LocalJobRunner: Records R/W=1785/1\n","2021-11-10 13:25:23,274 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000024_0' done.\n","2021-11-10 13:25:23,275 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000024_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=105095\n","\t\tFILE: Number of bytes written=251826646\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3355545600\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=53\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=900413\n","\t\tMap output records=900413\n","\t\tMap output bytes=8103717\n","\t\tMap output materialized bytes=9904549\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=900413\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=6\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:25:23,275 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000024_0\n","2021-11-10 13:25:23,275 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000025_0\n","2021-11-10 13:25:23,280 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:25:23,280 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:25:23,280 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:25:23,281 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:3355443200+134217728\n","2021-11-10 13:25:23,312 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:25:23,343 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:25:23,343 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:25:23,343 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:25:23,343 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:25:23,343 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:25:23,344 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:25:23,352 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:25:23,369 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:23,369 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:23,369 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:23,488 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:23,494 INFO streaming.PipeMapRed: Records R/W=1729/1\n","2021-11-10 13:25:23,585 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:24,281 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:25,063 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:25:25,827 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:25:26,569 INFO streaming.PipeMapRed: R/W/S=400000/399457/0 in:133333=400000/3 [rec/s] out:133152=399457/3 [rec/s]\n","2021-11-10 13:25:27,323 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:25:28,067 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:25:28,832 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:25:29,556 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:25:30,294 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:25:30,470 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:25:30,471 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:25:30,471 INFO mapred.LocalJobRunner: \n","2021-11-10 13:25:30,472 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:25:30,472 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:25:30,472 INFO mapred.MapTask: bufstart = 0; bufend = 8286966; bufvoid = 104857600\n","2021-11-10 13:25:30,472 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22531304(90125216); length = 3683093/6553600\n","2021-11-10 13:25:30,771 INFO mapreduce.Job:  map 42% reduce 0%\n","2021-11-10 13:25:30,807 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:25:30,810 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000025_0 is done. And is in the process of committing\n","2021-11-10 13:25:30,815 INFO mapred.LocalJobRunner: Records R/W=1729/1\n","2021-11-10 13:25:30,815 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000025_0' done.\n","2021-11-10 13:25:30,815 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000025_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=108951\n","\t\tFILE: Number of bytes written=261955198\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3489767424\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=55\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=920774\n","\t\tMap output records=920774\n","\t\tMap output bytes=8286966\n","\t\tMap output materialized bytes=10128520\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=920774\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:25:30,815 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000025_0\n","2021-11-10 13:25:30,815 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000026_0\n","2021-11-10 13:25:30,821 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:25:30,821 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:25:30,821 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:25:30,822 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:3489660928+134217728\n","2021-11-10 13:25:30,840 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:25:30,874 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:25:30,874 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:25:30,874 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:25:30,874 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:25:30,874 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:25:30,875 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:25:30,895 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:25:30,920 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:30,920 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:30,920 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:31,048 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:31,054 INFO streaming.PipeMapRed: Records R/W=1881/1\n","2021-11-10 13:25:31,111 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:31,772 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:25:31,809 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:32,560 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:25:33,281 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:25:34,046 INFO streaming.PipeMapRed: R/W/S=400000/398839/0 in:133333=400000/3 [rec/s] out:132951=398855/3 [rec/s]\n","2021-11-10 13:25:34,849 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:25:35,597 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:25:36,359 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:25:37,126 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:25:37,872 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:25:37,993 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:25:37,993 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:25:37,994 INFO mapred.LocalJobRunner: \n","2021-11-10 13:25:37,994 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:25:37,994 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:25:37,994 INFO mapred.MapTask: bufstart = 0; bufend = 8221185; bufvoid = 104857600\n","2021-11-10 13:25:37,994 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22560540(90242160); length = 3653857/6553600\n","2021-11-10 13:25:38,316 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:25:38,319 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000026_0 is done. And is in the process of committing\n","2021-11-10 13:25:38,323 INFO mapred.LocalJobRunner: Records R/W=1881/1\n","2021-11-10 13:25:38,323 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000026_0' done.\n","2021-11-10 13:25:38,323 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000026_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=112295\n","\t\tFILE: Number of bytes written=272003351\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3623989248\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=57\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=913465\n","\t\tMap output records=913465\n","\t\tMap output bytes=8221185\n","\t\tMap output materialized bytes=10048121\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=913465\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:25:38,324 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000026_0\n","2021-11-10 13:25:38,324 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000027_0\n","2021-11-10 13:25:38,326 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:25:38,326 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:25:38,327 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:25:38,328 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:3623878656+134217728\n","2021-11-10 13:25:38,340 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:25:38,370 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:25:38,370 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:25:38,370 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:25:38,370 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:25:38,370 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:25:38,371 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:25:38,380 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:25:38,387 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:38,387 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:38,387 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:38,489 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:38,494 INFO streaming.PipeMapRed: Records R/W=1836/1\n","2021-11-10 13:25:38,554 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:39,248 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:40,016 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:25:40,756 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:25:41,497 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:25:42,258 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:25:42,973 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:25:43,769 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:25:44,512 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:25:45,259 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:25:45,490 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:25:45,491 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:25:45,491 INFO mapred.LocalJobRunner: \n","2021-11-10 13:25:45,491 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:25:45,491 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:25:45,491 INFO mapred.MapTask: bufstart = 0; bufend = 8392392; bufvoid = 104857600\n","2021-11-10 13:25:45,491 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22484448(89937792); length = 3729949/6553600\n","2021-11-10 13:25:45,789 INFO mapreduce.Job:  map 46% reduce 0%\n","2021-11-10 13:25:45,827 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:25:45,831 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000027_0 is done. And is in the process of committing\n","2021-11-10 13:25:45,838 INFO mapred.LocalJobRunner: Records R/W=1836/1\n","2021-11-10 13:25:45,838 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000027_0' done.\n","2021-11-10 13:25:45,838 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000027_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=115639\n","\t\tFILE: Number of bytes written=282260757\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3758211072\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=59\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=932488\n","\t\tMap output records=932488\n","\t\tMap output bytes=8392392\n","\t\tMap output materialized bytes=10257374\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=932488\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=10\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:25:45,838 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000027_0\n","2021-11-10 13:25:45,838 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000028_0\n","2021-11-10 13:25:45,846 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:25:45,846 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:25:45,846 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:25:45,847 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:3758096384+134217728\n","2021-11-10 13:25:45,860 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:25:45,877 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:25:45,877 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:25:45,877 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:25:45,877 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:25:45,877 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:25:45,878 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:25:45,884 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:25:45,892 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:45,892 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:45,892 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:45,980 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:45,987 INFO streaming.PipeMapRed: Records R/W=1813/1\n","2021-11-10 13:25:46,059 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:46,728 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:46,789 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:25:47,515 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:25:48,285 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:25:49,027 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:25:49,729 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:25:50,485 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:25:51,216 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:25:51,951 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:25:52,760 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:25:52,979 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:25:52,981 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:25:52,981 INFO mapred.LocalJobRunner: \n","2021-11-10 13:25:52,981 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:25:52,981 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:25:52,981 INFO mapred.MapTask: bufstart = 0; bufend = 8348490; bufvoid = 104857600\n","2021-11-10 13:25:52,981 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22503960(90015840); length = 3710437/6553600\n","2021-11-10 13:25:53,326 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:25:53,329 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000028_0 is done. And is in the process of committing\n","2021-11-10 13:25:53,335 INFO mapred.LocalJobRunner: Records R/W=1813/1\n","2021-11-10 13:25:53,335 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000028_0' done.\n","2021-11-10 13:25:53,335 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000028_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=118983\n","\t\tFILE: Number of bytes written=292464505\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=3892432896\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=61\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=927610\n","\t\tMap output records=927610\n","\t\tMap output bytes=8348490\n","\t\tMap output materialized bytes=10203716\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=927610\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:25:53,337 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000028_0\n","2021-11-10 13:25:53,337 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000029_0\n","2021-11-10 13:25:53,339 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:25:53,339 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:25:53,339 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:25:53,339 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:3892314112+134217728\n","2021-11-10 13:25:53,362 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:25:53,400 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:25:53,406 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:25:53,407 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:25:53,407 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:25:53,407 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:25:53,414 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:25:53,439 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:25:53,488 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:53,488 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:53,488 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:53,589 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:53,596 INFO streaming.PipeMapRed: Records R/W=1759/1\n","2021-11-10 13:25:53,652 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:54,306 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:25:55,043 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:25:55,759 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:25:56,506 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:25:57,221 INFO streaming.PipeMapRed: R/W/S=500000/497785/0 in:166666=500000/3 [rec/s] out:165928=497785/3 [rec/s]\n","2021-11-10 13:25:58,018 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:25:58,786 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:25:59,547 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:26:00,248 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:26:00,414 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:26:00,416 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:26:00,417 INFO mapred.LocalJobRunner: \n","2021-11-10 13:26:00,417 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:26:00,417 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:26:00,417 INFO mapred.MapTask: bufstart = 0; bufend = 8256708; bufvoid = 104857600\n","2021-11-10 13:26:00,417 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22544752(90179008); length = 3669645/6553600\n","2021-11-10 13:26:00,767 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:26:00,769 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000029_0 is done. And is in the process of committing\n","2021-11-10 13:26:00,776 INFO mapred.LocalJobRunner: Records R/W=1759/1\n","2021-11-10 13:26:00,776 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000029_0' done.\n","2021-11-10 13:26:00,776 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000029_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=122327\n","\t\tFILE: Number of bytes written=302556075\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4026654720\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=63\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=917412\n","\t\tMap output records=917412\n","\t\tMap output bytes=8256708\n","\t\tMap output materialized bytes=10091538\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=917412\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:26:00,776 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000029_0\n","2021-11-10 13:26:00,776 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000030_0\n","2021-11-10 13:26:00,778 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:26:00,778 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:26:00,778 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:26:00,779 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:4026531840+134217728\n","2021-11-10 13:26:00,805 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:26:00,837 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:26:00,837 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:26:00,837 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:26:00,837 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:26:00,837 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:26:00,838 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:26:00,848 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:26:00,857 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:00,857 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:00,857 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:00,958 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:00,964 INFO streaming.PipeMapRed: Records R/W=1756/1\n","2021-11-10 13:26:01,025 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:01,676 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:02,401 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:26:03,128 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:26:03,918 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:26:04,650 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:26:05,381 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:26:06,123 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:26:06,793 INFO mapred.LocalJobRunner: Records R/W=1756/1 > map\n","2021-11-10 13:26:06,804 INFO mapreduce.Job:  map 52% reduce 0%\n","2021-11-10 13:26:06,870 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:26:07,613 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:26:07,798 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:26:07,799 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:26:07,800 INFO mapred.LocalJobRunner: Records R/W=1756/1 > map\n","2021-11-10 13:26:07,800 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:26:07,800 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:26:07,800 INFO mapred.MapTask: bufstart = 0; bufend = 8313210; bufvoid = 104857600\n","2021-11-10 13:26:07,800 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22519640(90078560); length = 3694757/6553600\n","2021-11-10 13:26:08,117 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:26:08,122 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000030_0 is done. And is in the process of committing\n","2021-11-10 13:26:08,128 INFO mapred.LocalJobRunner: Records R/W=1756/1\n","2021-11-10 13:26:08,128 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000030_0' done.\n","2021-11-10 13:26:08,129 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000030_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=125671\n","\t\tFILE: Number of bytes written=312716703\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4160876544\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=65\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=923690\n","\t\tMap output records=923690\n","\t\tMap output bytes=8313210\n","\t\tMap output materialized bytes=10160596\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=923690\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=9\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:26:08,129 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000030_0\n","2021-11-10 13:26:08,129 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000031_0\n","2021-11-10 13:26:08,131 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:26:08,131 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:26:08,132 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:26:08,133 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:4160749568+134217728\n","2021-11-10 13:26:08,151 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:26:08,191 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:26:08,192 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:26:08,192 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:26:08,192 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:26:08,192 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:26:08,193 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:26:08,200 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:26:08,209 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:08,209 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:08,209 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:08,307 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:08,317 INFO streaming.PipeMapRed: Records R/W=1758/1\n","2021-11-10 13:26:08,385 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:08,805 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:26:09,071 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:09,830 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:26:10,564 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:26:11,339 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:26:12,092 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:26:12,856 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:26:13,579 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:26:14,350 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:26:15,084 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:26:15,263 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:26:15,263 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:26:15,265 INFO mapred.LocalJobRunner: \n","2021-11-10 13:26:15,265 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:26:15,265 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:26:15,265 INFO mapred.MapTask: bufstart = 0; bufend = 8300565; bufvoid = 104857600\n","2021-11-10 13:26:15,265 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22525260(90101040); length = 3689137/6553600\n","2021-11-10 13:26:15,576 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:26:15,579 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000031_0 is done. And is in the process of committing\n","2021-11-10 13:26:15,584 INFO mapred.LocalJobRunner: Records R/W=1758/1\n","2021-11-10 13:26:15,584 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000031_0' done.\n","2021-11-10 13:26:15,584 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000031_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=128503\n","\t\tFILE: Number of bytes written=322861876\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4295098368\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=67\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=922285\n","\t\tMap output records=922285\n","\t\tMap output bytes=8300565\n","\t\tMap output materialized bytes=10145141\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=922285\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:26:15,586 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000031_0\n","2021-11-10 13:26:15,586 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000032_0\n","2021-11-10 13:26:15,587 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:26:15,587 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:26:15,588 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:26:15,589 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:4294967296+134217728\n","2021-11-10 13:26:15,607 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:26:15,647 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:26:15,647 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:26:15,647 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:26:15,647 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:26:15,647 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:26:15,648 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:26:15,666 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:26:15,695 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:15,695 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:15,695 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:15,825 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:15,834 INFO streaming.PipeMapRed: Records R/W=1821/1\n","2021-11-10 13:26:15,899 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:16,550 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:17,276 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:26:18,027 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:26:18,786 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:26:19,499 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:26:20,231 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:26:20,962 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:26:21,598 INFO mapred.LocalJobRunner: Records R/W=1821/1 > map\n","2021-11-10 13:26:21,734 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:26:21,822 INFO mapreduce.Job:  map 55% reduce 0%\n","2021-11-10 13:26:22,452 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:26:22,517 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:26:22,518 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:26:22,519 INFO mapred.LocalJobRunner: Records R/W=1821/1 > map\n","2021-11-10 13:26:22,519 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:26:22,519 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:26:22,519 INFO mapred.MapTask: bufstart = 0; bufend = 8157699; bufvoid = 104857600\n","2021-11-10 13:26:22,519 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22588756(90355024); length = 3625641/6553600\n","2021-11-10 13:26:22,852 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:26:22,855 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000032_0 is done. And is in the process of committing\n","2021-11-10 13:26:22,859 INFO mapred.LocalJobRunner: Records R/W=1821/1\n","2021-11-10 13:26:22,859 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000032_0' done.\n","2021-11-10 13:26:22,859 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000032_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=131335\n","\t\tFILE: Number of bytes written=332832435\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4429320192\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=69\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=906411\n","\t\tMap output records=906411\n","\t\tMap output bytes=8157699\n","\t\tMap output materialized bytes=9970527\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=906411\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:26:22,860 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000032_0\n","2021-11-10 13:26:22,860 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000033_0\n","2021-11-10 13:26:22,861 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:26:22,861 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:26:22,861 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:26:22,862 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:4429185024+134217728\n","2021-11-10 13:26:22,881 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:26:22,913 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:26:22,913 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:26:22,913 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:26:22,913 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:26:22,913 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:26:22,914 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:26:22,920 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:26:22,930 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:22,930 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:22,930 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:23,020 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:23,025 INFO streaming.PipeMapRed: Records R/W=1845/1\n","2021-11-10 13:26:23,088 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:23,793 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:23,827 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:26:24,572 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:26:25,363 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:26:26,107 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:26:26,879 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:26:27,629 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:26:28,365 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:26:29,117 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:26:29,903 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:26:29,927 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:26:29,928 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:26:29,928 INFO mapred.LocalJobRunner: \n","2021-11-10 13:26:29,928 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:26:29,928 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:26:29,928 INFO mapred.MapTask: bufstart = 0; bufend = 8109990; bufvoid = 104857600\n","2021-11-10 13:26:29,928 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22609960(90439840); length = 3604437/6553600\n","2021-11-10 13:26:30,340 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:26:30,344 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000033_0 is done. And is in the process of committing\n","2021-11-10 13:26:30,348 INFO mapred.LocalJobRunner: Records R/W=1845/1\n","2021-11-10 13:26:30,349 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000033_0' done.\n","2021-11-10 13:26:30,349 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000033_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=134167\n","\t\tFILE: Number of bytes written=342744683\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4563542016\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=71\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=901110\n","\t\tMap output records=901110\n","\t\tMap output bytes=8109990\n","\t\tMap output materialized bytes=9912216\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=901110\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:26:30,349 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000033_0\n","2021-11-10 13:26:30,349 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000034_0\n","2021-11-10 13:26:30,353 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:26:30,353 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:26:30,354 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:26:30,355 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:4563402752+134217728\n","2021-11-10 13:26:30,410 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:26:30,464 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:26:30,464 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:26:30,464 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:26:30,464 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:26:30,464 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:26:30,465 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:26:30,474 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:26:30,489 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:30,489 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:30,489 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:30,585 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:30,591 INFO streaming.PipeMapRed: Records R/W=1791/1\n","2021-11-10 13:26:30,658 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:31,334 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:32,123 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:26:32,858 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:26:33,621 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:26:34,385 INFO streaming.PipeMapRed: R/W/S=500000/497785/0 in:166666=500000/3 [rec/s] out:165928=497785/3 [rec/s]\n","2021-11-10 13:26:35,124 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:26:35,861 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:26:36,368 INFO mapred.LocalJobRunner: Records R/W=1791/1 > map\n","2021-11-10 13:26:36,598 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:26:36,853 INFO mapreduce.Job:  map 59% reduce 0%\n","2021-11-10 13:26:37,311 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:26:37,488 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:26:37,491 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:26:37,495 INFO mapred.LocalJobRunner: Records R/W=1791/1 > map\n","2021-11-10 13:26:37,495 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:26:37,495 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:26:37,495 INFO mapred.MapTask: bufstart = 0; bufend = 8306811; bufvoid = 104857600\n","2021-11-10 13:26:37,495 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22522484(90089936); length = 3691913/6553600\n","2021-11-10 13:26:37,805 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:26:37,808 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000034_0 is done. And is in the process of committing\n","2021-11-10 13:26:37,811 INFO mapred.LocalJobRunner: Records R/W=1791/1\n","2021-11-10 13:26:37,811 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000034_0' done.\n","2021-11-10 13:26:37,812 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000034_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=136999\n","\t\tFILE: Number of bytes written=352897490\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4697763840\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=73\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=922979\n","\t\tMap output records=922979\n","\t\tMap output bytes=8306811\n","\t\tMap output materialized bytes=10152775\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=922979\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:26:37,812 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000034_0\n","2021-11-10 13:26:37,812 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000035_0\n","2021-11-10 13:26:37,813 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:26:37,813 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:26:37,814 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:26:37,815 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:4697620480+134217728\n","2021-11-10 13:26:37,838 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:26:37,853 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:26:37,865 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:26:37,866 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:26:37,866 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:26:37,866 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:26:37,866 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:26:37,866 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:26:37,880 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:26:37,919 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:37,919 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:37,919 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:38,035 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:38,042 INFO streaming.PipeMapRed: Records R/W=1862/1\n","2021-11-10 13:26:38,104 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:38,768 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:39,499 INFO streaming.PipeMapRed: R/W/S=200000/197680/0 in:200000=200000/1 [rec/s] out:197680=197680/1 [rec/s]\n","2021-11-10 13:26:40,219 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:26:40,974 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:26:41,716 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:26:42,469 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:26:43,212 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:26:43,992 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:26:44,734 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:26:45,052 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:26:45,053 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:26:45,054 INFO mapred.LocalJobRunner: \n","2021-11-10 13:26:45,054 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:26:45,054 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:26:45,054 INFO mapred.MapTask: bufstart = 0; bufend = 8453799; bufvoid = 104857600\n","2021-11-10 13:26:45,054 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22457156(89828624); length = 3757241/6553600\n","2021-11-10 13:26:45,383 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:26:45,387 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000035_0 is done. And is in the process of committing\n","2021-11-10 13:26:45,391 INFO mapred.LocalJobRunner: Records R/W=1862/1\n","2021-11-10 13:26:45,391 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000035_0' done.\n","2021-11-10 13:26:45,391 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000035_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=139831\n","\t\tFILE: Number of bytes written=363229949\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4831985664\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=75\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=939311\n","\t\tMap output records=939311\n","\t\tMap output bytes=8453799\n","\t\tMap output materialized bytes=10332427\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=939311\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=6\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:26:45,391 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000035_0\n","2021-11-10 13:26:45,391 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000036_0\n","2021-11-10 13:26:45,393 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:26:45,393 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:26:45,393 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:26:45,394 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:4831838208+134217728\n","2021-11-10 13:26:45,414 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:26:45,445 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:26:45,445 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:26:45,445 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:26:45,445 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:26:45,445 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:26:45,447 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:26:45,462 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:26:45,470 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:45,470 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:45,470 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:45,580 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:45,588 INFO streaming.PipeMapRed: Records R/W=1835/1\n","2021-11-10 13:26:45,657 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:46,353 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:47,099 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:26:47,852 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:26:48,607 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:26:49,320 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:26:50,068 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:26:50,807 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:26:51,618 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:26:52,341 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:26:52,642 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:26:52,643 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:26:52,643 INFO mapred.LocalJobRunner: \n","2021-11-10 13:26:52,644 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:26:52,644 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:26:52,644 INFO mapred.MapTask: bufstart = 0; bufend = 8443764; bufvoid = 104857600\n","2021-11-10 13:26:52,644 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22461616(89846464); length = 3752781/6553600\n","2021-11-10 13:26:52,878 INFO mapreduce.Job:  map 61% reduce 0%\n","2021-11-10 13:26:52,967 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:26:52,970 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000036_0 is done. And is in the process of committing\n","2021-11-10 13:26:52,976 INFO mapred.LocalJobRunner: Records R/W=1835/1\n","2021-11-10 13:26:52,976 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000036_0' done.\n","2021-11-10 13:26:52,976 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000036_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142663\n","\t\tFILE: Number of bytes written=373550143\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=4966207488\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=77\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=938196\n","\t\tMap output records=938196\n","\t\tMap output bytes=8443764\n","\t\tMap output materialized bytes=10320162\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=938196\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=8\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:26:52,976 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000036_0\n","2021-11-10 13:26:52,977 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000037_0\n","2021-11-10 13:26:52,978 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:26:52,978 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:26:52,978 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:26:52,979 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:4966055936+134217728\n","2021-11-10 13:26:53,012 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:26:53,046 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:26:53,046 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:26:53,046 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:26:53,046 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:26:53,046 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:26:53,047 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:26:53,056 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:26:53,085 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:53,085 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:53,085 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:53,209 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:53,215 INFO streaming.PipeMapRed: Records R/W=1811/1\n","2021-11-10 13:26:53,279 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:53,879 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:26:53,940 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:26:54,709 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:26:55,426 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:26:56,149 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:26:56,864 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:26:57,605 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:26:58,324 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:26:58,994 INFO mapred.LocalJobRunner: Records R/W=1811/1 > map\n","2021-11-10 13:26:59,053 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:160000=800000/5 [rec/s] out:159578=797890/5 [rec/s]\n","2021-11-10 13:26:59,811 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:26:59,882 INFO mapreduce.Job:  map 64% reduce 0%\n","2021-11-10 13:27:00,038 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:27:00,039 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:27:00,040 INFO mapred.LocalJobRunner: Records R/W=1811/1 > map\n","2021-11-10 13:27:00,041 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:27:00,041 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:27:00,041 INFO mapred.MapTask: bufstart = 0; bufend = 8370504; bufvoid = 104857600\n","2021-11-10 13:27:00,041 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22494176(89976704); length = 3720221/6553600\n","2021-11-10 13:27:00,385 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:27:00,388 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000037_0 is done. And is in the process of committing\n","2021-11-10 13:27:00,392 INFO mapred.LocalJobRunner: Records R/W=1811/1\n","2021-11-10 13:27:00,392 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000037_0' done.\n","2021-11-10 13:27:00,393 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000037_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=144983\n","\t\tFILE: Number of bytes written=383780797\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5100429312\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=79\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=930056\n","\t\tMap output records=930056\n","\t\tMap output bytes=8370504\n","\t\tMap output materialized bytes=10230622\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=930056\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:27:00,393 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000037_0\n","2021-11-10 13:27:00,393 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000038_0\n","2021-11-10 13:27:00,395 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:27:00,395 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:27:00,395 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:27:00,396 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:5100273664+134217728\n","2021-11-10 13:27:00,413 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:27:00,481 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:27:00,481 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:27:00,481 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:27:00,481 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:27:00,481 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:27:00,505 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:27:00,525 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:27:00,564 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:00,564 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:00,564 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:00,677 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:00,684 INFO streaming.PipeMapRed: Records R/W=1873/1\n","2021-11-10 13:27:00,737 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:00,882 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:27:01,389 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:02,154 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:27:02,875 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:27:03,615 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:27:04,358 INFO streaming.PipeMapRed: R/W/S=500000/497785/0 in:166666=500000/3 [rec/s] out:165928=497785/3 [rec/s]\n","2021-11-10 13:27:05,106 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:27:05,841 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:27:06,579 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:27:07,297 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:27:07,478 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:27:07,479 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:27:07,484 INFO mapred.LocalJobRunner: \n","2021-11-10 13:27:07,484 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:27:07,484 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:27:07,484 INFO mapred.MapTask: bufstart = 0; bufend = 8303553; bufvoid = 104857600\n","2021-11-10 13:27:07,484 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22523932(90095728); length = 3690465/6553600\n","2021-11-10 13:27:07,807 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:27:07,811 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000038_0 is done. And is in the process of committing\n","2021-11-10 13:27:07,815 INFO mapred.LocalJobRunner: Records R/W=1873/1\n","2021-11-10 13:27:07,815 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000038_0' done.\n","2021-11-10 13:27:07,815 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000038_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=147303\n","\t\tFILE: Number of bytes written=393929622\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5234651136\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=81\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=922617\n","\t\tMap output records=922617\n","\t\tMap output bytes=8303553\n","\t\tMap output materialized bytes=10148793\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=922617\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:27:07,815 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000038_0\n","2021-11-10 13:27:07,815 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000039_0\n","2021-11-10 13:27:07,819 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:27:07,819 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:27:07,819 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:27:07,820 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:5234491392+134217728\n","2021-11-10 13:27:07,847 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:27:07,878 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:27:07,878 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:27:07,878 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:27:07,878 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:27:07,878 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:27:07,879 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:27:07,885 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:27:07,893 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:07,893 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:07,893 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:07,993 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:07,998 INFO streaming.PipeMapRed: Records R/W=1817/1\n","2021-11-10 13:27:08,054 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:08,702 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:09,441 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:27:10,229 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:27:10,996 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:27:11,734 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:27:12,505 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:27:13,316 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:27:13,827 INFO mapred.LocalJobRunner: Records R/W=1817/1 > map\n","2021-11-10 13:27:13,891 INFO mapreduce.Job:  map 67% reduce 0%\n","2021-11-10 13:27:14,072 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:27:14,792 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:27:15,010 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:27:15,012 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:27:15,012 INFO mapred.LocalJobRunner: Records R/W=1817/1 > map\n","2021-11-10 13:27:15,012 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:27:15,012 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:27:15,012 INFO mapred.MapTask: bufstart = 0; bufend = 8341191; bufvoid = 104857600\n","2021-11-10 13:27:15,012 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22507204(90028816); length = 3707193/6553600\n","2021-11-10 13:27:15,331 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:27:15,334 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000039_0 is done. And is in the process of committing\n","2021-11-10 13:27:15,339 INFO mapred.LocalJobRunner: Records R/W=1817/1\n","2021-11-10 13:27:15,339 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000039_0' done.\n","2021-11-10 13:27:15,339 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000039_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=149623\n","\t\tFILE: Number of bytes written=404124449\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5368872960\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=83\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=926799\n","\t\tMap output records=926799\n","\t\tMap output bytes=8341191\n","\t\tMap output materialized bytes=10194795\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=926799\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:27:15,339 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000039_0\n","2021-11-10 13:27:15,340 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000040_0\n","2021-11-10 13:27:15,345 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:27:15,345 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:27:15,345 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:27:15,346 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:5368709120+134217728\n","2021-11-10 13:27:15,354 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:27:15,371 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:27:15,371 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:27:15,371 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:27:15,371 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:27:15,371 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:27:15,372 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:27:15,378 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:27:15,384 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:15,384 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:15,384 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:15,474 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:15,479 INFO streaming.PipeMapRed: Records R/W=1738/1\n","2021-11-10 13:27:15,537 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:15,892 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:27:16,226 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:17,024 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:27:17,773 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:27:18,498 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:27:19,244 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:27:19,990 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:27:20,770 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:27:21,517 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:27:22,207 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:27:22,211 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:27:22,212 INFO mapred.LocalJobRunner: \n","2021-11-10 13:27:22,213 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:27:22,213 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:27:22,213 INFO mapred.MapTask: bufstart = 0; bufend = 8009298; bufvoid = 104857600\n","2021-11-10 13:27:22,213 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22654712(90618848); length = 3559685/6553600\n","2021-11-10 13:27:22,525 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:27:22,528 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000040_0 is done. And is in the process of committing\n","2021-11-10 13:27:22,532 INFO mapred.LocalJobRunner: Records R/W=1738/1\n","2021-11-10 13:27:22,532 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000040_0' done.\n","2021-11-10 13:27:22,532 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000040_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=151943\n","\t\tFILE: Number of bytes written=413913629\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5503094784\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=85\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=889922\n","\t\tMap output records=889922\n","\t\tMap output bytes=8009298\n","\t\tMap output materialized bytes=9789148\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=889922\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:27:22,532 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000040_0\n","2021-11-10 13:27:22,532 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000041_0\n","2021-11-10 13:27:22,533 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:27:22,533 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:27:22,534 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:27:22,535 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:5502926848+134217728\n","2021-11-10 13:27:22,552 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:27:22,605 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:27:22,605 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:27:22,605 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:27:22,605 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:27:22,605 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:27:22,607 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:27:22,622 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:27:22,664 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:22,664 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:22,664 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:22,761 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:22,769 INFO streaming.PipeMapRed: Records R/W=1795/1\n","2021-11-10 13:27:22,829 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:23,507 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:24,238 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:27:24,980 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:27:25,737 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:27:26,491 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:27:27,231 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:27:27,977 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:27:28,543 INFO mapred.LocalJobRunner: Records R/W=1795/1 > map\n","2021-11-10 13:27:28,767 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:27:28,898 INFO mapreduce.Job:  map 70% reduce 0%\n","2021-11-10 13:27:29,517 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:27:29,640 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:27:29,643 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:27:29,644 INFO mapred.LocalJobRunner: Records R/W=1795/1 > map\n","2021-11-10 13:27:29,644 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:27:29,644 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:27:29,644 INFO mapred.MapTask: bufstart = 0; bufend = 8228025; bufvoid = 104857600\n","2021-11-10 13:27:29,644 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22557500(90230000); length = 3656897/6553600\n","2021-11-10 13:27:29,963 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:27:29,965 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000041_0 is done. And is in the process of committing\n","2021-11-10 13:27:29,970 INFO mapred.LocalJobRunner: Records R/W=1795/1\n","2021-11-10 13:27:29,970 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000041_0' done.\n","2021-11-10 13:27:29,970 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000041_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=154263\n","\t\tFILE: Number of bytes written=423970142\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5637316608\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=87\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=914225\n","\t\tMap output records=914225\n","\t\tMap output bytes=8228025\n","\t\tMap output materialized bytes=10056481\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=914225\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:27:29,970 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000041_0\n","2021-11-10 13:27:29,972 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000042_0\n","2021-11-10 13:27:29,973 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:27:29,973 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:27:29,974 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:27:29,975 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:5637144576+134217728\n","2021-11-10 13:27:30,006 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:27:30,038 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:27:30,038 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:27:30,038 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:27:30,038 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:27:30,038 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:27:30,039 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:27:30,046 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:27:30,057 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:30,057 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:30,057 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:30,145 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:30,151 INFO streaming.PipeMapRed: Records R/W=1791/1\n","2021-11-10 13:27:30,221 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:30,886 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:30,900 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:27:31,637 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:27:32,380 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:27:33,120 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:27:33,866 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:27:34,629 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:27:35,372 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:27:35,982 INFO mapred.LocalJobRunner: Records R/W=1791/1 > map\n","2021-11-10 13:27:36,094 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:27:36,831 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:27:36,903 INFO mapreduce.Job:  map 72% reduce 0%\n","2021-11-10 13:27:36,931 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:27:36,932 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:27:36,933 INFO mapred.LocalJobRunner: Records R/W=1791/1 > map\n","2021-11-10 13:27:36,933 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:27:36,933 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:27:36,933 INFO mapred.MapTask: bufstart = 0; bufend = 8196822; bufvoid = 104857600\n","2021-11-10 13:27:36,933 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22571368(90285472); length = 3643029/6553600\n","2021-11-10 13:27:37,255 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:27:37,258 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000042_0 is done. And is in the process of committing\n","2021-11-10 13:27:37,263 INFO mapred.LocalJobRunner: Records R/W=1791/1\n","2021-11-10 13:27:37,263 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000042_0' done.\n","2021-11-10 13:27:37,263 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000042_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=156071\n","\t\tFILE: Number of bytes written=433988518\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5771538432\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=89\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=910758\n","\t\tMap output records=910758\n","\t\tMap output bytes=8196822\n","\t\tMap output materialized bytes=10018344\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=910758\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:27:37,263 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000042_0\n","2021-11-10 13:27:37,263 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000043_0\n","2021-11-10 13:27:37,265 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:27:37,265 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:27:37,265 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:27:37,266 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:5771362304+134217728\n","2021-11-10 13:27:37,275 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:27:37,306 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:27:37,306 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:27:37,306 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:27:37,306 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:27:37,306 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:27:37,307 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:27:37,314 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:27:37,324 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:37,324 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:37,324 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:37,424 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:37,430 INFO streaming.PipeMapRed: Records R/W=1734/1\n","2021-11-10 13:27:37,497 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:37,904 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:27:38,200 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:38,959 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:27:39,675 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:27:40,375 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:27:41,103 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:27:41,850 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:27:42,622 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:27:43,366 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:27:44,137 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:27:44,472 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:27:44,472 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:27:44,473 INFO mapred.LocalJobRunner: \n","2021-11-10 13:27:44,473 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:27:44,473 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:27:44,473 INFO mapred.MapTask: bufstart = 0; bufend = 8517249; bufvoid = 104857600\n","2021-11-10 13:27:44,473 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22428956(89715824); length = 3785441/6553600\n","2021-11-10 13:27:44,805 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:27:44,808 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000043_0 is done. And is in the process of committing\n","2021-11-10 13:27:44,817 INFO mapred.LocalJobRunner: Records R/W=1734/1\n","2021-11-10 13:27:44,817 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000043_0' done.\n","2021-11-10 13:27:44,818 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000043_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=157879\n","\t\tFILE: Number of bytes written=444398527\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=5905760256\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=91\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=946361\n","\t\tMap output records=946361\n","\t\tMap output bytes=8517249\n","\t\tMap output materialized bytes=10409977\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=946361\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:27:44,818 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000043_0\n","2021-11-10 13:27:44,818 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000044_0\n","2021-11-10 13:27:44,820 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:27:44,820 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:27:44,820 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:27:44,821 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:5905580032+134217728\n","2021-11-10 13:27:44,843 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:27:44,893 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:27:44,893 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:27:44,893 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:27:44,893 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:27:44,893 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:27:44,905 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:27:44,914 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:27:44,932 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:44,932 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:44,932 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:45,044 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:45,049 INFO streaming.PipeMapRed: Records R/W=1913/1\n","2021-11-10 13:27:45,114 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:45,800 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:46,538 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:27:47,295 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:27:48,039 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:27:48,808 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:27:49,544 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:27:50,296 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:27:50,825 INFO mapred.LocalJobRunner: Records R/W=1913/1 > map\n","2021-11-10 13:27:50,910 INFO mapreduce.Job:  map 75% reduce 0%\n","2021-11-10 13:27:51,041 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:27:51,786 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:27:52,176 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:27:52,177 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:27:52,178 INFO mapred.LocalJobRunner: Records R/W=1913/1 > map\n","2021-11-10 13:27:52,178 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:27:52,178 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:27:52,178 INFO mapred.MapTask: bufstart = 0; bufend = 8579025; bufvoid = 104857600\n","2021-11-10 13:27:52,178 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22401500(89606000); length = 3812897/6553600\n","2021-11-10 13:27:52,499 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:27:52,502 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000044_0 is done. And is in the process of committing\n","2021-11-10 13:27:52,505 INFO mapred.LocalJobRunner: Records R/W=1913/1\n","2021-11-10 13:27:52,505 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000044_0' done.\n","2021-11-10 13:27:52,505 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000044_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=159687\n","\t\tFILE: Number of bytes written=454884040\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=6039982080\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=93\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=953225\n","\t\tMap output records=953225\n","\t\tMap output bytes=8579025\n","\t\tMap output materialized bytes=10485481\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=953225\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=9\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:27:52,505 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000044_0\n","2021-11-10 13:27:52,505 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000045_0\n","2021-11-10 13:27:52,506 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:27:52,506 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:27:52,506 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:27:52,507 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:6039797760+134217728\n","2021-11-10 13:27:52,528 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:27:52,547 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:27:52,547 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:27:52,547 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:27:52,547 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:27:52,547 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:27:52,548 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:27:52,553 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:27:52,560 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:52,560 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:52,560 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:52,654 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:52,659 INFO streaming.PipeMapRed: Records R/W=1835/1\n","2021-11-10 13:27:52,723 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:52,911 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:27:53,393 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:27:54,141 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:27:54,891 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:27:55,695 INFO streaming.PipeMapRed: R/W/S=400000/399457/0 in:133333=400000/3 [rec/s] out:133152=399457/3 [rec/s]\n","2021-11-10 13:27:56,460 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:27:57,198 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:27:57,971 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:27:58,728 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:133333=800000/6 [rec/s] out:132981=797890/6 [rec/s]\n","2021-11-10 13:27:59,463 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:27:59,715 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:27:59,716 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:27:59,717 INFO mapred.LocalJobRunner: \n","2021-11-10 13:27:59,717 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:27:59,717 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:27:59,717 INFO mapred.MapTask: bufstart = 0; bufend = 8384382; bufvoid = 104857600\n","2021-11-10 13:27:59,717 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22488008(89952032); length = 3726389/6553600\n","2021-11-10 13:27:59,914 INFO mapreduce.Job:  map 76% reduce 0%\n","2021-11-10 13:28:00,038 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:28:00,041 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000045_0 is done. And is in the process of committing\n","2021-11-10 13:28:00,044 INFO mapred.LocalJobRunner: Records R/W=1835/1\n","2021-11-10 13:28:00,044 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000045_0' done.\n","2021-11-10 13:28:00,044 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000045_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=161495\n","\t\tFILE: Number of bytes written=465131656\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=6174203904\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=95\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=931598\n","\t\tMap output records=931598\n","\t\tMap output bytes=8384382\n","\t\tMap output materialized bytes=10247584\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=931598\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=8\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:28:00,044 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000045_0\n","2021-11-10 13:28:00,044 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000046_0\n","2021-11-10 13:28:00,050 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:28:00,050 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:28:00,050 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:28:00,051 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:6174015488+134217728\n","2021-11-10 13:28:00,067 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:28:00,085 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:28:00,085 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:28:00,085 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:28:00,085 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:28:00,085 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:28:00,087 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:28:00,095 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:28:00,114 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:00,114 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:00,114 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:00,239 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:00,248 INFO streaming.PipeMapRed: Records R/W=1787/1\n","2021-11-10 13:28:00,359 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:00,914 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:28:01,025 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:01,749 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:28:02,521 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:28:03,287 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:28:04,073 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:28:04,766 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:28:05,500 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:28:06,061 INFO mapred.LocalJobRunner: Records R/W=1787/1 > map\n","2021-11-10 13:28:06,313 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:28:06,917 INFO mapreduce.Job:  map 79% reduce 0%\n","2021-11-10 13:28:07,053 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:28:07,110 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:28:07,110 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:28:07,111 INFO mapred.LocalJobRunner: Records R/W=1787/1 > map\n","2021-11-10 13:28:07,111 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:28:07,111 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:28:07,111 INFO mapred.MapTask: bufstart = 0; bufend = 8147817; bufvoid = 104857600\n","2021-11-10 13:28:07,111 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22593148(90372592); length = 3621249/6553600\n","2021-11-10 13:28:07,431 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:28:07,433 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000046_0 is done. And is in the process of committing\n","2021-11-10 13:28:07,436 INFO mapred.LocalJobRunner: Records R/W=1787/1\n","2021-11-10 13:28:07,436 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000046_0' done.\n","2021-11-10 13:28:07,437 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000046_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=163303\n","\t\tFILE: Number of bytes written=475090137\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=6308425728\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=97\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=905313\n","\t\tMap output records=905313\n","\t\tMap output bytes=8147817\n","\t\tMap output materialized bytes=9958449\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=905313\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:28:07,437 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000046_0\n","2021-11-10 13:28:07,437 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000047_0\n","2021-11-10 13:28:07,438 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:28:07,438 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:28:07,438 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:28:07,438 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:6308233216+134217728\n","2021-11-10 13:28:07,453 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:28:07,473 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:28:07,473 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:28:07,473 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:28:07,473 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:28:07,473 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:28:07,479 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:28:07,486 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:28:07,511 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:07,511 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:07,511 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:07,615 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:07,621 INFO streaming.PipeMapRed: Records R/W=1832/1\n","2021-11-10 13:28:07,684 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:07,917 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:28:08,343 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:09,063 INFO streaming.PipeMapRed: R/W/S=200000/197680/0 in:200000=200000/1 [rec/s] out:197680=197680/1 [rec/s]\n","2021-11-10 13:28:09,791 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:28:10,523 INFO streaming.PipeMapRed: R/W/S=400000/399457/0 in:133333=400000/3 [rec/s] out:133152=399457/3 [rec/s]\n","2021-11-10 13:28:11,237 INFO streaming.PipeMapRed: R/W/S=500000/497785/0 in:166666=500000/3 [rec/s] out:165928=497785/3 [rec/s]\n","2021-11-10 13:28:11,989 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:28:12,724 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:28:13,467 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:160000=800000/5 [rec/s] out:159783=798915/5 [rec/s]\n","2021-11-10 13:28:14,199 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:28:14,532 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:28:14,532 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:28:14,532 INFO mapred.LocalJobRunner: \n","2021-11-10 13:28:14,533 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:28:14,533 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:28:14,533 INFO mapred.MapTask: bufstart = 0; bufend = 8491077; bufvoid = 104857600\n","2021-11-10 13:28:14,533 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22440588(89762352); length = 3773809/6553600\n","2021-11-10 13:28:14,844 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:28:14,847 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000047_0 is done. And is in the process of committing\n","2021-11-10 13:28:14,850 INFO mapred.LocalJobRunner: Records R/W=1832/1\n","2021-11-10 13:28:14,850 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000047_0' done.\n","2021-11-10 13:28:14,854 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000047_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=164599\n","\t\tFILE: Number of bytes written=485468158\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=6442647552\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=99\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=943453\n","\t\tMap output records=943453\n","\t\tMap output bytes=8491077\n","\t\tMap output materialized bytes=10377989\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=943453\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=4\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:28:14,854 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000047_0\n","2021-11-10 13:28:14,854 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000048_0\n","2021-11-10 13:28:14,855 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:28:14,855 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:28:14,855 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:28:14,856 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:6442450944+134217728\n","2021-11-10 13:28:14,871 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:28:14,902 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:28:14,902 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:28:14,902 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:28:14,902 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:28:14,902 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:28:14,903 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:28:14,910 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:28:14,916 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:14,916 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:14,916 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:15,014 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:15,020 INFO streaming.PipeMapRed: Records R/W=1813/1\n","2021-11-10 13:28:15,089 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:15,765 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:16,570 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:28:17,318 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:28:18,126 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:28:18,892 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:28:19,658 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:28:20,410 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:28:20,864 INFO mapred.LocalJobRunner: Records R/W=1813/1 > map\n","2021-11-10 13:28:20,927 INFO mapreduce.Job:  map 82% reduce 0%\n","2021-11-10 13:28:21,156 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:28:21,915 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:28:22,090 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:28:22,092 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:28:22,092 INFO mapred.LocalJobRunner: Records R/W=1813/1 > map\n","2021-11-10 13:28:22,093 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:28:22,093 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:28:22,093 INFO mapred.MapTask: bufstart = 0; bufend = 8281494; bufvoid = 104857600\n","2021-11-10 13:28:22,093 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22533736(90134944); length = 3680661/6553600\n","2021-11-10 13:28:22,409 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:28:22,412 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000048_0 is done. And is in the process of committing\n","2021-11-10 13:28:22,417 INFO mapred.LocalJobRunner: Records R/W=1813/1\n","2021-11-10 13:28:22,417 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000048_0' done.\n","2021-11-10 13:28:22,417 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000048_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=165895\n","\t\tFILE: Number of bytes written=495590022\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=6576869376\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=101\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=920166\n","\t\tMap output records=920166\n","\t\tMap output bytes=8281494\n","\t\tMap output materialized bytes=10121832\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=920166\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=11\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:28:22,417 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000048_0\n","2021-11-10 13:28:22,418 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000049_0\n","2021-11-10 13:28:22,424 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:28:22,424 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:28:22,424 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:28:22,425 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:6576668672+134217728\n","2021-11-10 13:28:22,452 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:28:22,490 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:28:22,490 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:28:22,490 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:28:22,490 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:28:22,490 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:28:22,491 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:28:22,500 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:28:22,520 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:22,520 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:22,520 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:22,666 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:22,674 INFO streaming.PipeMapRed: Records R/W=1721/1\n","2021-11-10 13:28:22,770 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:22,929 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:28:23,464 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:24,202 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:28:24,941 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:28:25,695 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:28:26,449 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:28:27,171 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:28:27,879 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:28:28,430 INFO mapred.LocalJobRunner: Records R/W=1721/1 > map\n","2021-11-10 13:28:28,668 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:28:28,931 INFO mapreduce.Job:  map 84% reduce 0%\n","2021-11-10 13:28:29,374 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:28:29,375 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:28:29,376 INFO mapred.LocalJobRunner: Records R/W=1721/1 > map\n","2021-11-10 13:28:29,376 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:28:29,376 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:28:29,376 INFO mapred.MapTask: bufstart = 0; bufend = 8083044; bufvoid = 104857600\n","2021-11-10 13:28:29,376 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22621936(90487744); length = 3592461/6553600\n","2021-11-10 13:28:29,695 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:28:29,698 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000049_0 is done. And is in the process of committing\n","2021-11-10 13:28:29,701 INFO mapred.LocalJobRunner: Records R/W=1721/1\n","2021-11-10 13:28:29,701 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000049_0' done.\n","2021-11-10 13:28:29,701 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000049_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=167191\n","\t\tFILE: Number of bytes written=505469336\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=6711091200\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=103\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=898116\n","\t\tMap output records=898116\n","\t\tMap output bytes=8083044\n","\t\tMap output materialized bytes=9879282\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=898116\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:28:29,701 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000049_0\n","2021-11-10 13:28:29,702 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000050_0\n","2021-11-10 13:28:29,703 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:28:29,703 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:28:29,703 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:28:29,704 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:6710886400+134217728\n","2021-11-10 13:28:29,712 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:28:29,735 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:28:29,735 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:28:29,735 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:28:29,735 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:28:29,735 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:28:29,735 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:28:29,747 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:28:29,764 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:29,764 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:29,764 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:29,881 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:29,887 INFO streaming.PipeMapRed: Records R/W=1791/1\n","2021-11-10 13:28:29,932 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:28:29,956 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:30,605 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:31,300 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:28:32,095 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:28:32,805 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:28:33,494 INFO streaming.PipeMapRed: R/W/S=500000/497785/0 in:166666=500000/3 [rec/s] out:165928=497785/3 [rec/s]\n","2021-11-10 13:28:34,205 INFO streaming.PipeMapRed: R/W/S=600000/598922/0 in:150000=600000/4 [rec/s] out:149730=598922/4 [rec/s]\n","2021-11-10 13:28:34,948 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:28:35,708 INFO streaming.PipeMapRed: R/W/S=800000/797890/0 in:160000=800000/5 [rec/s] out:159578=797890/5 [rec/s]\n","2021-11-10 13:28:36,395 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:28:36,565 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:28:36,569 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:28:36,569 INFO mapred.LocalJobRunner: \n","2021-11-10 13:28:36,570 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:28:36,570 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:28:36,570 INFO mapred.MapTask: bufstart = 0; bufend = 8300601; bufvoid = 104857600\n","2021-11-10 13:28:36,570 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22525244(90100976); length = 3689153/6553600\n","2021-11-10 13:28:36,916 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:28:36,919 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000050_0 is done. And is in the process of committing\n","2021-11-10 13:28:36,923 INFO mapred.LocalJobRunner: Records R/W=1791/1\n","2021-11-10 13:28:36,923 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000050_0' done.\n","2021-11-10 13:28:36,923 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000050_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=168487\n","\t\tFILE: Number of bytes written=515614553\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=6845313024\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=105\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=922289\n","\t\tMap output records=922289\n","\t\tMap output bytes=8300601\n","\t\tMap output materialized bytes=10145185\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=922289\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=6\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:28:36,923 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000050_0\n","2021-11-10 13:28:36,923 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000051_0\n","2021-11-10 13:28:36,924 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:28:36,924 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:28:36,925 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:28:36,925 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:6845104128+134217728\n","2021-11-10 13:28:36,946 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:28:36,966 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:28:36,966 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:28:36,966 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:28:36,966 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:28:36,966 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:28:36,975 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:28:36,983 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:28:37,008 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:37,008 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:37,008 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:37,117 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:37,125 INFO streaming.PipeMapRed: Records R/W=1721/1\n","2021-11-10 13:28:37,220 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:37,986 INFO streaming.PipeMapRed: R/W/S=100000/99352/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:38,715 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:28:39,434 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:28:40,165 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:28:40,899 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:28:41,624 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:28:42,341 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:28:43,078 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:28:43,814 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:28:44,016 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:28:44,017 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:28:44,017 INFO mapred.LocalJobRunner: \n","2021-11-10 13:28:44,017 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:28:44,017 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:28:44,017 INFO mapred.MapTask: bufstart = 0; bufend = 8332902; bufvoid = 104857600\n","2021-11-10 13:28:44,017 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22510888(90043552); length = 3703509/6553600\n","2021-11-10 13:28:44,332 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:28:44,335 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000051_0 is done. And is in the process of committing\n","2021-11-10 13:28:44,339 INFO mapred.LocalJobRunner: Records R/W=1721/1\n","2021-11-10 13:28:44,339 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000051_0' done.\n","2021-11-10 13:28:44,339 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000051_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=169783\n","\t\tFILE: Number of bytes written=525799249\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=6979534848\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=107\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=925878\n","\t\tMap output records=925878\n","\t\tMap output bytes=8332902\n","\t\tMap output materialized bytes=10184664\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=925878\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=10\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:28:44,339 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000051_0\n","2021-11-10 13:28:44,339 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000052_0\n","2021-11-10 13:28:44,340 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:28:44,340 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:28:44,340 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:28:44,341 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:6979321856+134217728\n","2021-11-10 13:28:44,357 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:28:44,387 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:28:44,387 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:28:44,387 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:28:44,387 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:28:44,387 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:28:44,392 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:28:44,399 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:28:44,409 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:44,409 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:44,409 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:44,503 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:44,509 INFO streaming.PipeMapRed: Records R/W=1843/1\n","2021-11-10 13:28:44,574 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:45,233 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:45,947 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:28:46,667 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:28:47,381 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:200000=400000/2 [rec/s] out:199216=398433/2 [rec/s]\n","2021-11-10 13:28:48,094 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:28:48,814 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:28:49,536 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:28:50,265 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:160000=800000/5 [rec/s] out:159783=798915/5 [rec/s]\n","2021-11-10 13:28:50,980 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:28:51,156 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:28:51,156 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:28:51,157 INFO mapred.LocalJobRunner: \n","2021-11-10 13:28:51,157 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:28:51,157 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:28:51,157 INFO mapred.MapTask: bufstart = 0; bufend = 8302635; bufvoid = 104857600\n","2021-11-10 13:28:51,157 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22524340(90097360); length = 3690057/6553600\n","2021-11-10 13:28:51,462 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:28:51,465 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000052_0 is done. And is in the process of committing\n","2021-11-10 13:28:51,468 INFO mapred.LocalJobRunner: Records R/W=1843/1\n","2021-11-10 13:28:51,468 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000052_0' done.\n","2021-11-10 13:28:51,468 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000052_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=170567\n","\t\tFILE: Number of bytes written=535946952\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=7113756672\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=109\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=922515\n","\t\tMap output records=922515\n","\t\tMap output bytes=8302635\n","\t\tMap output materialized bytes=10147671\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=922515\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:28:51,468 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000052_0\n","2021-11-10 13:28:51,468 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000053_0\n","2021-11-10 13:28:51,469 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:28:51,469 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:28:51,470 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:28:51,470 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:7113539584+134217728\n","2021-11-10 13:28:51,483 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:28:51,506 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:28:51,506 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:28:51,506 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:28:51,506 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:28:51,506 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:28:51,517 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:28:51,524 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:28:51,540 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:51,556 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:51,558 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:51,645 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:51,650 INFO streaming.PipeMapRed: Records R/W=1782/1\n","2021-11-10 13:28:51,715 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:52,381 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:53,097 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:28:53,808 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:28:54,548 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:28:55,263 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:28:55,985 INFO streaming.PipeMapRed: R/W/S=600000/599186/0 in:150000=600000/4 [rec/s] out:149796=599186/4 [rec/s]\n","2021-11-10 13:28:56,698 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:28:57,394 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:160000=800000/5 [rec/s] out:159783=798915/5 [rec/s]\n","2021-11-10 13:28:58,113 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:28:58,258 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:28:58,262 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:28:58,262 INFO mapred.LocalJobRunner: \n","2021-11-10 13:28:58,262 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:28:58,262 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:28:58,262 INFO mapred.MapTask: bufstart = 0; bufend = 8264583; bufvoid = 104857600\n","2021-11-10 13:28:58,262 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22541252(90165008); length = 3673145/6553600\n","2021-11-10 13:28:58,591 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:28:58,594 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000053_0 is done. And is in the process of committing\n","2021-11-10 13:28:58,597 INFO mapred.LocalJobRunner: Records R/W=1782/1\n","2021-11-10 13:28:58,597 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000053_0' done.\n","2021-11-10 13:28:58,597 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000053_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=171351\n","\t\tFILE: Number of bytes written=546048147\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=7247978496\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=111\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=918287\n","\t\tMap output records=918287\n","\t\tMap output bytes=8264583\n","\t\tMap output materialized bytes=10101163\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=918287\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:28:58,597 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000053_0\n","2021-11-10 13:28:58,598 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000054_0\n","2021-11-10 13:28:58,599 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:28:58,599 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:28:58,599 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:28:58,600 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:7247757312+134217728\n","2021-11-10 13:28:58,610 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:28:58,643 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:28:58,643 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:28:58,643 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:28:58,643 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:28:58,643 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:28:58,644 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:28:58,652 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:28:58,657 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:58,658 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:58,658 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:58,752 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:58,759 INFO streaming.PipeMapRed: Records R/W=1859/1\n","2021-11-10 13:28:58,820 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:28:59,465 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:00,201 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:29:00,914 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:29:01,668 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:29:02,408 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:29:03,132 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:29:03,838 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:29:04,602 INFO mapred.LocalJobRunner: Records R/W=1859/1 > map\n","2021-11-10 13:29:04,648 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:160000=800000/5 [rec/s] out:159783=798915/5 [rec/s]\n","2021-11-10 13:29:04,953 INFO mapreduce.Job:  map 93% reduce 0%\n","2021-11-10 13:29:05,407 INFO streaming.PipeMapRed: R/W/S=900000/899291/0 in:150000=900000/6 [rec/s] out:149881=899291/6 [rec/s]\n","2021-11-10 13:29:05,476 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:29:05,477 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:29:05,478 INFO mapred.LocalJobRunner: Records R/W=1859/1 > map\n","2021-11-10 13:29:05,478 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:29:05,478 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:29:05,478 INFO mapred.MapTask: bufstart = 0; bufend = 8169786; bufvoid = 104857600\n","2021-11-10 13:29:05,478 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22583384(90333536); length = 3631013/6553600\n","2021-11-10 13:29:05,787 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:29:05,789 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000054_0 is done. And is in the process of committing\n","2021-11-10 13:29:05,794 INFO mapred.LocalJobRunner: Records R/W=1859/1\n","2021-11-10 13:29:05,795 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000054_0' done.\n","2021-11-10 13:29:05,795 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000054_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=172135\n","\t\tFILE: Number of bytes written=556033479\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=7382200320\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=113\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=907754\n","\t\tMap output records=907754\n","\t\tMap output bytes=8169786\n","\t\tMap output materialized bytes=9985300\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=907754\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=9\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:29:05,795 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000054_0\n","2021-11-10 13:29:05,795 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000055_0\n","2021-11-10 13:29:05,797 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:29:05,797 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:29:05,797 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:29:05,798 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:7381975040+134217728\n","2021-11-10 13:29:05,815 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:29:05,844 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:29:05,845 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:29:05,845 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:29:05,845 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:29:05,845 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:29:05,845 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:29:05,852 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:29:05,860 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:05,860 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:05,860 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:05,946 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:05,951 INFO streaming.PipeMapRed: Records R/W=1851/1\n","2021-11-10 13:29:05,954 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:29:06,006 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:06,657 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:07,409 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:29:08,149 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:29:08,874 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:29:09,596 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:29:10,324 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:29:11,058 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:29:11,793 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:160000=800000/5 [rec/s] out:159783=798915/5 [rec/s]\n","2021-11-10 13:29:11,810 INFO mapred.LocalJobRunner: Records R/W=1851/1 > map\n","2021-11-10 13:29:11,957 INFO mapreduce.Job:  map 94% reduce 0%\n","2021-11-10 13:29:12,504 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:29:12,701 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:29:12,702 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:29:12,702 INFO mapred.LocalJobRunner: Records R/W=1851/1 > map\n","2021-11-10 13:29:12,702 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:29:12,702 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:29:12,702 INFO mapred.MapTask: bufstart = 0; bufend = 8338590; bufvoid = 104857600\n","2021-11-10 13:29:12,702 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22508360(90033440); length = 3706037/6553600\n","2021-11-10 13:29:13,014 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:29:13,017 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000055_0 is done. And is in the process of committing\n","2021-11-10 13:29:13,019 INFO mapred.LocalJobRunner: Records R/W=1851/1\n","2021-11-10 13:29:13,022 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000055_0' done.\n","2021-11-10 13:29:13,022 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000055_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=172919\n","\t\tFILE: Number of bytes written=566225127\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=7516422144\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=115\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=926510\n","\t\tMap output records=926510\n","\t\tMap output bytes=8338590\n","\t\tMap output materialized bytes=10191616\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=926510\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:29:13,022 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000055_0\n","2021-11-10 13:29:13,022 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000056_0\n","2021-11-10 13:29:13,026 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:29:13,026 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:29:13,026 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:29:13,027 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:7516192768+134217728\n","2021-11-10 13:29:13,043 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:29:13,064 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:29:13,064 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:29:13,064 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:29:13,064 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:29:13,064 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:29:13,064 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:29:13,074 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:29:13,098 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:13,098 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:13,098 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:13,196 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:13,201 INFO streaming.PipeMapRed: Records R/W=1817/1\n","2021-11-10 13:29:13,260 INFO streaming.PipeMapRed: R/W/S=10000/8194/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:13,936 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:13,958 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:29:14,697 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:29:15,428 INFO streaming.PipeMapRed: R/W/S=300000/298056/0 in:150000=300000/2 [rec/s] out:149028=298056/2 [rec/s]\n","2021-11-10 13:29:16,170 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:29:16,914 INFO streaming.PipeMapRed: R/W/S=500000/497785/0 in:166666=500000/3 [rec/s] out:165928=497785/3 [rec/s]\n","2021-11-10 13:29:17,672 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:29:18,418 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:29:19,034 INFO mapred.LocalJobRunner: Records R/W=1817/1 > map\n","2021-11-10 13:29:19,177 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:29:19,925 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:29:19,960 INFO mapreduce.Job:  map 96% reduce 0%\n","2021-11-10 13:29:20,133 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:29:20,133 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:29:20,134 INFO mapred.LocalJobRunner: Records R/W=1817/1 > map\n","2021-11-10 13:29:20,135 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:29:20,135 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:29:20,135 INFO mapred.MapTask: bufstart = 0; bufend = 8332308; bufvoid = 104857600\n","2021-11-10 13:29:20,135 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22511152(90044608); length = 3703245/6553600\n","2021-11-10 13:29:20,459 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:29:20,463 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000056_0 is done. And is in the process of committing\n","2021-11-10 13:29:20,466 INFO mapred.LocalJobRunner: Records R/W=1817/1\n","2021-11-10 13:29:20,466 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000056_0' done.\n","2021-11-10 13:29:20,466 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000056_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=173703\n","\t\tFILE: Number of bytes written=576409097\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=7650643968\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=117\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=925812\n","\t\tMap output records=925812\n","\t\tMap output bytes=8332308\n","\t\tMap output materialized bytes=10183938\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=925812\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=3\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:29:20,466 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000056_0\n","2021-11-10 13:29:20,466 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000057_0\n","2021-11-10 13:29:20,468 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:29:20,468 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:29:20,468 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:29:20,469 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:7650410496+134217728\n","2021-11-10 13:29:20,484 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:29:20,516 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:29:20,516 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:29:20,516 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:29:20,516 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:29:20,516 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:29:20,517 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:29:20,525 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:29:20,536 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:20,536 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:20,536 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:20,625 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:20,630 INFO streaming.PipeMapRed: Records R/W=1760/1\n","2021-11-10 13:29:20,692 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:20,960 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-10 13:29:21,378 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:22,180 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:29:22,913 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:29:23,642 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:29:24,373 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:29:25,077 INFO streaming.PipeMapRed: R/W/S=600000/598162/0 in:150000=600000/4 [rec/s] out:149540=598162/4 [rec/s]\n","2021-11-10 13:29:25,814 INFO streaming.PipeMapRed: R/W/S=700000/698538/0 in:140000=700000/5 [rec/s] out:139707=698538/5 [rec/s]\n","2021-11-10 13:29:26,560 INFO streaming.PipeMapRed: R/W/S=800000/798915/0 in:133333=800000/6 [rec/s] out:133152=798915/6 [rec/s]\n","2021-11-10 13:29:27,274 INFO streaming.PipeMapRed: R/W/S=900000/898267/0 in:150000=900000/6 [rec/s] out:149711=898267/6 [rec/s]\n","2021-11-10 13:29:27,413 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:29:27,413 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:29:27,414 INFO mapred.LocalJobRunner: \n","2021-11-10 13:29:27,414 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:29:27,414 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:29:27,414 INFO mapred.MapTask: bufstart = 0; bufend = 8263224; bufvoid = 104857600\n","2021-11-10 13:29:27,414 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22541856(90167424); length = 3672541/6553600\n","2021-11-10 13:29:27,740 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:29:27,744 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000057_0 is done. And is in the process of committing\n","2021-11-10 13:29:27,747 INFO mapred.LocalJobRunner: Records R/W=1760/1\n","2021-11-10 13:29:27,747 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000057_0' done.\n","2021-11-10 13:29:27,747 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000057_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=173975\n","\t\tFILE: Number of bytes written=586508631\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=7784865792\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=119\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=918136\n","\t\tMap output records=918136\n","\t\tMap output bytes=8263224\n","\t\tMap output materialized bytes=10099502\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=918136\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=5\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-10 13:29:27,747 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000057_0\n","2021-11-10 13:29:27,747 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_m_000058_0\n","2021-11-10 13:29:27,750 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:29:27,751 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:29:27,751 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:29:27,752 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/clean_data:7784628224+84017483\n","2021-11-10 13:29:27,766 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-10 13:29:27,799 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-10 13:29:27,799 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-10 13:29:27,799 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-10 13:29:27,799 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-10 13:29:27,799 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-10 13:29:27,799 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-10 13:29:27,813 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-10 13:29:27,821 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:27,822 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:27,822 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:27,920 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:27,925 INFO streaming.PipeMapRed: Records R/W=1797/1\n","2021-11-10 13:29:27,987 INFO streaming.PipeMapRed: R/W/S=10000/9218/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:28,671 INFO streaming.PipeMapRed: R/W/S=100000/98328/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:29,384 INFO streaming.PipeMapRed: R/W/S=200000/198704/0 in:200000=200000/1 [rec/s] out:198704=198704/1 [rec/s]\n","2021-11-10 13:29:30,134 INFO streaming.PipeMapRed: R/W/S=300000/299081/0 in:150000=300000/2 [rec/s] out:149540=299081/2 [rec/s]\n","2021-11-10 13:29:30,890 INFO streaming.PipeMapRed: R/W/S=400000/398433/0 in:133333=400000/3 [rec/s] out:132811=398433/3 [rec/s]\n","2021-11-10 13:29:31,635 INFO streaming.PipeMapRed: R/W/S=500000/498809/0 in:166666=500000/3 [rec/s] out:166269=498809/3 [rec/s]\n","2021-11-10 13:29:32,304 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:29:32,305 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:29:32,305 INFO mapred.LocalJobRunner: \n","2021-11-10 13:29:32,305 INFO mapred.MapTask: Starting flush of map output\n","2021-11-10 13:29:32,305 INFO mapred.MapTask: Spilling map output\n","2021-11-10 13:29:32,305 INFO mapred.MapTask: bufstart = 0; bufend = 5306490; bufvoid = 104857600\n","2021-11-10 13:29:32,305 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 23855960(95423840); length = 2358437/6553600\n","2021-11-10 13:29:32,495 INFO mapred.MapTask: Finished spill 0\n","2021-11-10 13:29:32,497 INFO mapred.Task: Task:attempt_local1867248162_0001_m_000058_0 is done. And is in the process of committing\n","2021-11-10 13:29:32,501 INFO mapred.LocalJobRunner: Records R/W=1797/1\n","2021-11-10 13:29:32,501 INFO mapred.Task: Task 'attempt_local1867248162_0001_m_000058_0' done.\n","2021-11-10 13:29:32,501 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_m_000058_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=174247\n","\t\tFILE: Number of bytes written=592994379\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=7868883275\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=121\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=589610\n","\t\tMap output records=589610\n","\t\tMap output bytes=5306490\n","\t\tMap output materialized bytes=6485716\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=589610\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tFile Input Format Counters \n","\t\tBytes Read=84017483\n","2021-11-10 13:29:32,501 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_m_000058_0\n","2021-11-10 13:29:32,502 INFO mapred.LocalJobRunner: map task executor complete.\n","2021-11-10 13:29:32,506 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2021-11-10 13:29:32,506 INFO mapred.LocalJobRunner: Starting task: attempt_local1867248162_0001_r_000000_0\n","2021-11-10 13:29:32,522 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-10 13:29:32,522 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-10 13:29:32,522 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-10 13:29:32,528 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1bd9d787\n","2021-11-10 13:29:32,531 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2021-11-10 13:29:32,571 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2021-11-10 13:29:32,591 INFO reduce.EventFetcher: attempt_local1867248162_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2021-11-10 13:29:32,646 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000022_0 decomp: 10035874 len: 10035878 to MEMORY\n","2021-11-10 13:29:32,719 INFO reduce.InMemoryMapOutput: Read 10035874 bytes from map-output for attempt_local1867248162_0001_m_000022_0\n","2021-11-10 13:29:32,720 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10035874, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->10035874\n","2021-11-10 13:29:32,730 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000048_0 decomp: 10121828 len: 10121832 to MEMORY\n","2021-11-10 13:29:32,739 INFO reduce.InMemoryMapOutput: Read 10121828 bytes from map-output for attempt_local1867248162_0001_m_000048_0\n","2021-11-10 13:29:32,739 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10121828, inMemoryMapOutputs.size() -> 2, commitMemory -> 10035874, usedMemory ->20157702\n","2021-11-10 13:29:32,742 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000047_0 decomp: 10377985 len: 10377989 to MEMORY\n","2021-11-10 13:29:32,752 INFO reduce.InMemoryMapOutput: Read 10377985 bytes from map-output for attempt_local1867248162_0001_m_000047_0\n","2021-11-10 13:29:32,753 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10377985, inMemoryMapOutputs.size() -> 3, commitMemory -> 20157702, usedMemory ->30535687\n","2021-11-10 13:29:32,755 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000046_0 decomp: 9958445 len: 9958449 to MEMORY\n","2021-11-10 13:29:32,764 INFO reduce.InMemoryMapOutput: Read 9958445 bytes from map-output for attempt_local1867248162_0001_m_000046_0\n","2021-11-10 13:29:32,764 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9958445, inMemoryMapOutputs.size() -> 4, commitMemory -> 30535687, usedMemory ->40494132\n","2021-11-10 13:29:32,768 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000021_0 decomp: 9856508 len: 9856512 to MEMORY\n","2021-11-10 13:29:32,917 INFO reduce.InMemoryMapOutput: Read 9856508 bytes from map-output for attempt_local1867248162_0001_m_000021_0\n","2021-11-10 13:29:32,917 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9856508, inMemoryMapOutputs.size() -> 5, commitMemory -> 40494132, usedMemory ->50350640\n","2021-11-10 13:29:32,924 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000020_0 decomp: 9940196 len: 9940200 to MEMORY\n","2021-11-10 13:29:33,125 INFO reduce.InMemoryMapOutput: Read 9940196 bytes from map-output for attempt_local1867248162_0001_m_000020_0\n","2021-11-10 13:29:33,125 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9940196, inMemoryMapOutputs.size() -> 6, commitMemory -> 50350640, usedMemory ->60290836\n","2021-11-10 13:29:33,147 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000019_0 decomp: 10164893 len: 10164897 to MEMORY\n","2021-11-10 13:29:33,329 INFO reduce.InMemoryMapOutput: Read 10164893 bytes from map-output for attempt_local1867248162_0001_m_000019_0\n","2021-11-10 13:29:33,329 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10164893, inMemoryMapOutputs.size() -> 7, commitMemory -> 60290836, usedMemory ->70455729\n","2021-11-10 13:29:33,332 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000045_0 decomp: 10247580 len: 10247584 to MEMORY\n","2021-11-10 13:29:33,341 INFO reduce.InMemoryMapOutput: Read 10247580 bytes from map-output for attempt_local1867248162_0001_m_000045_0\n","2021-11-10 13:29:33,341 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10247580, inMemoryMapOutputs.size() -> 8, commitMemory -> 70455729, usedMemory ->80703309\n","2021-11-10 13:29:33,344 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000018_0 decomp: 10090049 len: 10090053 to MEMORY\n","2021-11-10 13:29:33,517 INFO reduce.InMemoryMapOutput: Read 10090049 bytes from map-output for attempt_local1867248162_0001_m_000018_0\n","2021-11-10 13:29:33,517 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10090049, inMemoryMapOutputs.size() -> 9, commitMemory -> 80703309, usedMemory ->90793358\n","2021-11-10 13:29:33,520 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000044_0 decomp: 10485477 len: 10485481 to MEMORY\n","2021-11-10 13:29:33,529 INFO reduce.InMemoryMapOutput: Read 10485477 bytes from map-output for attempt_local1867248162_0001_m_000044_0\n","2021-11-10 13:29:33,529 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10485477, inMemoryMapOutputs.size() -> 10, commitMemory -> 90793358, usedMemory ->101278835\n","2021-11-10 13:29:33,532 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000043_0 decomp: 10409973 len: 10409977 to MEMORY\n","2021-11-10 13:29:33,541 INFO reduce.InMemoryMapOutput: Read 10409973 bytes from map-output for attempt_local1867248162_0001_m_000043_0\n","2021-11-10 13:29:33,542 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10409973, inMemoryMapOutputs.size() -> 11, commitMemory -> 101278835, usedMemory ->111688808\n","2021-11-10 13:29:33,544 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000042_0 decomp: 10018340 len: 10018344 to MEMORY\n","2021-11-10 13:29:33,553 INFO reduce.InMemoryMapOutput: Read 10018340 bytes from map-output for attempt_local1867248162_0001_m_000042_0\n","2021-11-10 13:29:33,553 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10018340, inMemoryMapOutputs.size() -> 12, commitMemory -> 111688808, usedMemory ->121707148\n","2021-11-10 13:29:33,557 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000017_0 decomp: 10121498 len: 10121502 to MEMORY\n","2021-11-10 13:29:33,718 INFO reduce.InMemoryMapOutput: Read 10121498 bytes from map-output for attempt_local1867248162_0001_m_000017_0\n","2021-11-10 13:29:33,718 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10121498, inMemoryMapOutputs.size() -> 13, commitMemory -> 121707148, usedMemory ->131828646\n","2021-11-10 13:29:33,722 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000003_0 decomp: 10047930 len: 10047934 to MEMORY\n","2021-11-10 13:29:33,834 INFO reduce.InMemoryMapOutput: Read 10047930 bytes from map-output for attempt_local1867248162_0001_m_000003_0\n","2021-11-10 13:29:33,835 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10047930, inMemoryMapOutputs.size() -> 14, commitMemory -> 131828646, usedMemory ->141876576\n","2021-11-10 13:29:33,838 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000054_0 decomp: 9985296 len: 9985300 to MEMORY\n","2021-11-10 13:29:33,846 INFO reduce.InMemoryMapOutput: Read 9985296 bytes from map-output for attempt_local1867248162_0001_m_000054_0\n","2021-11-10 13:29:33,847 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9985296, inMemoryMapOutputs.size() -> 15, commitMemory -> 141876576, usedMemory ->151861872\n","2021-11-10 13:29:33,849 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000029_0 decomp: 10091534 len: 10091538 to MEMORY\n","2021-11-10 13:29:33,858 INFO reduce.InMemoryMapOutput: Read 10091534 bytes from map-output for attempt_local1867248162_0001_m_000029_0\n","2021-11-10 13:29:33,859 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10091534, inMemoryMapOutputs.size() -> 16, commitMemory -> 151861872, usedMemory ->161953406\n","2021-11-10 13:29:33,862 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000002_0 decomp: 10094108 len: 10094112 to MEMORY\n","2021-11-10 13:29:34,028 INFO reduce.InMemoryMapOutput: Read 10094108 bytes from map-output for attempt_local1867248162_0001_m_000002_0\n","2021-11-10 13:29:34,028 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10094108, inMemoryMapOutputs.size() -> 17, commitMemory -> 161953406, usedMemory ->172047514\n","2021-11-10 13:29:34,031 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000028_0 decomp: 10203712 len: 10203716 to MEMORY\n","2021-11-10 13:29:34,040 INFO reduce.InMemoryMapOutput: Read 10203712 bytes from map-output for attempt_local1867248162_0001_m_000028_0\n","2021-11-10 13:29:34,040 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10203712, inMemoryMapOutputs.size() -> 18, commitMemory -> 172047514, usedMemory ->182251226\n","2021-11-10 13:29:34,044 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000027_0 decomp: 10257370 len: 10257374 to MEMORY\n","2021-11-10 13:29:34,053 INFO reduce.InMemoryMapOutput: Read 10257370 bytes from map-output for attempt_local1867248162_0001_m_000027_0\n","2021-11-10 13:29:34,053 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10257370, inMemoryMapOutputs.size() -> 19, commitMemory -> 182251226, usedMemory ->192508596\n","2021-11-10 13:29:34,056 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000053_0 decomp: 10101159 len: 10101163 to MEMORY\n","2021-11-10 13:29:34,064 INFO reduce.InMemoryMapOutput: Read 10101159 bytes from map-output for attempt_local1867248162_0001_m_000053_0\n","2021-11-10 13:29:34,064 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10101159, inMemoryMapOutputs.size() -> 20, commitMemory -> 192508596, usedMemory ->202609755\n","2021-11-10 13:29:34,067 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000026_0 decomp: 10048117 len: 10048121 to MEMORY\n","2021-11-10 13:29:34,075 INFO reduce.InMemoryMapOutput: Read 10048117 bytes from map-output for attempt_local1867248162_0001_m_000026_0\n","2021-11-10 13:29:34,075 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10048117, inMemoryMapOutputs.size() -> 21, commitMemory -> 202609755, usedMemory ->212657872\n","2021-11-10 13:29:34,079 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000001_0 decomp: 9969159 len: 9969163 to MEMORY\n","2021-11-10 13:29:34,227 INFO reduce.InMemoryMapOutput: Read 9969159 bytes from map-output for attempt_local1867248162_0001_m_000001_0\n","2021-11-10 13:29:34,227 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9969159, inMemoryMapOutputs.size() -> 22, commitMemory -> 212657872, usedMemory ->222627031\n","2021-11-10 13:29:34,230 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000052_0 decomp: 10147667 len: 10147671 to MEMORY\n","2021-11-10 13:29:34,238 INFO reduce.InMemoryMapOutput: Read 10147667 bytes from map-output for attempt_local1867248162_0001_m_000052_0\n","2021-11-10 13:29:34,238 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10147667, inMemoryMapOutputs.size() -> 23, commitMemory -> 222627031, usedMemory ->232774698\n","2021-11-10 13:29:34,240 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000051_0 decomp: 10184660 len: 10184664 to MEMORY\n","2021-11-10 13:29:34,256 INFO reduce.InMemoryMapOutput: Read 10184660 bytes from map-output for attempt_local1867248162_0001_m_000051_0\n","2021-11-10 13:29:34,256 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10184660, inMemoryMapOutputs.size() -> 24, commitMemory -> 232774698, usedMemory ->242959358\n","2021-11-10 13:29:34,266 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000000_0 decomp: 10091864 len: 10091868 to MEMORY\n","2021-11-10 13:29:34,424 INFO reduce.InMemoryMapOutput: Read 10091864 bytes from map-output for attempt_local1867248162_0001_m_000000_0\n","2021-11-10 13:29:34,424 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10091864, inMemoryMapOutputs.size() -> 25, commitMemory -> 242959358, usedMemory ->253051222\n","2021-11-10 13:29:34,428 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000050_0 decomp: 10145181 len: 10145185 to MEMORY\n","2021-11-10 13:29:34,440 INFO reduce.InMemoryMapOutput: Read 10145181 bytes from map-output for attempt_local1867248162_0001_m_000050_0\n","2021-11-10 13:29:34,440 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10145181, inMemoryMapOutputs.size() -> 26, commitMemory -> 253051222, usedMemory ->263196403\n","2021-11-10 13:29:34,457 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000025_0 decomp: 10128516 len: 10128520 to MEMORY\n","2021-11-10 13:29:34,470 INFO reduce.InMemoryMapOutput: Read 10128516 bytes from map-output for attempt_local1867248162_0001_m_000025_0\n","2021-11-10 13:29:34,470 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10128516, inMemoryMapOutputs.size() -> 27, commitMemory -> 263196403, usedMemory ->273324919\n","2021-11-10 13:29:34,474 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000024_0 decomp: 9904545 len: 9904549 to MEMORY\n","2021-11-10 13:29:34,522 INFO reduce.InMemoryMapOutput: Read 9904545 bytes from map-output for attempt_local1867248162_0001_m_000024_0\n","2021-11-10 13:29:34,522 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9904545, inMemoryMapOutputs.size() -> 28, commitMemory -> 273324919, usedMemory ->283229464\n","2021-11-10 13:29:34,528 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000023_0 decomp: 9914918 len: 9914922 to MEMORY\n","2021-11-10 13:29:34,726 INFO reduce.InMemoryMapOutput: Read 9914918 bytes from map-output for attempt_local1867248162_0001_m_000023_0\n","2021-11-10 13:29:34,727 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9914918, inMemoryMapOutputs.size() -> 29, commitMemory -> 283229464, usedMemory ->293144382\n","2021-11-10 13:29:34,743 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000049_0 decomp: 9879278 len: 9879282 to MEMORY\n","2021-11-10 13:29:34,755 INFO reduce.InMemoryMapOutput: Read 9879278 bytes from map-output for attempt_local1867248162_0001_m_000049_0\n","2021-11-10 13:29:34,755 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9879278, inMemoryMapOutputs.size() -> 30, commitMemory -> 293144382, usedMemory ->303023660\n","2021-11-10 13:29:34,759 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000035_0 decomp: 10332423 len: 10332427 to MEMORY\n","2021-11-10 13:29:34,772 INFO reduce.InMemoryMapOutput: Read 10332423 bytes from map-output for attempt_local1867248162_0001_m_000035_0\n","2021-11-10 13:29:34,772 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10332423, inMemoryMapOutputs.size() -> 31, commitMemory -> 303023660, usedMemory ->313356083\n","2021-11-10 13:29:34,777 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000034_0 decomp: 10152771 len: 10152775 to MEMORY\n","2021-11-10 13:29:34,790 INFO reduce.InMemoryMapOutput: Read 10152771 bytes from map-output for attempt_local1867248162_0001_m_000034_0\n","2021-11-10 13:29:34,790 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10152771, inMemoryMapOutputs.size() -> 32, commitMemory -> 313356083, usedMemory ->323508854\n","2021-11-10 13:29:34,795 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000009_0 decomp: 9876187 len: 9876191 to MEMORY\n","2021-11-10 13:29:34,932 INFO reduce.InMemoryMapOutput: Read 9876187 bytes from map-output for attempt_local1867248162_0001_m_000009_0\n","2021-11-10 13:29:34,932 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9876187, inMemoryMapOutputs.size() -> 33, commitMemory -> 323508854, usedMemory ->333385041\n","2021-11-10 13:29:34,943 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000008_0 decomp: 10261429 len: 10261433 to MEMORY\n","2021-11-10 13:29:35,114 INFO reduce.InMemoryMapOutput: Read 10261429 bytes from map-output for attempt_local1867248162_0001_m_000008_0\n","2021-11-10 13:29:35,114 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10261429, inMemoryMapOutputs.size() -> 34, commitMemory -> 333385041, usedMemory ->343646470\n","2021-11-10 13:29:35,126 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000007_0 decomp: 10100862 len: 10100866 to MEMORY\n","2021-11-10 13:29:35,321 INFO reduce.InMemoryMapOutput: Read 10100862 bytes from map-output for attempt_local1867248162_0001_m_000007_0\n","2021-11-10 13:29:35,322 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10100862, inMemoryMapOutputs.size() -> 35, commitMemory -> 343646470, usedMemory ->353747332\n","2021-11-10 13:29:35,331 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000058_0 decomp: 6485712 len: 6485716 to MEMORY\n","2021-11-10 13:29:35,339 INFO reduce.InMemoryMapOutput: Read 6485712 bytes from map-output for attempt_local1867248162_0001_m_000058_0\n","2021-11-10 13:29:35,339 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 6485712, inMemoryMapOutputs.size() -> 36, commitMemory -> 353747332, usedMemory ->360233044\n","2021-11-10 13:29:35,343 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000033_0 decomp: 9912212 len: 9912216 to MEMORY\n","2021-11-10 13:29:35,355 INFO reduce.InMemoryMapOutput: Read 9912212 bytes from map-output for attempt_local1867248162_0001_m_000033_0\n","2021-11-10 13:29:35,355 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9912212, inMemoryMapOutputs.size() -> 37, commitMemory -> 360233044, usedMemory ->370145256\n","2021-11-10 13:29:35,370 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000006_0 decomp: 10105042 len: 10105046 to MEMORY\n","2021-11-10 13:29:35,424 INFO reduce.InMemoryMapOutput: Read 10105042 bytes from map-output for attempt_local1867248162_0001_m_000006_0\n","2021-11-10 13:29:35,424 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10105042, inMemoryMapOutputs.size() -> 38, commitMemory -> 370145256, usedMemory ->380250298\n","2021-11-10 13:29:35,428 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000032_0 decomp: 9970523 len: 9970527 to MEMORY\n","2021-11-10 13:29:35,439 INFO reduce.InMemoryMapOutput: Read 9970523 bytes from map-output for attempt_local1867248162_0001_m_000032_0\n","2021-11-10 13:29:35,439 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9970523, inMemoryMapOutputs.size() -> 39, commitMemory -> 380250298, usedMemory ->390220821\n","2021-11-10 13:29:35,444 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000031_0 decomp: 10145137 len: 10145141 to MEMORY\n","2021-11-10 13:29:35,458 INFO reduce.InMemoryMapOutput: Read 10145137 bytes from map-output for attempt_local1867248162_0001_m_000031_0\n","2021-11-10 13:29:35,458 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10145137, inMemoryMapOutputs.size() -> 40, commitMemory -> 390220821, usedMemory ->400365958\n","2021-11-10 13:29:35,469 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000057_0 decomp: 10099498 len: 10099502 to MEMORY\n","2021-11-10 13:29:35,481 INFO reduce.InMemoryMapOutput: Read 10099498 bytes from map-output for attempt_local1867248162_0001_m_000057_0\n","2021-11-10 13:29:35,481 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10099498, inMemoryMapOutputs.size() -> 41, commitMemory -> 400365958, usedMemory ->410465456\n","2021-11-10 13:29:35,488 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000030_0 decomp: 10160592 len: 10160596 to MEMORY\n","2021-11-10 13:29:35,500 INFO reduce.InMemoryMapOutput: Read 10160592 bytes from map-output for attempt_local1867248162_0001_m_000030_0\n","2021-11-10 13:29:35,500 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10160592, inMemoryMapOutputs.size() -> 42, commitMemory -> 410465456, usedMemory ->420626048\n","2021-11-10 13:29:35,505 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000005_0 decomp: 10128395 len: 10128399 to MEMORY\n","2021-11-10 13:29:35,646 INFO reduce.InMemoryMapOutput: Read 10128395 bytes from map-output for attempt_local1867248162_0001_m_000005_0\n","2021-11-10 13:29:35,647 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10128395, inMemoryMapOutputs.size() -> 43, commitMemory -> 420626048, usedMemory ->430754443\n","2021-11-10 13:29:35,667 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000056_0 decomp: 10183934 len: 10183938 to MEMORY\n","2021-11-10 13:29:35,683 INFO reduce.InMemoryMapOutput: Read 10183934 bytes from map-output for attempt_local1867248162_0001_m_000056_0\n","2021-11-10 13:29:35,683 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10183934, inMemoryMapOutputs.size() -> 44, commitMemory -> 430754443, usedMemory ->440938377\n","2021-11-10 13:29:35,692 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000055_0 decomp: 10191612 len: 10191616 to MEMORY\n","2021-11-10 13:29:35,707 INFO reduce.InMemoryMapOutput: Read 10191612 bytes from map-output for attempt_local1867248162_0001_m_000055_0\n","2021-11-10 13:29:35,707 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10191612, inMemoryMapOutputs.size() -> 45, commitMemory -> 440938377, usedMemory ->451129989\n","2021-11-10 13:29:35,712 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000004_0 decomp: 10142497 len: 10142501 to MEMORY\n","2021-11-10 13:29:35,824 INFO reduce.InMemoryMapOutput: Read 10142497 bytes from map-output for attempt_local1867248162_0001_m_000004_0\n","2021-11-10 13:29:35,825 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10142497, inMemoryMapOutputs.size() -> 46, commitMemory -> 451129989, usedMemory ->461272486\n","2021-11-10 13:29:35,836 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000016_0 decomp: 10002335 len: 10002339 to MEMORY\n","2021-11-10 13:29:36,041 INFO reduce.InMemoryMapOutput: Read 10002335 bytes from map-output for attempt_local1867248162_0001_m_000016_0\n","2021-11-10 13:29:36,041 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10002335, inMemoryMapOutputs.size() -> 47, commitMemory -> 461272486, usedMemory ->471274821\n","2021-11-10 13:29:36,050 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000015_0 decomp: 9997825 len: 9997829 to MEMORY\n","2021-11-10 13:29:36,224 INFO reduce.InMemoryMapOutput: Read 9997825 bytes from map-output for attempt_local1867248162_0001_m_000015_0\n","2021-11-10 13:29:36,224 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9997825, inMemoryMapOutputs.size() -> 48, commitMemory -> 471274821, usedMemory ->481272646\n","2021-11-10 13:29:36,236 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000041_0 decomp: 10056477 len: 10056481 to MEMORY\n","2021-11-10 13:29:36,248 INFO reduce.InMemoryMapOutput: Read 10056477 bytes from map-output for attempt_local1867248162_0001_m_000041_0\n","2021-11-10 13:29:36,248 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10056477, inMemoryMapOutputs.size() -> 49, commitMemory -> 481272646, usedMemory ->491329123\n","2021-11-10 13:29:36,253 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000014_0 decomp: 9959391 len: 9959395 to MEMORY\n","2021-11-10 13:29:36,426 INFO reduce.InMemoryMapOutput: Read 9959391 bytes from map-output for attempt_local1867248162_0001_m_000014_0\n","2021-11-10 13:29:36,426 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9959391, inMemoryMapOutputs.size() -> 50, commitMemory -> 491329123, usedMemory ->501288514\n","2021-11-10 13:29:36,434 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000040_0 decomp: 9789144 len: 9789148 to MEMORY\n","2021-11-10 13:29:36,446 INFO reduce.InMemoryMapOutput: Read 9789144 bytes from map-output for attempt_local1867248162_0001_m_000040_0\n","2021-11-10 13:29:36,449 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9789144, inMemoryMapOutputs.size() -> 51, commitMemory -> 501288514, usedMemory ->511077658\n","2021-11-10 13:29:36,455 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000039_0 decomp: 10194791 len: 10194795 to MEMORY\n","2021-11-10 13:29:36,470 INFO reduce.InMemoryMapOutput: Read 10194791 bytes from map-output for attempt_local1867248162_0001_m_000039_0\n","2021-11-10 13:29:36,470 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10194791, inMemoryMapOutputs.size() -> 52, commitMemory -> 511077658, usedMemory ->521272449\n","2021-11-10 13:29:36,474 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000038_0 decomp: 10148789 len: 10148793 to MEMORY\n","2021-11-10 13:29:36,497 INFO reduce.InMemoryMapOutput: Read 10148789 bytes from map-output for attempt_local1867248162_0001_m_000038_0\n","2021-11-10 13:29:36,497 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10148789, inMemoryMapOutputs.size() -> 53, commitMemory -> 521272449, usedMemory ->531421238\n","2021-11-10 13:29:36,502 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000013_0 decomp: 10199554 len: 10199558 to MEMORY\n","2021-11-10 13:29:36,644 INFO reduce.InMemoryMapOutput: Read 10199554 bytes from map-output for attempt_local1867248162_0001_m_000013_0\n","2021-11-10 13:29:36,645 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10199554, inMemoryMapOutputs.size() -> 54, commitMemory -> 531421238, usedMemory ->541620792\n","2021-11-10 13:29:36,656 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000012_0 decomp: 10239638 len: 10239642 to MEMORY\n","2021-11-10 13:29:36,827 INFO reduce.InMemoryMapOutput: Read 10239638 bytes from map-output for attempt_local1867248162_0001_m_000012_0\n","2021-11-10 13:29:36,827 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10239638, inMemoryMapOutputs.size() -> 55, commitMemory -> 541620792, usedMemory ->551860430\n","2021-11-10 13:29:36,837 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000011_0 decomp: 9947863 len: 9947867 to MEMORY\n","2021-11-10 13:29:37,039 INFO reduce.InMemoryMapOutput: Read 9947863 bytes from map-output for attempt_local1867248162_0001_m_000011_0\n","2021-11-10 13:29:37,039 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 9947863, inMemoryMapOutputs.size() -> 56, commitMemory -> 551860430, usedMemory ->561808293\n","2021-11-10 13:29:37,048 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000037_0 decomp: 10230618 len: 10230622 to MEMORY\n","2021-11-10 13:29:37,060 INFO reduce.InMemoryMapOutput: Read 10230618 bytes from map-output for attempt_local1867248162_0001_m_000037_0\n","2021-11-10 13:29:37,060 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10230618, inMemoryMapOutputs.size() -> 57, commitMemory -> 561808293, usedMemory ->572038911\n","2021-11-10 13:29:37,065 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000010_0 decomp: 10008011 len: 10008015 to MEMORY\n","2021-11-10 13:29:37,221 INFO reduce.InMemoryMapOutput: Read 10008011 bytes from map-output for attempt_local1867248162_0001_m_000010_0\n","2021-11-10 13:29:37,221 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10008011, inMemoryMapOutputs.size() -> 58, commitMemory -> 572038911, usedMemory ->582046922\n","2021-11-10 13:29:37,228 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1867248162_0001_m_000036_0 decomp: 10320158 len: 10320162 to MEMORY\n","2021-11-10 13:29:37,243 INFO reduce.InMemoryMapOutput: Read 10320158 bytes from map-output for attempt_local1867248162_0001_m_000036_0\n","2021-11-10 13:29:37,243 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10320158, inMemoryMapOutputs.size() -> 59, commitMemory -> 582046922, usedMemory ->592367080\n","2021-11-10 13:29:37,243 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2021-11-10 13:29:37,244 INFO mapred.LocalJobRunner: 59 / 59 copied.\n","2021-11-10 13:29:37,245 INFO reduce.MergeManagerImpl: finalMerge called with 59 in-memory map-outputs and 0 on-disk map-outputs\n","2021-11-10 13:29:37,269 INFO mapred.Merger: Merging 59 sorted segments\n","2021-11-10 13:29:37,279 INFO mapred.Merger: Down to the last merge-pass, with 59 segments left of total size: 592366490 bytes\n","2021-11-10 13:29:44,527 INFO mapred.LocalJobRunner: reduce > sort\n","2021-11-10 13:29:44,976 INFO mapreduce.Job:  map 100% reduce 45%\n","2021-11-10 13:29:50,528 INFO mapred.LocalJobRunner: reduce > sort\n","2021-11-10 13:29:50,978 INFO mapreduce.Job:  map 100% reduce 55%\n","2021-11-10 13:29:56,529 INFO mapred.LocalJobRunner: reduce > sort\n","2021-11-10 13:29:56,982 INFO mapreduce.Job:  map 100% reduce 65%\n","2021-11-10 13:29:57,796 INFO reduce.MergeManagerImpl: Merged 59 segments, 592367080 bytes to disk to satisfy reduce memory limit\n","2021-11-10 13:29:57,797 INFO reduce.MergeManagerImpl: Merging 1 files, 592366968 bytes from disk\n","2021-11-10 13:29:57,800 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2021-11-10 13:29:57,800 INFO mapred.Merger: Merging 1 sorted segments\n","2021-11-10 13:29:57,801 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 592366954 bytes\n","2021-11-10 13:29:57,802 INFO mapred.LocalJobRunner: reduce > sort\n","2021-11-10 13:29:57,812 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n","2021-11-10 13:29:57,814 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2021-11-10 13:29:57,814 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2021-11-10 13:29:58,223 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:58,229 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:58,233 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:58,256 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:58,302 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-10 13:29:58,318 INFO streaming.PipeMapRed: Records R/W=14564/1\n","2021-11-10 13:29:58,861 INFO streaming.PipeMapRed: R/W/S=100000/79075/0 in:100000=100000/1 [rec/s] out:79075=79075/1 [rec/s]\n","2021-11-10 13:29:59,443 INFO streaming.PipeMapRed: R/W/S=200000/181091/0 in:200000=200000/1 [rec/s] out:181091=181091/1 [rec/s]\n","2021-11-10 13:29:59,900 INFO streaming.PipeMapRed: R/W/S=300000/284598/0 in:150000=300000/2 [rec/s] out:142299=284598/2 [rec/s]\n","2021-11-10 13:30:00,569 INFO streaming.PipeMapRed: R/W/S=400000/386987/0 in:200000=400000/2 [rec/s] out:193493=386987/2 [rec/s]\n","2021-11-10 13:30:01,198 INFO streaming.PipeMapRed: R/W/S=500000/486951/0 in:166666=500000/3 [rec/s] out:162317=486951/3 [rec/s]\n","2021-11-10 13:30:01,659 INFO streaming.PipeMapRed: R/W/S=600000/588780/0 in:200000=600000/3 [rec/s] out:196260=588780/3 [rec/s]\n","2021-11-10 13:30:01,968 INFO streaming.PipeMapRed: R/W/S=700000/690795/0 in:175000=700000/4 [rec/s] out:172698=690795/4 [rec/s]\n","2021-11-10 13:30:02,267 INFO streaming.PipeMapRed: R/W/S=800000/780688/0 in:200000=800000/4 [rec/s] out:195172=780688/4 [rec/s]\n","2021-11-10 13:30:02,532 INFO mapred.LocalJobRunner: Records R/W=14564/1 > reduce\n","2021-11-10 13:30:02,639 INFO streaming.PipeMapRed: R/W/S=900000/880093/0 in:225000=900000/4 [rec/s] out:220023=880093/4 [rec/s]\n","2021-11-10 13:30:02,957 INFO streaming.PipeMapRed: R/W/S=1000000/983608/0 in:200000=1000000/5 [rec/s] out:196722=983613/5 [rec/s]\n","2021-11-10 13:30:02,986 INFO mapreduce.Job:  map 100% reduce 67%\n","2021-11-10 13:30:03,286 INFO streaming.PipeMapRed: R/W/S=1100000/1084124/0 in:220000=1100000/5 [rec/s] out:216824=1084124/5 [rec/s]\n","2021-11-10 13:30:03,611 INFO streaming.PipeMapRed: R/W/S=1200000/1185865/0 in:240000=1200000/5 [rec/s] out:237173=1185865/5 [rec/s]\n","2021-11-10 13:30:04,030 INFO streaming.PipeMapRed: R/W/S=1300000/1289274/0 in:216666=1300000/6 [rec/s] out:214879=1289274/6 [rec/s]\n","2021-11-10 13:30:04,335 INFO streaming.PipeMapRed: R/W/S=1400000/1389797/0 in:233333=1400000/6 [rec/s] out:231632=1389797/6 [rec/s]\n","2021-11-10 13:30:04,663 INFO streaming.PipeMapRed: R/W/S=1500000/1491626/0 in:250000=1500000/6 [rec/s] out:248604=1491626/6 [rec/s]\n","2021-11-10 13:30:04,952 INFO streaming.PipeMapRed: R/W/S=1600000/1580970/0 in:228571=1600000/7 [rec/s] out:225852=1580970/7 [rec/s]\n","2021-11-10 13:30:05,243 INFO streaming.PipeMapRed: R/W/S=1700000/1681110/0 in:242857=1700000/7 [rec/s] out:240158=1681110/7 [rec/s]\n","2021-11-10 13:30:05,603 INFO streaming.PipeMapRed: R/W/S=1800000/1782939/0 in:257142=1800000/7 [rec/s] out:254705=1782939/7 [rec/s]\n","2021-11-10 13:30:05,976 INFO streaming.PipeMapRed: R/W/S=1900000/1886260/0 in:237500=1900000/8 [rec/s] out:235782=1886260/8 [rec/s]\n","2021-11-10 13:30:06,269 INFO streaming.PipeMapRed: R/W/S=2000000/1986784/0 in:250000=2000000/8 [rec/s] out:248348=1986784/8 [rec/s]\n","2021-11-10 13:30:06,569 INFO streaming.PipeMapRed: R/W/S=2100000/2088788/0 in:262500=2100000/8 [rec/s] out:261098=2088788/8 [rec/s]\n","2021-11-10 13:30:06,967 INFO streaming.PipeMapRed: R/W/S=2200000/2190628/0 in:244444=2200000/9 [rec/s] out:243403=2190628/9 [rec/s]\n","2021-11-10 13:30:07,250 INFO streaming.PipeMapRed: R/W/S=2300000/2279812/0 in:255555=2300000/9 [rec/s] out:253312=2279812/9 [rec/s]\n","2021-11-10 13:30:07,579 INFO streaming.PipeMapRed: R/W/S=2400000/2386640/0 in:266666=2400000/9 [rec/s] out:265182=2386640/9 [rec/s]\n","2021-11-10 13:30:07,880 INFO streaming.PipeMapRed: R/W/S=2500000/2484179/0 in:250000=2500000/10 [rec/s] out:248417=2484179/10 [rec/s]\n","2021-11-10 13:30:08,188 INFO streaming.PipeMapRed: R/W/S=2600000/2584889/0 in:260000=2600000/10 [rec/s] out:258488=2584889/10 [rec/s]\n","2021-11-10 13:30:08,319 INFO streaming.PipeMapRed: Records R/W=2621341/2612938\n","2021-11-10 13:30:08,533 INFO mapred.LocalJobRunner: Records R/W=2621341/2612938 > reduce\n","2021-11-10 13:30:08,566 INFO streaming.PipeMapRed: R/W/S=2700000/2685784/0 in:270000=2700000/10 [rec/s] out:268578=2685784/10 [rec/s]\n","2021-11-10 13:30:09,012 INFO streaming.PipeMapRed: R/W/S=2800000/2787801/0 in:254545=2800000/11 [rec/s] out:253436=2787801/11 [rec/s]\n","2021-11-10 13:30:09,331 INFO streaming.PipeMapRed: R/W/S=2900000/2890003/0 in:263636=2900000/11 [rec/s] out:262727=2890003/11 [rec/s]\n","2021-11-10 13:30:09,629 INFO streaming.PipeMapRed: R/W/S=3000000/2991330/0 in:272727=3000000/11 [rec/s] out:271939=2991330/11 [rec/s]\n","2021-11-10 13:30:09,971 INFO streaming.PipeMapRed: R/W/S=3100000/3080986/0 in:258333=3100000/12 [rec/s] out:256749=3080990/12 [rec/s]\n","2021-11-10 13:30:10,268 INFO streaming.PipeMapRed: R/W/S=3200000/3182249/0 in:266666=3200000/12 [rec/s] out:265187=3182249/12 [rec/s]\n","2021-11-10 13:30:10,588 INFO streaming.PipeMapRed: R/W/S=3300000/3282959/0 in:275000=3300000/12 [rec/s] out:273579=3282959/12 [rec/s]\n","2021-11-10 13:30:10,922 INFO streaming.PipeMapRed: R/W/S=3400000/3385152/0 in:261538=3400000/13 [rec/s] out:260396=3385157/13 [rec/s]\n","2021-11-10 13:30:11,241 INFO streaming.PipeMapRed: R/W/S=3500000/3488295/0 in:269230=3500000/13 [rec/s] out:268330=3488295/13 [rec/s]\n","2021-11-10 13:30:11,603 INFO streaming.PipeMapRed: R/W/S=3600000/3588584/0 in:276923=3600000/13 [rec/s] out:276044=3588584/13 [rec/s]\n","2021-11-10 13:30:11,906 INFO streaming.PipeMapRed: R/W/S=3700000/3690626/0 in:264285=3700000/14 [rec/s] out:263616=3690626/14 [rec/s]\n","2021-11-10 13:30:12,174 INFO streaming.PipeMapRed: R/W/S=3800000/3780541/0 in:271428=3800000/14 [rec/s] out:270038=3780541/14 [rec/s]\n","2021-11-10 13:30:12,498 INFO streaming.PipeMapRed: R/W/S=3900000/3882183/0 in:278571=3900000/14 [rec/s] out:277298=3882183/14 [rec/s]\n","2021-11-10 13:30:12,793 INFO streaming.PipeMapRed: R/W/S=4000000/3982707/0 in:285714=4000000/14 [rec/s] out:284479=3982707/14 [rec/s]\n","2021-11-10 13:30:13,098 INFO streaming.PipeMapRed: R/W/S=4100000/4083883/0 in:273333=4100000/15 [rec/s] out:272258=4083883/15 [rec/s]\n","2021-11-10 13:30:13,400 INFO streaming.PipeMapRed: R/W/S=4200000/4185805/0 in:280000=4200000/15 [rec/s] out:279053=4185805/15 [rec/s]\n","2021-11-10 13:30:13,717 INFO streaming.PipeMapRed: R/W/S=4300000/4287821/0 in:286666=4300000/15 [rec/s] out:285854=4287821/15 [rec/s]\n","2021-11-10 13:30:14,008 INFO streaming.PipeMapRed: R/W/S=4400000/4389650/0 in:275000=4400000/16 [rec/s] out:274353=4389650/16 [rec/s]\n","2021-11-10 13:30:14,303 INFO streaming.PipeMapRed: R/W/S=4500000/4491665/0 in:281250=4500000/16 [rec/s] out:280729=4491665/16 [rec/s]\n","2021-11-10 13:30:14,534 INFO mapred.LocalJobRunner: Records R/W=2621341/2612938 > reduce\n","2021-11-10 13:30:14,579 INFO streaming.PipeMapRed: R/W/S=4600000/4580107/0 in:287500=4600000/16 [rec/s] out:286257=4580113/16 [rec/s]\n","2021-11-10 13:30:14,881 INFO streaming.PipeMapRed: R/W/S=4700000/4680890/0 in:276470=4700000/17 [rec/s] out:275346=4680890/17 [rec/s]\n","2021-11-10 13:30:15,221 INFO streaming.PipeMapRed: R/W/S=4800000/4782978/0 in:282352=4800000/17 [rec/s] out:281351=4782978/17 [rec/s]\n","2021-11-10 13:30:15,542 INFO streaming.PipeMapRed: R/W/S=4900000/4887593/0 in:288235=4900000/17 [rec/s] out:287505=4887593/17 [rec/s]\n","2021-11-10 13:30:15,846 INFO streaming.PipeMapRed: R/W/S=5000000/4988128/0 in:277777=5000000/18 [rec/s] out:277118=4988128/18 [rec/s]\n","2021-11-10 13:30:16,149 INFO streaming.PipeMapRed: R/W/S=5100000/5092258/0 in:283333=5100000/18 [rec/s] out:282905=5092302/18 [rec/s]\n","2021-11-10 13:30:16,434 INFO streaming.PipeMapRed: R/W/S=5200000/5190108/0 in:288888=5200000/18 [rec/s] out:288339=5190108/18 [rec/s]\n","2021-11-10 13:30:16,718 INFO streaming.PipeMapRed: R/W/S=5300000/5278136/0 in:294444=5300000/18 [rec/s] out:293229=5278136/18 [rec/s]\n","2021-11-10 13:30:17,034 INFO streaming.PipeMapRed: R/W/S=5400000/5381270/0 in:284210=5400000/19 [rec/s] out:283224=5381270/19 [rec/s]\n","2021-11-10 13:30:17,320 INFO streaming.PipeMapRed: R/W/S=5500000/5483854/0 in:289473=5500000/19 [rec/s] out:288626=5483894/19 [rec/s]\n","2021-11-10 13:30:17,643 INFO streaming.PipeMapRed: R/W/S=5600000/5583809/0 in:294736=5600000/19 [rec/s] out:293884=5583809/19 [rec/s]\n","2021-11-10 13:30:17,935 INFO streaming.PipeMapRed: R/W/S=5700000/5686763/0 in:285000=5700000/20 [rec/s] out:284338=5686763/20 [rec/s]\n","2021-11-10 13:30:18,254 INFO streaming.PipeMapRed: R/W/S=5800000/5790451/0 in:290000=5800000/20 [rec/s] out:289522=5790451/20 [rec/s]\n","2021-11-10 13:30:18,320 INFO streaming.PipeMapRed: Records R/W=5825201/5812524\n","2021-11-10 13:30:18,537 INFO streaming.PipeMapRed: R/W/S=5900000/5889669/0 in:295000=5900000/20 [rec/s] out:294483=5889669/20 [rec/s]\n","2021-11-10 13:30:18,849 INFO streaming.PipeMapRed: R/W/S=6000000/5991656/0 in:285714=6000000/21 [rec/s] out:285316=5991656/21 [rec/s]\n","2021-11-10 13:30:19,199 INFO streaming.PipeMapRed: R/W/S=6100000/6084748/0 in:290476=6100000/21 [rec/s] out:289749=6084748/21 [rec/s]\n","2021-11-10 13:30:19,513 INFO streaming.PipeMapRed: R/W/S=6200000/6188559/0 in:295238=6200000/21 [rec/s] out:294693=6188559/21 [rec/s]\n","2021-11-10 13:30:19,844 INFO streaming.PipeMapRed: R/W/S=6300000/6287011/0 in:286363=6300000/22 [rec/s] out:285773=6287011/22 [rec/s]\n","2021-11-10 13:30:20,142 INFO streaming.PipeMapRed: R/W/S=6400000/6385944/0 in:290909=6400000/22 [rec/s] out:290270=6385944/22 [rec/s]\n","2021-11-10 13:30:20,440 INFO streaming.PipeMapRed: R/W/S=6500000/6486777/0 in:295454=6500000/22 [rec/s] out:294853=6486781/22 [rec/s]\n","2021-11-10 13:30:20,535 INFO mapred.LocalJobRunner: Records R/W=5825201/5812524 > reduce\n","2021-11-10 13:30:20,768 INFO streaming.PipeMapRed: R/W/S=6600000/6588671/0 in:300000=6600000/22 [rec/s] out:299485=6588671/22 [rec/s]\n","2021-11-10 13:30:21,067 INFO streaming.PipeMapRed: R/W/S=6700000/6690687/0 in:291304=6700000/23 [rec/s] out:290899=6690687/23 [rec/s]\n","2021-11-10 13:30:21,340 INFO streaming.PipeMapRed: R/W/S=6800000/6780766/0 in:295652=6800000/23 [rec/s] out:294815=6780766/23 [rec/s]\n","2021-11-10 13:30:21,634 INFO streaming.PipeMapRed: R/W/S=6900000/6880895/0 in:300000=6900000/23 [rec/s] out:299169=6880895/23 [rec/s]\n","2021-11-10 13:30:22,013 INFO streaming.PipeMapRed: R/W/S=7000000/6984232/0 in:291666=7000000/24 [rec/s] out:291009=6984234/24 [rec/s]\n","2021-11-10 13:30:22,364 INFO streaming.PipeMapRed: R/W/S=7100000/7083828/0 in:295833=7100000/24 [rec/s] out:295159=7083828/24 [rec/s]\n","2021-11-10 13:30:22,696 INFO streaming.PipeMapRed: R/W/S=7200000/7187150/0 in:300000=7200000/24 [rec/s] out:299464=7187150/24 [rec/s]\n","2021-11-10 13:30:22,982 INFO streaming.PipeMapRed: R/W/S=7300000/7287759/0 in:292000=7300000/25 [rec/s] out:291510=7287759/25 [rec/s]\n","2021-11-10 13:30:23,282 INFO streaming.PipeMapRed: R/W/S=7400000/7389661/0 in:296000=7400000/25 [rec/s] out:295586=7389661/25 [rec/s]\n","2021-11-10 13:30:23,588 INFO streaming.PipeMapRed: R/W/S=7500000/7491634/0 in:300000=7500000/25 [rec/s] out:299665=7491634/25 [rec/s]\n","2021-11-10 13:30:23,870 INFO streaming.PipeMapRed: R/W/S=7600000/7580851/0 in:292307=7600000/26 [rec/s] out:291571=7580851/26 [rec/s]\n","2021-11-10 13:30:24,170 INFO streaming.PipeMapRed: R/W/S=7700000/7680977/0 in:296153=7700000/26 [rec/s] out:295422=7680977/26 [rec/s]\n","2021-11-10 13:30:24,477 INFO streaming.PipeMapRed: R/W/S=7800000/7783153/0 in:300000=7800000/26 [rec/s] out:299353=7783195/26 [rec/s]\n","2021-11-10 13:30:24,836 INFO streaming.PipeMapRed: R/W/S=7900000/7884846/0 in:292592=7900000/27 [rec/s] out:292031=7884846/27 [rec/s]\n","2021-11-10 13:30:25,175 INFO streaming.PipeMapRed: R/W/S=8000000/7986862/0 in:296296=8000000/27 [rec/s] out:295809=7986862/27 [rec/s]\n","2021-11-10 13:30:25,545 INFO streaming.PipeMapRed: R/W/S=8100000/8088504/0 in:300000=8100000/27 [rec/s] out:299574=8088504/27 [rec/s]\n","2021-11-10 13:30:25,855 INFO streaming.PipeMapRed: R/W/S=8200000/8190706/0 in:292857=8200000/28 [rec/s] out:292525=8190706/28 [rec/s]\n","2021-11-10 13:30:26,127 INFO streaming.PipeMapRed: R/W/S=8300000/8278083/0 in:296428=8300000/28 [rec/s] out:295646=8278088/28 [rec/s]\n","2021-11-10 13:30:26,423 INFO streaming.PipeMapRed: R/W/S=8400000/8380004/0 in:300000=8400000/28 [rec/s] out:299285=8380004/28 [rec/s]\n","2021-11-10 13:30:26,536 INFO mapred.LocalJobRunner: Records R/W=5825201/5812524 > reduce\n","2021-11-10 13:30:26,739 INFO streaming.PipeMapRed: R/W/S=8500000/8488733/0 in:303571=8500000/28 [rec/s] out:303169=8488733/28 [rec/s]\n","2021-11-10 13:30:27,049 INFO streaming.PipeMapRed: R/W/S=8600000/8587392/0 in:296551=8600000/29 [rec/s] out:296116=8587392/29 [rec/s]\n","2021-11-10 13:30:27,337 INFO streaming.PipeMapRed: R/W/S=8700000/8685646/0 in:300000=8700000/29 [rec/s] out:299506=8685681/29 [rec/s]\n","2021-11-10 13:30:27,645 INFO streaming.PipeMapRed: R/W/S=8800000/8790677/0 in:303448=8800000/29 [rec/s] out:303126=8790677/29 [rec/s]\n","2021-11-10 13:30:27,940 INFO streaming.PipeMapRed: R/W/S=8900000/8889522/0 in:296666=8900000/30 [rec/s] out:296317=8889522/30 [rec/s]\n","2021-11-10 13:30:28,245 INFO streaming.PipeMapRed: R/W/S=9000000/8991594/0 in:300000=9000000/30 [rec/s] out:299719=8991594/30 [rec/s]\n","2021-11-10 13:30:28,321 INFO streaming.PipeMapRed: Records R/W=9029061/9016716\n","2021-11-10 13:30:28,511 INFO streaming.PipeMapRed: R/W/S=9100000/9081803/0 in:303333=9100000/30 [rec/s] out:302726=9081803/30 [rec/s]\n","2021-11-10 13:30:28,820 INFO streaming.PipeMapRed: R/W/S=9200000/9183016/0 in:296774=9200000/31 [rec/s] out:296226=9183020/31 [rec/s]\n","2021-11-10 13:30:29,121 INFO streaming.PipeMapRed: R/W/S=9300000/9283739/0 in:300000=9300000/31 [rec/s] out:299475=9283743/31 [rec/s]\n","2021-11-10 13:30:29,423 INFO streaming.PipeMapRed: R/W/S=9400000/9384866/0 in:303225=9400000/31 [rec/s] out:302737=9384866/31 [rec/s]\n","2021-11-10 13:30:29,716 INFO streaming.PipeMapRed: R/W/S=9500000/9486881/0 in:306451=9500000/31 [rec/s] out:306028=9486881/31 [rec/s]\n","2021-11-10 13:30:30,020 INFO streaming.PipeMapRed: R/W/S=9600000/9588710/0 in:300000=9600000/32 [rec/s] out:299647=9588710/32 [rec/s]\n","2021-11-10 13:30:30,317 INFO streaming.PipeMapRed: R/W/S=9700000/9690622/0 in:303125=9700000/32 [rec/s] out:302831=9690622/32 [rec/s]\n","2021-11-10 13:30:30,597 INFO streaming.PipeMapRed: R/W/S=9800000/9780882/0 in:306250=9800000/32 [rec/s] out:305652=9780893/32 [rec/s]\n","2021-11-10 13:30:30,960 INFO streaming.PipeMapRed: R/W/S=9900000/9882075/0 in:300000=9900000/33 [rec/s] out:299456=9882075/33 [rec/s]\n","2021-11-10 13:30:31,273 INFO streaming.PipeMapRed: R/W/S=10000000/9988380/0 in:303030=10000000/33 [rec/s] out:302678=9988380/33 [rec/s]\n","2021-11-10 13:30:31,569 INFO streaming.PipeMapRed: R/W/S=10100000/10088530/0 in:306060=10100000/33 [rec/s] out:305713=10088530/33 [rec/s]\n","2021-11-10 13:30:31,873 INFO streaming.PipeMapRed: R/W/S=10200000/10185984/0 in:300000=10200000/34 [rec/s] out:299588=10186010/34 [rec/s]\n","2021-11-10 13:30:32,182 INFO streaming.PipeMapRed: R/W/S=10300000/10287569/0 in:302941=10300000/34 [rec/s] out:302575=10287569/34 [rec/s]\n","2021-11-10 13:30:32,526 INFO streaming.PipeMapRed: R/W/S=10400000/10389728/0 in:305882=10400000/34 [rec/s] out:305580=10389728/34 [rec/s]\n","2021-11-10 13:30:32,536 INFO mapred.LocalJobRunner: Records R/W=9029061/9016716 > reduce\n","2021-11-10 13:30:32,830 INFO streaming.PipeMapRed: R/W/S=10500000/10491557/0 in:300000=10500000/35 [rec/s] out:299758=10491557/35 [rec/s]\n","2021-11-10 13:30:33,119 INFO streaming.PipeMapRed: R/W/S=10600000/10580331/0 in:302857=10600000/35 [rec/s] out:302295=10580331/35 [rec/s]\n","2021-11-10 13:30:33,420 INFO streaming.PipeMapRed: R/W/S=10700000/10680668/0 in:305714=10700000/35 [rec/s] out:305161=10680668/35 [rec/s]\n","2021-11-10 13:30:33,737 INFO streaming.PipeMapRed: R/W/S=10800000/10784921/0 in:308571=10800000/35 [rec/s] out:308140=10784921/35 [rec/s]\n","2021-11-10 13:30:34,048 INFO streaming.PipeMapRed: R/W/S=10900000/10885258/0 in:302777=10900000/36 [rec/s] out:302368=10885258/36 [rec/s]\n","2021-11-10 13:30:34,351 INFO streaming.PipeMapRed: R/W/S=11000000/10987503/0 in:305555=11000000/36 [rec/s] out:305208=10987503/36 [rec/s]\n","2021-11-10 13:30:34,651 INFO streaming.PipeMapRed: R/W/S=11100000/11088410/0 in:308333=11100000/36 [rec/s] out:308011=11088410/36 [rec/s]\n","2021-11-10 13:30:34,967 INFO streaming.PipeMapRed: R/W/S=11200000/11190737/0 in:302702=11200000/37 [rec/s] out:302452=11190737/37 [rec/s]\n","2021-11-10 13:30:35,258 INFO streaming.PipeMapRed: R/W/S=11300000/11282966/0 in:305405=11300000/37 [rec/s] out:304945=11282972/37 [rec/s]\n","2021-11-10 13:30:35,563 INFO streaming.PipeMapRed: R/W/S=11400000/11381779/0 in:308108=11400000/37 [rec/s] out:307616=11381794/37 [rec/s]\n","2021-11-10 13:30:35,867 INFO streaming.PipeMapRed: R/W/S=11500000/11483374/0 in:302631=11500000/38 [rec/s] out:302194=11483378/38 [rec/s]\n","2021-11-10 13:30:36,180 INFO streaming.PipeMapRed: R/W/S=11600000/11585377/0 in:305263=11600000/38 [rec/s] out:304878=11585377/38 [rec/s]\n","2021-11-10 13:30:36,471 INFO streaming.PipeMapRed: R/W/S=11700000/11685903/0 in:307894=11700000/38 [rec/s] out:307523=11685903/38 [rec/s]\n","2021-11-10 13:30:36,768 INFO streaming.PipeMapRed: R/W/S=11800000/11787696/0 in:310526=11800000/38 [rec/s] out:310202=11787700/38 [rec/s]\n","2021-11-10 13:30:37,079 INFO streaming.PipeMapRed: R/W/S=11900000/11889633/0 in:305128=11900000/39 [rec/s] out:304862=11889633/39 [rec/s]\n","2021-11-10 13:30:37,381 INFO streaming.PipeMapRed: R/W/S=12000000/11991169/0 in:307692=12000000/39 [rec/s] out:307465=11991169/39 [rec/s]\n","2021-11-10 13:30:37,661 INFO streaming.PipeMapRed: R/W/S=12100000/12078112/0 in:310256=12100000/39 [rec/s] out:309695=12078112/39 [rec/s]\n","2021-11-10 13:30:37,990 INFO streaming.PipeMapRed: R/W/S=12200000/12186282/0 in:305000=12200000/40 [rec/s] out:304657=12186282/40 [rec/s]\n","2021-11-10 13:30:38,310 INFO streaming.PipeMapRed: R/W/S=12300000/12283601/0 in:307500=12300000/40 [rec/s] out:307090=12283605/40 [rec/s]\n","2021-11-10 13:30:38,322 INFO streaming.PipeMapRed: Records R/W=12305736/12288299\n","2021-11-10 13:30:38,537 INFO mapred.LocalJobRunner: Records R/W=12305736/12288299 > reduce\n","2021-11-10 13:30:38,608 INFO streaming.PipeMapRed: R/W/S=12400000/12384905/0 in:310000=12400000/40 [rec/s] out:309622=12384905/40 [rec/s]\n","2021-11-10 13:30:38,915 INFO streaming.PipeMapRed: R/W/S=12500000/12486734/0 in:304878=12500000/41 [rec/s] out:304554=12486734/41 [rec/s]\n","2021-11-10 13:30:39,220 INFO streaming.PipeMapRed: R/W/S=12600000/12588674/0 in:307317=12600000/41 [rec/s] out:307040=12588674/41 [rec/s]\n","2021-11-10 13:30:39,546 INFO streaming.PipeMapRed: R/W/S=12700000/12690661/0 in:309756=12700000/41 [rec/s] out:309528=12690665/41 [rec/s]\n","2021-11-10 13:30:39,871 INFO streaming.PipeMapRed: R/W/S=12800000/12778047/0 in:304761=12800000/42 [rec/s] out:304239=12778047/42 [rec/s]\n","2021-11-10 13:30:40,238 INFO streaming.PipeMapRed: R/W/S=12900000/12882487/0 in:307142=12900000/42 [rec/s] out:306725=12882487/42 [rec/s]\n","2021-11-10 13:30:40,539 INFO streaming.PipeMapRed: R/W/S=13000000/12982411/0 in:309523=13000000/42 [rec/s] out:309105=12982417/42 [rec/s]\n","2021-11-10 13:30:40,842 INFO streaming.PipeMapRed: R/W/S=13100000/13085585/0 in:304651=13100000/43 [rec/s] out:304315=13085585/43 [rec/s]\n","2021-11-10 13:30:41,144 INFO streaming.PipeMapRed: R/W/S=13200000/13185676/0 in:306976=13200000/43 [rec/s] out:306643=13185676/43 [rec/s]\n","2021-11-10 13:30:41,451 INFO streaming.PipeMapRed: R/W/S=13300000/13287751/0 in:309302=13300000/43 [rec/s] out:309017=13287751/43 [rec/s]\n","2021-11-10 13:30:41,749 INFO streaming.PipeMapRed: R/W/S=13400000/13389638/0 in:311627=13400000/43 [rec/s] out:311386=13389638/43 [rec/s]\n","2021-11-10 13:30:42,048 INFO streaming.PipeMapRed: R/W/S=13500000/13491596/0 in:306818=13500000/44 [rec/s] out:306627=13491596/44 [rec/s]\n","2021-11-10 13:30:42,320 INFO streaming.PipeMapRed: R/W/S=13600000/13580370/0 in:309090=13600000/44 [rec/s] out:308644=13580370/44 [rec/s]\n","2021-11-10 13:30:42,616 INFO streaming.PipeMapRed: R/W/S=13700000/13680893/0 in:311363=13700000/44 [rec/s] out:310929=13680893/44 [rec/s]\n","2021-11-10 13:30:42,920 INFO streaming.PipeMapRed: R/W/S=13800000/13782858/0 in:306666=13800000/45 [rec/s] out:306285=13782858/45 [rec/s]\n","2021-11-10 13:30:43,236 INFO streaming.PipeMapRed: R/W/S=13900000/13888894/0 in:308888=13900000/45 [rec/s] out:308642=13888924/45 [rec/s]\n","2021-11-10 13:30:43,523 INFO streaming.PipeMapRed: R/W/S=14000000/13987499/0 in:311111=14000000/45 [rec/s] out:310833=13987499/45 [rec/s]\n","2021-11-10 13:30:43,831 INFO streaming.PipeMapRed: R/W/S=14100000/14088769/0 in:306521=14100000/46 [rec/s] out:306277=14088769/46 [rec/s]\n","2021-11-10 13:30:44,149 INFO streaming.PipeMapRed: R/W/S=14200000/14190598/0 in:308695=14200000/46 [rec/s] out:308491=14190598/46 [rec/s]\n","2021-11-10 13:30:44,432 INFO streaming.PipeMapRed: R/W/S=14300000/14280677/0 in:310869=14300000/46 [rec/s] out:310449=14280677/46 [rec/s]\n","2021-11-10 13:30:44,538 INFO mapred.LocalJobRunner: Records R/W=12305736/12288299 > reduce\n","2021-11-10 13:30:44,765 INFO streaming.PipeMapRed: R/W/S=14400000/14382320/0 in:313043=14400000/46 [rec/s] out:312659=14382320/46 [rec/s]\n","2021-11-10 13:30:45,079 INFO streaming.PipeMapRed: R/W/S=14500000/14481911/0 in:308510=14500000/47 [rec/s] out:308125=14481911/47 [rec/s]\n","2021-11-10 13:30:45,454 INFO streaming.PipeMapRed: R/W/S=14600000/14583926/0 in:310638=14600000/47 [rec/s] out:310296=14583926/47 [rec/s]\n","2021-11-10 13:30:45,762 INFO streaming.PipeMapRed: R/W/S=14700000/14686037/0 in:312765=14700000/47 [rec/s] out:312469=14686062/47 [rec/s]\n","2021-11-10 13:30:46,062 INFO streaming.PipeMapRed: R/W/S=14800000/14787690/0 in:308333=14800000/48 [rec/s] out:308076=14787690/48 [rec/s]\n","2021-11-10 13:30:46,378 INFO streaming.PipeMapRed: R/W/S=14900000/14890056/0 in:310416=14900000/48 [rec/s] out:310210=14890109/48 [rec/s]\n","2021-11-10 13:30:46,677 INFO streaming.PipeMapRed: R/W/S=15000000/14991615/0 in:312500=15000000/48 [rec/s] out:312325=14991615/48 [rec/s]\n","2021-11-10 13:30:46,942 INFO streaming.PipeMapRed: R/W/S=15100000/15079457/0 in:308163=15100000/49 [rec/s] out:307744=15079457/49 [rec/s]\n","2021-11-10 13:30:47,265 INFO streaming.PipeMapRed: R/W/S=15200000/15186321/0 in:310204=15200000/49 [rec/s] out:309924=15186321/49 [rec/s]\n","2021-11-10 13:30:47,580 INFO streaming.PipeMapRed: R/W/S=15300000/15282928/0 in:312244=15300000/49 [rec/s] out:311896=15282928/49 [rec/s]\n","2021-11-10 13:30:47,882 INFO streaming.PipeMapRed: R/W/S=15400000/15384757/0 in:308000=15400000/50 [rec/s] out:307695=15384757/50 [rec/s]\n","2021-11-10 13:30:48,268 INFO streaming.PipeMapRed: R/W/S=15500000/15489582/0 in:310000=15500000/50 [rec/s] out:309791=15489582/50 [rec/s]\n","2021-11-10 13:30:48,323 INFO streaming.PipeMapRed: Records R/W=15524159/15507885\n","2021-11-10 13:30:48,582 INFO streaming.PipeMapRed: R/W/S=15600000/15588602/0 in:312000=15600000/50 [rec/s] out:311772=15588602/50 [rec/s]\n","2021-11-10 13:30:48,917 INFO streaming.PipeMapRed: R/W/S=15700000/15690516/0 in:307843=15700000/51 [rec/s] out:307657=15690516/51 [rec/s]\n","2021-11-10 13:30:49,193 INFO streaming.PipeMapRed: R/W/S=15800000/15785724/0 in:309803=15800000/51 [rec/s] out:309524=15785729/51 [rec/s]\n","2021-11-10 13:30:49,569 INFO streaming.PipeMapRed: R/W/S=15900000/15881764/0 in:311764=15900000/51 [rec/s] out:311407=15881764/51 [rec/s]\n","2021-11-10 13:30:49,889 INFO streaming.PipeMapRed: R/W/S=16000000/15986593/0 in:307692=16000000/52 [rec/s] out:307434=15986593/52 [rec/s]\n","2021-11-10 13:30:50,201 INFO streaming.PipeMapRed: R/W/S=16100000/16087303/0 in:309615=16100000/52 [rec/s] out:309371=16087303/52 [rec/s]\n","2021-11-10 13:30:50,532 INFO streaming.PipeMapRed: R/W/S=16200000/16186905/0 in:311538=16200000/52 [rec/s] out:311286=16186911/52 [rec/s]\n","2021-11-10 13:30:50,539 INFO mapred.LocalJobRunner: Records R/W=15524159/15507885 > reduce\n","2021-11-10 13:30:50,844 INFO streaming.PipeMapRed: R/W/S=16300000/16287790/0 in:307547=16300000/53 [rec/s] out:307316=16287790/53 [rec/s]\n","2021-11-10 13:30:51,150 INFO streaming.PipeMapRed: R/W/S=16400000/16389619/0 in:309433=16400000/53 [rec/s] out:309238=16389619/53 [rec/s]\n","2021-11-10 13:30:51,465 INFO streaming.PipeMapRed: R/W/S=16500000/16491601/0 in:311320=16500000/53 [rec/s] out:311162=16491601/53 [rec/s]\n","2021-11-10 13:30:51,731 INFO streaming.PipeMapRed: R/W/S=16600000/16578917/0 in:313207=16600000/53 [rec/s] out:312809=16578917/53 [rec/s]\n","2021-11-10 13:30:52,034 INFO streaming.PipeMapRed: R/W/S=16700000/16682192/0 in:309259=16700000/54 [rec/s] out:308929=16682197/54 [rec/s]\n","2021-11-10 13:30:52,338 INFO streaming.PipeMapRed: R/W/S=16800000/16782859/0 in:311111=16800000/54 [rec/s] out:310793=16782861/54 [rec/s]\n","2021-11-10 13:30:52,656 INFO streaming.PipeMapRed: R/W/S=16900000/16887761/0 in:312962=16900000/54 [rec/s] out:312736=16887761/54 [rec/s]\n","2021-11-10 13:30:52,962 INFO streaming.PipeMapRed: R/W/S=17000000/16986682/0 in:309090=17000000/55 [rec/s] out:308848=16986682/55 [rec/s]\n","2021-11-10 13:30:53,267 INFO streaming.PipeMapRed: R/W/S=17100000/17088621/0 in:310909=17100000/55 [rec/s] out:310702=17088621/55 [rec/s]\n","2021-11-10 13:30:53,572 INFO streaming.PipeMapRed: R/W/S=17200000/17190264/0 in:312727=17200000/55 [rec/s] out:312550=17190264/55 [rec/s]\n","2021-11-10 13:30:53,850 INFO streaming.PipeMapRed: R/W/S=17300000/17280343/0 in:308928=17300000/56 [rec/s] out:308577=17280343/56 [rec/s]\n","2021-11-10 13:30:54,153 INFO streaming.PipeMapRed: R/W/S=17400000/17379934/0 in:310714=17400000/56 [rec/s] out:310355=17379934/56 [rec/s]\n","2021-11-10 13:30:54,487 INFO streaming.PipeMapRed: R/W/S=17500000/17486380/0 in:312500=17500000/56 [rec/s] out:312256=17486386/56 [rec/s]\n","2021-11-10 13:30:54,770 INFO streaming.PipeMapRed: R/W/S=17600000/17584698/0 in:314285=17600000/56 [rec/s] out:314012=17584704/56 [rec/s]\n","2021-11-10 13:30:55,078 INFO streaming.PipeMapRed: R/W/S=17700000/17685794/0 in:310526=17700000/57 [rec/s] out:310277=17685794/57 [rec/s]\n","2021-11-10 13:30:55,383 INFO streaming.PipeMapRed: R/W/S=17800000/17787650/0 in:312280=17800000/57 [rec/s] out:312064=17787650/57 [rec/s]\n","2021-11-10 13:30:55,722 INFO streaming.PipeMapRed: R/W/S=17900000/17889639/0 in:314035=17900000/57 [rec/s] out:313853=17889639/57 [rec/s]\n","2021-11-10 13:30:56,041 INFO streaming.PipeMapRed: R/W/S=18000000/17991600/0 in:310344=18000000/58 [rec/s] out:310200=17991600/58 [rec/s]\n","2021-11-10 13:30:56,319 INFO streaming.PipeMapRed: R/W/S=18100000/18084158/0 in:312068=18100000/58 [rec/s] out:311795=18084158/58 [rec/s]\n","2021-11-10 13:30:56,541 INFO mapred.LocalJobRunner: Records R/W=15524159/15507885 > reduce\n","2021-11-10 13:30:56,616 INFO streaming.PipeMapRed: R/W/S=18200000/18183003/0 in:313793=18200000/58 [rec/s] out:313500=18183003/58 [rec/s]\n","2021-11-10 13:30:56,922 INFO streaming.PipeMapRed: R/W/S=18300000/18285709/0 in:310169=18300000/59 [rec/s] out:309927=18285709/59 [rec/s]\n","2021-11-10 13:30:57,237 INFO streaming.PipeMapRed: R/W/S=18400000/18385713/0 in:311864=18400000/59 [rec/s] out:311622=18385718/59 [rec/s]\n","2021-11-10 13:30:57,533 INFO streaming.PipeMapRed: R/W/S=18500000/18486812/0 in:313559=18500000/59 [rec/s] out:313335=18486812/59 [rec/s]\n","2021-11-10 13:30:57,842 INFO streaming.PipeMapRed: R/W/S=18600000/18590879/0 in:310000=18600000/60 [rec/s] out:309847=18590879/60 [rec/s]\n","2021-11-10 13:30:58,143 INFO streaming.PipeMapRed: R/W/S=18700000/18690601/0 in:311666=18700000/60 [rec/s] out:311510=18690601/60 [rec/s]\n","2021-11-10 13:30:58,325 INFO streaming.PipeMapRed: Records R/W=18771708/18751270\n","2021-11-10 13:30:58,408 INFO streaming.PipeMapRed: R/W/S=18800000/18779430/0 in:313333=18800000/60 [rec/s] out:312990=18779430/60 [rec/s]\n","2021-11-10 13:30:58,728 INFO streaming.PipeMapRed: R/W/S=18900000/18882192/0 in:315000=18900000/60 [rec/s] out:314703=18882192/60 [rec/s]\n","2021-11-10 13:30:59,040 INFO streaming.PipeMapRed: R/W/S=19000000/18984021/0 in:311475=19000000/61 [rec/s] out:311213=18984021/61 [rec/s]\n","2021-11-10 13:30:59,348 INFO streaming.PipeMapRed: R/W/S=19100000/19083798/0 in:313114=19100000/61 [rec/s] out:312849=19083798/61 [rec/s]\n","2021-11-10 13:30:59,749 INFO streaming.PipeMapRed: R/W/S=19200000/19185814/0 in:314754=19200000/61 [rec/s] out:314521=19185814/61 [rec/s]\n","2021-11-10 13:31:00,057 INFO streaming.PipeMapRed: R/W/S=19300000/19287643/0 in:311290=19300000/62 [rec/s] out:311091=19287643/62 [rec/s]\n","2021-11-10 13:31:00,366 INFO streaming.PipeMapRed: R/W/S=19400000/19388912/0 in:312903=19400000/62 [rec/s] out:312724=19388912/62 [rec/s]\n","2021-11-10 13:31:00,708 INFO streaming.PipeMapRed: R/W/S=19500000/19491487/0 in:314516=19500000/62 [rec/s] out:314378=19491487/62 [rec/s]\n","2021-11-10 13:31:00,989 INFO streaming.PipeMapRed: R/W/S=19600000/19585670/0 in:311111=19600000/63 [rec/s] out:310883=19585670/63 [rec/s]\n","2021-11-10 13:31:01,279 INFO streaming.PipeMapRed: R/W/S=19700000/19682991/0 in:312698=19700000/63 [rec/s] out:312428=19682996/63 [rec/s]\n","2021-11-10 13:31:01,588 INFO streaming.PipeMapRed: R/W/S=19800000/19787090/0 in:314285=19800000/63 [rec/s] out:314080=19787090/63 [rec/s]\n","2021-11-10 13:31:01,913 INFO streaming.PipeMapRed: R/W/S=19900000/19884816/0 in:310937=19900000/64 [rec/s] out:310700=19884816/64 [rec/s]\n","2021-11-10 13:31:02,220 INFO streaming.PipeMapRed: R/W/S=20000000/19986645/0 in:312500=20000000/64 [rec/s] out:312291=19986645/64 [rec/s]\n","2021-11-10 13:31:02,524 INFO streaming.PipeMapRed: R/W/S=20100000/20088660/0 in:314062=20100000/64 [rec/s] out:313885=20088660/64 [rec/s]\n","2021-11-10 13:31:02,542 INFO mapred.LocalJobRunner: Records R/W=18771708/18751270 > reduce\n","2021-11-10 13:31:02,836 INFO streaming.PipeMapRed: R/W/S=20200000/20190537/0 in:310769=20200000/65 [rec/s] out:310623=20190537/65 [rec/s]\n","2021-11-10 13:31:03,110 INFO streaming.PipeMapRed: R/W/S=20300000/20280833/0 in:312307=20300000/65 [rec/s] out:312013=20280855/65 [rec/s]\n","2021-11-10 13:31:03,399 INFO streaming.PipeMapRed: R/W/S=20400000/20380839/0 in:313846=20400000/65 [rec/s] out:313551=20380844/65 [rec/s]\n","2021-11-10 13:31:03,712 INFO streaming.PipeMapRed: R/W/S=20500000/20482735/0 in:315384=20500000/65 [rec/s] out:315119=20482735/65 [rec/s]\n","2021-11-10 13:31:04,026 INFO streaming.PipeMapRed: R/W/S=20600000/20583818/0 in:312121=20600000/66 [rec/s] out:311876=20583818/66 [rec/s]\n","2021-11-10 13:31:04,328 INFO streaming.PipeMapRed: R/W/S=20700000/20689190/0 in:313636=20700000/66 [rec/s] out:313472=20689190/66 [rec/s]\n","2021-11-10 13:31:04,617 INFO streaming.PipeMapRed: R/W/S=20800000/20788376/0 in:315151=20800000/66 [rec/s] out:314975=20788376/66 [rec/s]\n","2021-11-10 13:31:04,927 INFO streaming.PipeMapRed: R/W/S=20900000/20889566/0 in:311940=20900000/67 [rec/s] out:311784=20889566/67 [rec/s]\n","2021-11-10 13:31:05,242 INFO streaming.PipeMapRed: R/W/S=21000000/20991429/0 in:313432=21000000/67 [rec/s] out:313304=20991429/67 [rec/s]\n","2021-11-10 13:31:05,507 INFO streaming.PipeMapRed: R/W/S=21100000/21081773/0 in:314925=21100000/67 [rec/s] out:314653=21081773/67 [rec/s]\n","2021-11-10 13:31:05,836 INFO streaming.PipeMapRed: R/W/S=21200000/21180804/0 in:311764=21200000/68 [rec/s] out:311482=21180804/68 [rec/s]\n","2021-11-10 13:31:06,157 INFO streaming.PipeMapRed: R/W/S=21300000/21282820/0 in:313235=21300000/68 [rec/s] out:312982=21282820/68 [rec/s]\n","2021-11-10 13:31:06,469 INFO streaming.PipeMapRed: R/W/S=21400000/21386141/0 in:314705=21400000/68 [rec/s] out:314502=21386141/68 [rec/s]\n","2021-11-10 13:31:06,774 INFO streaming.PipeMapRed: R/W/S=21500000/21487143/0 in:316176=21500000/68 [rec/s] out:315987=21487182/68 [rec/s]\n","2021-11-10 13:31:07,080 INFO streaming.PipeMapRed: R/W/S=21600000/21588551/0 in:313043=21600000/69 [rec/s] out:312877=21588551/69 [rec/s]\n","2021-11-10 13:31:07,379 INFO streaming.PipeMapRed: R/W/S=21700000/21690509/0 in:314492=21700000/69 [rec/s] out:314355=21690509/69 [rec/s]\n","2021-11-10 13:31:07,669 INFO streaming.PipeMapRed: R/W/S=21800000/21782290/0 in:315942=21800000/69 [rec/s] out:315685=21782295/69 [rec/s]\n","2021-11-10 13:31:07,966 INFO streaming.PipeMapRed: R/W/S=21900000/21882417/0 in:312857=21900000/70 [rec/s] out:312605=21882417/70 [rec/s]\n","2021-11-10 13:31:08,270 INFO streaming.PipeMapRed: R/W/S=22000000/21983901/0 in:314285=22000000/70 [rec/s] out:314055=21983907/70 [rec/s]\n","2021-11-10 13:31:08,326 INFO streaming.PipeMapRed: Records R/W=22019257/22003629\n","2021-11-10 13:31:08,543 INFO mapred.LocalJobRunner: Records R/W=22019257/22003629 > reduce\n","2021-11-10 13:31:08,571 INFO streaming.PipeMapRed: R/W/S=22100000/22083837/0 in:315714=22100000/70 [rec/s] out:315483=22083837/70 [rec/s]\n","2021-11-10 13:31:08,885 INFO streaming.PipeMapRed: R/W/S=22200000/22186039/0 in:312676=22200000/71 [rec/s] out:312479=22186039/71 [rec/s]\n","2021-11-10 13:31:09,197 INFO streaming.PipeMapRed: R/W/S=22300000/22287505/0 in:314084=22300000/71 [rec/s] out:313908=22287505/71 [rec/s]\n","2021-11-10 13:31:09,539 INFO streaming.PipeMapRed: R/W/S=22400000/22392495/0 in:315492=22400000/71 [rec/s] out:315387=22392495/71 [rec/s]\n","2021-11-10 13:31:09,890 INFO streaming.PipeMapRed: R/W/S=22500000/22491463/0 in:312500=22500000/72 [rec/s] out:312381=22491463/72 [rec/s]\n","2021-11-10 13:31:10,177 INFO streaming.PipeMapRed: R/W/S=22600000/22580281/0 in:313888=22600000/72 [rec/s] out:313615=22580283/72 [rec/s]\n","2021-11-10 13:31:10,508 INFO streaming.PipeMapRed: R/W/S=22700000/22682129/0 in:315277=22700000/72 [rec/s] out:315029=22682129/72 [rec/s]\n","2021-11-10 13:31:10,824 INFO streaming.PipeMapRed: R/W/S=22800000/22783839/0 in:312328=22800000/73 [rec/s] out:312107=22783839/73 [rec/s]\n","2021-11-10 13:31:11,129 INFO streaming.PipeMapRed: R/W/S=22900000/22884668/0 in:313698=22900000/73 [rec/s] out:313488=22884668/73 [rec/s]\n","2021-11-10 13:31:11,436 INFO streaming.PipeMapRed: R/W/S=23000000/22987803/0 in:315068=23000000/73 [rec/s] out:314901=22987803/73 [rec/s]\n","2021-11-10 13:31:11,740 INFO streaming.PipeMapRed: R/W/S=23100000/23089072/0 in:316438=23100000/73 [rec/s] out:316288=23089072/73 [rec/s]\n","2021-11-10 13:31:12,048 INFO streaming.PipeMapRed: R/W/S=23200000/23190528/0 in:313513=23200000/74 [rec/s] out:313385=23190528/74 [rec/s]\n","2021-11-10 13:31:12,320 INFO streaming.PipeMapRed: R/W/S=23300000/23280639/0 in:314864=23300000/74 [rec/s] out:314603=23280687/74 [rec/s]\n","2021-11-10 13:31:12,620 INFO streaming.PipeMapRed: R/W/S=23400000/23382923/0 in:316216=23400000/74 [rec/s] out:315985=23382928/74 [rec/s]\n","2021-11-10 13:31:12,932 INFO streaming.PipeMapRed: R/W/S=23500000/23481468/0 in:313333=23500000/75 [rec/s] out:313086=23481468/75 [rec/s]\n","2021-11-10 13:31:13,246 INFO streaming.PipeMapRed: R/W/S=23600000/23584564/0 in:314666=23600000/75 [rec/s] out:314460=23584569/75 [rec/s]\n","2021-11-10 13:31:13,554 INFO streaming.PipeMapRed: R/W/S=23700000/23685686/0 in:316000=23700000/75 [rec/s] out:315809=23685686/75 [rec/s]\n","2021-11-10 13:31:13,859 INFO streaming.PipeMapRed: R/W/S=23800000/23787625/0 in:313157=23800000/76 [rec/s] out:312995=23787625/76 [rec/s]\n","2021-11-10 13:31:14,158 INFO streaming.PipeMapRed: R/W/S=23900000/23889530/0 in:314473=23900000/76 [rec/s] out:314335=23889530/76 [rec/s]\n","2021-11-10 13:31:14,461 INFO streaming.PipeMapRed: R/W/S=24000000/23991546/0 in:315789=24000000/76 [rec/s] out:315678=23991546/76 [rec/s]\n","2021-11-10 13:31:14,549 INFO mapred.LocalJobRunner: Records R/W=22019257/22003629 > reduce\n","2021-11-10 13:31:14,728 INFO streaming.PipeMapRed: R/W/S=24100000/24084340/0 in:317105=24100000/76 [rec/s] out:316899=24084340/76 [rec/s]\n","2021-11-10 13:31:15,031 INFO streaming.PipeMapRed: R/W/S=24200000/24183935/0 in:314285=24200000/77 [rec/s] out:314077=24183938/77 [rec/s]\n","2021-11-10 13:31:15,328 INFO streaming.PipeMapRed: R/W/S=24300000/24287335/0 in:315584=24300000/77 [rec/s] out:315419=24287335/77 [rec/s]\n","2021-11-10 13:31:15,611 INFO streaming.PipeMapRed: R/W/S=24400000/24385299/0 in:316883=24400000/77 [rec/s] out:316692=24385299/77 [rec/s]\n","2021-11-10 13:31:15,946 INFO streaming.PipeMapRed: R/W/S=24500000/24486703/0 in:314102=24500000/78 [rec/s] out:313932=24486703/78 [rec/s]\n","2021-11-10 13:31:16,259 INFO streaming.PipeMapRed: R/W/S=24600000/24588532/0 in:315384=24600000/78 [rec/s] out:315237=24588532/78 [rec/s]\n","2021-11-10 13:31:16,582 INFO streaming.PipeMapRed: R/W/S=24700000/24690548/0 in:316666=24700000/78 [rec/s] out:316545=24690548/78 [rec/s]\n","2021-11-10 13:31:16,855 INFO streaming.PipeMapRed: R/W/S=24800000/24780021/0 in:313924=24800000/79 [rec/s] out:313671=24780026/79 [rec/s]\n","2021-11-10 13:31:17,154 INFO streaming.PipeMapRed: R/W/S=24900000/24879845/0 in:315189=24900000/79 [rec/s] out:314934=24879845/79 [rec/s]\n","2021-11-10 13:31:17,452 INFO streaming.PipeMapRed: R/W/S=25000000/24983726/0 in:316455=25000000/79 [rec/s] out:316249=24983726/79 [rec/s]\n","2021-11-10 13:31:17,748 INFO streaming.PipeMapRed: R/W/S=25100000/25083543/0 in:317721=25100000/79 [rec/s] out:317513=25083543/79 [rec/s]\n","2021-11-10 13:31:18,082 INFO streaming.PipeMapRed: R/W/S=25200000/25184773/0 in:315000=25200000/80 [rec/s] out:314809=25184773/80 [rec/s]\n","2021-11-10 13:31:18,327 INFO streaming.PipeMapRed: Records R/W=25281369/25265342\n","2021-11-10 13:31:18,391 INFO streaming.PipeMapRed: R/W/S=25300000/25287832/0 in:316250=25300000/80 [rec/s] out:316098=25287865/80 [rec/s]\n","2021-11-10 13:31:18,684 INFO streaming.PipeMapRed: R/W/S=25400000/25389550/0 in:317500=25400000/80 [rec/s] out:317369=25389550/80 [rec/s]\n","2021-11-10 13:31:18,983 INFO streaming.PipeMapRed: R/W/S=25500000/25491493/0 in:314814=25500000/81 [rec/s] out:314709=25491493/81 [rec/s]\n","2021-11-10 13:31:19,260 INFO streaming.PipeMapRed: R/W/S=25600000/25581334/0 in:316049=25600000/81 [rec/s] out:315818=25581338/81 [rec/s]\n","2021-11-10 13:31:19,621 INFO streaming.PipeMapRed: R/W/S=25700000/25683020/0 in:317283=25700000/81 [rec/s] out:317074=25683025/81 [rec/s]\n","2021-11-10 13:31:19,945 INFO streaming.PipeMapRed: R/W/S=25800000/25784378/0 in:314634=25800000/82 [rec/s] out:314443=25784378/82 [rec/s]\n","2021-11-10 13:31:20,277 INFO streaming.PipeMapRed: R/W/S=25900000/25886013/0 in:315853=25900000/82 [rec/s] out:315683=25886013/82 [rec/s]\n","2021-11-10 13:31:20,550 INFO mapred.LocalJobRunner: Records R/W=25281369/25265342 > reduce\n","2021-11-10 13:31:20,590 INFO streaming.PipeMapRed: R/W/S=26000000/25986163/0 in:317073=26000000/82 [rec/s] out:316904=25986163/82 [rec/s]\n","2021-11-10 13:31:20,889 INFO streaming.PipeMapRed: R/W/S=26100000/26089298/0 in:314457=26100000/83 [rec/s] out:314328=26089298/83 [rec/s]\n","2021-11-10 13:31:21,189 INFO streaming.PipeMapRed: R/W/S=26200000/26190510/0 in:315662=26200000/83 [rec/s] out:315548=26190510/83 [rec/s]\n","2021-11-10 13:31:21,461 INFO streaming.PipeMapRed: R/W/S=26300000/26281393/0 in:316867=26300000/83 [rec/s] out:316643=26281393/83 [rec/s]\n","2021-11-10 13:31:21,764 INFO streaming.PipeMapRed: R/W/S=26400000/26381916/0 in:318072=26400000/83 [rec/s] out:317854=26381916/83 [rec/s]\n","2021-11-10 13:31:22,063 INFO streaming.PipeMapRed: R/W/S=26500000/26483194/0 in:315476=26500000/84 [rec/s] out:315276=26483223/84 [rec/s]\n","2021-11-10 13:31:22,364 INFO streaming.PipeMapRed: R/W/S=26600000/26584197/0 in:316666=26600000/84 [rec/s] out:316478=26584202/84 [rec/s]\n","2021-11-10 13:31:22,662 INFO streaming.PipeMapRed: R/W/S=26700000/26685725/0 in:317857=26700000/84 [rec/s] out:317687=26685725/84 [rec/s]\n","2021-11-10 13:31:22,954 INFO streaming.PipeMapRed: R/W/S=26800000/26787740/0 in:315294=26800000/85 [rec/s] out:315149=26787740/85 [rec/s]\n","2021-11-10 13:31:23,275 INFO streaming.PipeMapRed: R/W/S=26900000/26889569/0 in:316470=26900000/85 [rec/s] out:316347=26889569/85 [rec/s]\n","2021-11-10 13:31:23,574 INFO streaming.PipeMapRed: R/W/S=27000000/26991585/0 in:317647=27000000/85 [rec/s] out:317548=26991585/85 [rec/s]\n","2021-11-10 13:31:23,831 INFO streaming.PipeMapRed: R/W/S=27100000/27078307/0 in:315116=27100000/86 [rec/s] out:314864=27078307/86 [rec/s]\n","2021-11-10 13:31:24,137 INFO streaming.PipeMapRed: R/W/S=27200000/27182561/0 in:316279=27200000/86 [rec/s] out:316076=27182561/86 [rec/s]\n","2021-11-10 13:31:24,441 INFO streaming.PipeMapRed: R/W/S=27300000/27287746/0 in:317441=27300000/86 [rec/s] out:317299=27287747/86 [rec/s]\n","2021-11-10 13:31:24,742 INFO streaming.PipeMapRed: R/W/S=27400000/27386071/0 in:318604=27400000/86 [rec/s] out:318442=27386075/86 [rec/s]\n","2021-11-10 13:31:25,053 INFO streaming.PipeMapRed: R/W/S=27500000/27486742/0 in:316091=27500000/87 [rec/s] out:315939=27486742/87 [rec/s]\n","2021-11-10 13:31:25,361 INFO streaming.PipeMapRed: R/W/S=27600000/27588571/0 in:317241=27600000/87 [rec/s] out:317110=27588571/87 [rec/s]\n","2021-11-10 13:31:25,698 INFO streaming.PipeMapRed: R/W/S=27700000/27690539/0 in:318390=27700000/87 [rec/s] out:318282=27690539/87 [rec/s]\n","2021-11-10 13:31:25,964 INFO streaming.PipeMapRed: R/W/S=27800000/27780107/0 in:315909=27800000/88 [rec/s] out:315683=27780107/88 [rec/s]\n","2021-11-10 13:31:26,283 INFO streaming.PipeMapRed: R/W/S=27900000/27883801/0 in:317045=27900000/88 [rec/s] out:316861=27883801/88 [rec/s]\n","2021-11-10 13:31:26,550 INFO mapred.LocalJobRunner: Records R/W=25281369/25265342 > reduce\n","2021-11-10 13:31:26,572 INFO streaming.PipeMapRed: R/W/S=28000000/27983951/0 in:318181=28000000/88 [rec/s] out:317999=27983951/88 [rec/s]\n","2021-11-10 13:31:26,872 INFO streaming.PipeMapRed: R/W/S=28100000/28083129/0 in:315730=28100000/89 [rec/s] out:315540=28083129/89 [rec/s]\n","2021-11-10 13:31:27,171 INFO streaming.PipeMapRed: R/W/S=28200000/28185744/0 in:316853=28200000/89 [rec/s] out:316693=28185744/89 [rec/s]\n","2021-11-10 13:31:27,487 INFO streaming.PipeMapRed: R/W/S=28300000/28287424/0 in:317977=28300000/89 [rec/s] out:317836=28287424/89 [rec/s]\n","2021-11-10 13:31:27,781 INFO streaming.PipeMapRed: R/W/S=28400000/28389546/0 in:319101=28400000/89 [rec/s] out:318983=28389546/89 [rec/s]\n","2021-11-10 13:31:28,088 INFO streaming.PipeMapRed: R/W/S=28500000/28491418/0 in:316666=28500000/90 [rec/s] out:316571=28491418/90 [rec/s]\n","2021-11-10 13:31:28,328 INFO streaming.PipeMapRed: Records R/W=28578269/28570122\n","2021-11-10 13:31:28,378 INFO streaming.PipeMapRed: R/W/S=28600000/28586533/0 in:317777=28600000/90 [rec/s] out:317628=28586533/90 [rec/s]\n","2021-11-10 13:31:28,658 INFO streaming.PipeMapRed: R/W/S=28700000/28680902/0 in:318888=28700000/90 [rec/s] out:318676=28680902/90 [rec/s]\n","2021-11-10 13:31:28,955 INFO streaming.PipeMapRed: R/W/S=28800000/28783308/0 in:316483=28800000/91 [rec/s] out:316300=28783308/91 [rec/s]\n","2021-11-10 13:31:29,254 INFO streaming.PipeMapRed: R/W/S=28900000/28884724/0 in:317582=28900000/91 [rec/s] out:317414=28884724/91 [rec/s]\n","2021-11-10 13:31:29,628 INFO streaming.PipeMapRed: R/W/S=29000000/28986887/0 in:318681=29000000/91 [rec/s] out:318537=28986929/91 [rec/s]\n","2021-11-10 13:31:29,917 INFO streaming.PipeMapRed: R/W/S=29100000/29088591/0 in:316304=29100000/92 [rec/s] out:316180=29088591/92 [rec/s]\n","2021-11-10 13:31:30,257 INFO streaming.PipeMapRed: R/W/S=29200000/29190451/0 in:317391=29200000/92 [rec/s] out:317287=29190451/92 [rec/s]\n","2021-11-10 13:31:30,565 INFO streaming.PipeMapRed: R/W/S=29300000/29281673/0 in:318478=29300000/92 [rec/s] out:318279=29281679/92 [rec/s]\n","2021-11-10 13:31:30,868 INFO streaming.PipeMapRed: R/W/S=29400000/29383634/0 in:316129=29400000/93 [rec/s] out:315953=29383634/93 [rec/s]\n","2021-11-10 13:31:31,165 INFO streaming.PipeMapRed: R/W/S=29500000/29483225/0 in:317204=29500000/93 [rec/s] out:317023=29483225/93 [rec/s]\n","2021-11-10 13:31:31,469 INFO streaming.PipeMapRed: R/W/S=29600000/29585240/0 in:318279=29600000/93 [rec/s] out:318120=29585240/93 [rec/s]\n","2021-11-10 13:31:31,778 INFO streaming.PipeMapRed: R/W/S=29700000/29685577/0 in:319354=29700000/93 [rec/s] out:319199=29685577/93 [rec/s]\n","2021-11-10 13:31:32,090 INFO streaming.PipeMapRed: R/W/S=29800000/29787593/0 in:317021=29800000/94 [rec/s] out:316889=29787593/94 [rec/s]\n","2021-11-10 13:31:32,404 INFO streaming.PipeMapRed: R/W/S=29900000/29889507/0 in:318085=29900000/94 [rec/s] out:317973=29889507/94 [rec/s]\n","2021-11-10 13:31:32,551 INFO mapred.LocalJobRunner: Records R/W=28578269/28570122 > reduce\n","2021-11-10 13:31:32,743 INFO streaming.PipeMapRed: R/W/S=30000000/29991437/0 in:319148=30000000/94 [rec/s] out:319057=29991437/94 [rec/s]\n","2021-11-10 13:31:33,020 INFO streaming.PipeMapRed: R/W/S=30100000/30081703/0 in:316842=30100000/95 [rec/s] out:316649=30081703/95 [rec/s]\n","2021-11-10 13:31:33,320 INFO streaming.PipeMapRed: R/W/S=30200000/30182612/0 in:317894=30200000/95 [rec/s] out:317711=30182635/95 [rec/s]\n","2021-11-10 13:31:33,624 INFO streaming.PipeMapRed: R/W/S=30300000/30282750/0 in:318947=30300000/95 [rec/s] out:318765=30282750/95 [rec/s]\n","2021-11-10 13:31:33,933 INFO streaming.PipeMapRed: R/W/S=30400000/30384766/0 in:316666=30400000/96 [rec/s] out:316507=30384766/96 [rec/s]\n","2021-11-10 13:31:34,231 INFO streaming.PipeMapRed: R/W/S=30500000/30486595/0 in:317708=30500000/96 [rec/s] out:317568=30486595/96 [rec/s]\n","2021-11-10 13:31:34,559 INFO streaming.PipeMapRed: R/W/S=30600000/30588556/0 in:318750=30600000/96 [rec/s] out:318630=30588556/96 [rec/s]\n","2021-11-10 13:31:34,850 INFO streaming.PipeMapRed: R/W/S=30700000/30690390/0 in:316494=30700000/97 [rec/s] out:316395=30690390/97 [rec/s]\n","2021-11-10 13:31:35,123 INFO streaming.PipeMapRed: R/W/S=30800000/30779274/0 in:317525=30800000/97 [rec/s] out:317312=30779279/97 [rec/s]\n","2021-11-10 13:31:35,447 INFO streaming.PipeMapRed: R/W/S=30900000/30882462/0 in:318556=30900000/97 [rec/s] out:318375=30882462/97 [rec/s]\n","2021-11-10 13:31:35,736 INFO streaming.PipeMapRed: R/W/S=31000000/30983296/0 in:319587=31000000/97 [rec/s] out:319415=30983300/97 [rec/s]\n","2021-11-10 13:31:36,033 INFO streaming.PipeMapRed: R/W/S=31100000/31083767/0 in:317346=31100000/98 [rec/s] out:317181=31083767/98 [rec/s]\n","2021-11-10 13:31:36,327 INFO streaming.PipeMapRed: R/W/S=31200000/31185896/0 in:318367=31200000/98 [rec/s] out:318223=31185896/98 [rec/s]\n","2021-11-10 13:31:36,652 INFO streaming.PipeMapRed: R/W/S=31300000/31288503/0 in:319387=31300000/98 [rec/s] out:319270=31288545/98 [rec/s]\n","2021-11-10 13:31:36,949 INFO streaming.PipeMapRed: R/W/S=31400000/31389472/0 in:317171=31400000/99 [rec/s] out:317065=31389472/99 [rec/s]\n","2021-11-10 13:31:37,250 INFO streaming.PipeMapRed: R/W/S=31500000/31491457/0 in:318181=31500000/99 [rec/s] out:318095=31491457/99 [rec/s]\n","2021-11-10 13:31:37,532 INFO streaming.PipeMapRed: R/W/S=31600000/31581536/0 in:319191=31600000/99 [rec/s] out:319005=31581536/99 [rec/s]\n","2021-11-10 13:31:37,835 INFO streaming.PipeMapRed: R/W/S=31700000/31686909/0 in:317000=31700000/100 [rec/s] out:316869=31686909/100 [rec/s]\n","2021-11-10 13:31:38,172 INFO streaming.PipeMapRed: R/W/S=31800000/31783463/0 in:318000=31800000/100 [rec/s] out:317834=31783489/100 [rec/s]\n","2021-11-10 13:31:38,329 INFO streaming.PipeMapRed: Records R/W=31849282/31838891\n","2021-11-10 13:31:38,483 INFO streaming.PipeMapRed: R/W/S=31900000/31886091/0 in:319000=31900000/100 [rec/s] out:318860=31886091/100 [rec/s]\n","2021-11-10 13:31:38,552 INFO mapred.LocalJobRunner: Records R/W=31849282/31838891 > reduce\n","2021-11-10 13:31:38,797 INFO streaming.PipeMapRed: R/W/S=32000000/31986614/0 in:320000=32000000/100 [rec/s] out:319866=31986614/100 [rec/s]\n","2021-11-10 13:31:39,100 INFO streaming.PipeMapRed: R/W/S=32100000/32088541/0 in:317821=32100000/101 [rec/s] out:317708=32088541/101 [rec/s]\n","2021-11-10 13:31:39,425 INFO streaming.PipeMapRed: R/W/S=32200000/32190459/0 in:318811=32200000/101 [rec/s] out:318717=32190459/101 [rec/s]\n","2021-11-10 13:31:39,709 INFO streaming.PipeMapRed: R/W/S=32300000/32284082/0 in:319801=32300000/101 [rec/s] out:319644=32284082/101 [rec/s]\n","2021-11-10 13:31:40,008 INFO streaming.PipeMapRed: R/W/S=32400000/32382554/0 in:317647=32400000/102 [rec/s] out:317476=32382554/102 [rec/s]\n","2021-11-10 13:31:40,317 INFO streaming.PipeMapRed: R/W/S=32500000/32481772/0 in:318627=32500000/102 [rec/s] out:318448=32481772/102 [rec/s]\n","2021-11-10 13:31:40,650 INFO streaming.PipeMapRed: R/W/S=32600000/32583624/0 in:319607=32600000/102 [rec/s] out:319447=32583624/102 [rec/s]\n","2021-11-10 13:31:40,961 INFO streaming.PipeMapRed: R/W/S=32700000/32686922/0 in:317475=32700000/103 [rec/s] out:317348=32686922/103 [rec/s]\n","2021-11-10 13:31:41,262 INFO streaming.PipeMapRed: R/W/S=32800000/32787564/0 in:318446=32800000/103 [rec/s] out:318325=32787564/103 [rec/s]\n","2021-11-10 13:31:41,554 INFO streaming.PipeMapRed: R/W/S=32900000/32889834/0 in:319417=32900000/103 [rec/s] out:319318=32889834/103 [rec/s]\n","2021-11-10 13:31:41,858 INFO streaming.PipeMapRed: R/W/S=33000000/32991476/0 in:317307=33000000/104 [rec/s] out:317225=32991476/104 [rec/s]\n","2021-11-10 13:31:42,116 INFO streaming.PipeMapRed: R/W/S=33100000/33080203/0 in:318269=33100000/104 [rec/s] out:318078=33080203/104 [rec/s]\n","2021-11-10 13:31:42,430 INFO streaming.PipeMapRed: R/W/S=33200000/33183008/0 in:319230=33200000/104 [rec/s] out:319067=33183008/104 [rec/s]\n","2021-11-10 13:31:42,729 INFO streaming.PipeMapRed: R/W/S=33300000/33282603/0 in:320192=33300000/104 [rec/s] out:320025=33282603/104 [rec/s]\n","2021-11-10 13:31:43,043 INFO streaming.PipeMapRed: R/W/S=33400000/33384618/0 in:318095=33400000/105 [rec/s] out:317948=33384618/105 [rec/s]\n","2021-11-10 13:31:43,354 INFO streaming.PipeMapRed: R/W/S=33500000/33486522/0 in:319047=33500000/105 [rec/s] out:318919=33486522/105 [rec/s]\n","2021-11-10 13:31:43,679 INFO streaming.PipeMapRed: R/W/S=33600000/33588649/0 in:320000=33600000/105 [rec/s] out:319891=33588649/105 [rec/s]\n","2021-11-10 13:31:43,990 INFO streaming.PipeMapRed: R/W/S=33700000/33690478/0 in:317924=33700000/106 [rec/s] out:317834=33690478/106 [rec/s]\n","2021-11-10 13:31:44,254 INFO streaming.PipeMapRed: R/W/S=33800000/33780371/0 in:318867=33800000/106 [rec/s] out:318682=33780371/106 [rec/s]\n","2021-11-10 13:31:44,553 INFO mapred.LocalJobRunner: Records R/W=31849282/31838891 > reduce\n","2021-11-10 13:31:44,557 INFO streaming.PipeMapRed: R/W/S=33900000/33882123/0 in:319811=33900000/106 [rec/s] out:319642=33882123/106 [rec/s]\n","2021-11-10 13:31:44,861 INFO streaming.PipeMapRed: R/W/S=34000000/33983656/0 in:317757=34000000/107 [rec/s] out:317604=33983656/107 [rec/s]\n","2021-11-10 13:31:45,166 INFO streaming.PipeMapRed: R/W/S=34100000/34089215/0 in:318691=34100000/107 [rec/s] out:318590=34089215/107 [rec/s]\n","2021-11-10 13:31:45,441 INFO streaming.PipeMapRed: R/W/S=34200000/34185636/0 in:319626=34200000/107 [rec/s] out:319491=34185636/107 [rec/s]\n","2021-11-10 13:31:45,810 INFO streaming.PipeMapRed: R/W/S=34300000/34287651/0 in:320560=34300000/107 [rec/s] out:320445=34287651/107 [rec/s]\n","2021-11-10 13:31:46,121 INFO streaming.PipeMapRed: R/W/S=34400000/34389480/0 in:318518=34400000/108 [rec/s] out:318421=34389480/108 [rec/s]\n","2021-11-10 13:31:46,419 INFO streaming.PipeMapRed: R/W/S=34500000/34491403/0 in:319444=34500000/108 [rec/s] out:319364=34491403/108 [rec/s]\n","2021-11-10 13:31:46,669 INFO streaming.PipeMapRed: R/W/S=34600000/34580840/0 in:320370=34600000/108 [rec/s] out:320192=34580840/108 [rec/s]\n","2021-11-10 13:31:46,976 INFO streaming.PipeMapRed: R/W/S=34700000/34680752/0 in:318348=34700000/109 [rec/s] out:318172=34680752/109 [rec/s]\n","2021-11-10 13:31:47,275 INFO streaming.PipeMapRed: R/W/S=34800000/34783015/0 in:319266=34800000/109 [rec/s] out:319110=34783022/109 [rec/s]\n","2021-11-10 13:31:47,573 INFO streaming.PipeMapRed: R/W/S=34900000/34884637/0 in:320183=34900000/109 [rec/s] out:320042=34884637/109 [rec/s]\n","2021-11-10 13:31:47,880 INFO streaming.PipeMapRed: R/W/S=35000000/34986653/0 in:318181=35000000/110 [rec/s] out:318060=34986653/110 [rec/s]\n","2021-11-10 13:31:48,187 INFO streaming.PipeMapRed: R/W/S=35100000/35088482/0 in:319090=35100000/110 [rec/s] out:318986=35088482/110 [rec/s]\n","2021-11-10 13:31:48,330 INFO streaming.PipeMapRed: Records R/W=35155083/35137990\n","2021-11-10 13:31:48,475 INFO streaming.PipeMapRed: R/W/S=35200000/35190311/0 in:320000=35200000/110 [rec/s] out:319911=35190311/110 [rec/s]\n","2021-11-10 13:31:48,763 INFO streaming.PipeMapRed: R/W/S=35300000/35282442/0 in:320909=35300000/110 [rec/s] out:320749=35282442/110 [rec/s]\n","2021-11-10 13:31:49,057 INFO streaming.PipeMapRed: R/W/S=35400000/35385774/0 in:318918=35400000/111 [rec/s] out:318790=35385774/111 [rec/s]\n","2021-11-10 13:31:49,343 INFO streaming.PipeMapRed: R/W/S=35500000/35486846/0 in:319819=35500000/111 [rec/s] out:319701=35486846/111 [rec/s]\n","2021-11-10 13:31:49,632 INFO streaming.PipeMapRed: R/W/S=35600000/35583665/0 in:320720=35600000/111 [rec/s] out:320573=35583670/111 [rec/s]\n","2021-11-10 13:31:49,940 INFO streaming.PipeMapRed: R/W/S=35700000/35685626/0 in:318750=35700000/112 [rec/s] out:318621=35685626/112 [rec/s]\n","2021-11-10 13:31:50,239 INFO streaming.PipeMapRed: R/W/S=35800000/35789163/0 in:319642=35800000/112 [rec/s] out:319546=35789163/112 [rec/s]\n","2021-11-10 13:31:50,536 INFO streaming.PipeMapRed: R/W/S=35900000/35889500/0 in:320535=35900000/112 [rec/s] out:320441=35889500/112 [rec/s]\n","2021-11-10 13:31:50,554 INFO mapred.LocalJobRunner: Records R/W=35155083/35137990 > reduce\n","2021-11-10 13:31:50,914 INFO streaming.PipeMapRed: R/W/S=36000000/35991329/0 in:318584=36000000/113 [rec/s] out:318507=35991329/113 [rec/s]\n","2021-11-10 13:31:51,189 INFO streaming.PipeMapRed: R/W/S=36100000/36081035/0 in:319469=36100000/113 [rec/s] out:319301=36081035/113 [rec/s]\n","2021-11-10 13:31:51,483 INFO streaming.PipeMapRed: R/W/S=36200000/36183237/0 in:320353=36200000/113 [rec/s] out:320205=36183237/113 [rec/s]\n","2021-11-10 13:31:51,779 INFO streaming.PipeMapRed: R/W/S=36300000/36284221/0 in:321238=36300000/113 [rec/s] out:321099=36284252/113 [rec/s]\n","2021-11-10 13:31:52,095 INFO streaming.PipeMapRed: R/W/S=36400000/36385233/0 in:319298=36400000/114 [rec/s] out:319168=36385251/114 [rec/s]\n","2021-11-10 13:31:52,405 INFO streaming.PipeMapRed: R/W/S=36500000/36486522/0 in:320175=36500000/114 [rec/s] out:320057=36486522/114 [rec/s]\n","2021-11-10 13:31:52,711 INFO streaming.PipeMapRed: R/W/S=36600000/36588688/0 in:321052=36600000/114 [rec/s] out:320953=36588688/114 [rec/s]\n","2021-11-10 13:31:53,062 INFO streaming.PipeMapRed: R/W/S=36700000/36690478/0 in:319130=36700000/115 [rec/s] out:319047=36690478/115 [rec/s]\n","2021-11-10 13:31:53,316 INFO streaming.PipeMapRed: R/W/S=36800000/36780970/0 in:320000=36800000/115 [rec/s] out:319834=36780970/115 [rec/s]\n","2021-11-10 13:31:53,608 INFO streaming.PipeMapRed: R/W/S=36900000/36882612/0 in:320869=36900000/115 [rec/s] out:320718=36882612/115 [rec/s]\n","2021-11-10 13:31:53,914 INFO streaming.PipeMapRed: R/W/S=37000000/36984031/0 in:318965=37000000/116 [rec/s] out:318827=36984037/116 [rec/s]\n","2021-11-10 13:31:54,216 INFO streaming.PipeMapRed: R/W/S=37100000/37083659/0 in:319827=37100000/116 [rec/s] out:319686=37083659/116 [rec/s]\n","2021-11-10 13:31:54,524 INFO streaming.PipeMapRed: R/W/S=37200000/37188659/0 in:320689=37200000/116 [rec/s] out:320591=37188659/116 [rec/s]\n","2021-11-10 13:31:54,809 INFO streaming.PipeMapRed: R/W/S=37300000/37287504/0 in:321551=37300000/116 [rec/s] out:321444=37287504/116 [rec/s]\n","2021-11-10 13:31:55,127 INFO streaming.PipeMapRed: R/W/S=37400000/37389519/0 in:319658=37400000/117 [rec/s] out:319568=37389519/117 [rec/s]\n","2021-11-10 13:31:55,418 INFO streaming.PipeMapRed: R/W/S=37500000/37491535/0 in:320512=37500000/117 [rec/s] out:320440=37491535/117 [rec/s]\n","2021-11-10 13:31:55,686 INFO streaming.PipeMapRed: R/W/S=37600000/37579915/0 in:321367=37600000/117 [rec/s] out:321195=37579915/117 [rec/s]\n","2021-11-10 13:31:56,006 INFO streaming.PipeMapRed: R/W/S=37700000/37683257/0 in:319491=37700000/118 [rec/s] out:319349=37683257/118 [rec/s]\n","2021-11-10 13:31:56,308 INFO streaming.PipeMapRed: R/W/S=37800000/37784284/0 in:320338=37800000/118 [rec/s] out:320205=37784289/118 [rec/s]\n","2021-11-10 13:31:56,555 INFO mapred.LocalJobRunner: Records R/W=35155083/35137990 > reduce\n","2021-11-10 13:31:56,608 INFO streaming.PipeMapRed: R/W/S=37900000/37886355/0 in:321186=37900000/118 [rec/s] out:321070=37886355/118 [rec/s]\n","2021-11-10 13:31:56,901 INFO streaming.PipeMapRed: R/W/S=38000000/37987276/0 in:319327=38000000/119 [rec/s] out:319220=37987281/119 [rec/s]\n","2021-11-10 13:31:57,227 INFO streaming.PipeMapRed: R/W/S=38100000/38089827/0 in:320168=38100000/119 [rec/s] out:320082=38089827/119 [rec/s]\n","2021-11-10 13:31:57,550 INFO streaming.PipeMapRed: R/W/S=38200000/38190723/0 in:321008=38200000/119 [rec/s] out:320930=38190723/119 [rec/s]\n","2021-11-10 13:31:57,830 INFO streaming.PipeMapRed: R/W/S=38300000/38283414/0 in:319166=38300000/120 [rec/s] out:319028=38283414/120 [rec/s]\n","2021-11-10 13:31:58,145 INFO streaming.PipeMapRed: R/W/S=38400000/38380649/0 in:320000=38400000/120 [rec/s] out:319838=38380654/120 [rec/s]\n","2021-11-10 13:31:58,331 INFO streaming.PipeMapRed: Records R/W=38460884/38444963\n","2021-11-10 13:31:58,449 INFO streaming.PipeMapRed: R/W/S=38500000/38483004/0 in:320833=38500000/120 [rec/s] out:320691=38483010/120 [rec/s]\n","2021-11-10 13:31:58,750 INFO streaming.PipeMapRed: R/W/S=38600000/38585171/0 in:321666=38600000/120 [rec/s] out:321543=38585171/120 [rec/s]\n","2021-11-10 13:31:59,060 INFO streaming.PipeMapRed: R/W/S=38700000/38685508/0 in:319834=38700000/121 [rec/s] out:319714=38685508/121 [rec/s]\n","2021-11-10 13:31:59,365 INFO streaming.PipeMapRed: R/W/S=38800000/38787523/0 in:320661=38800000/121 [rec/s] out:320558=38787523/121 [rec/s]\n","2021-11-10 13:31:59,659 INFO streaming.PipeMapRed: R/W/S=38900000/38890471/0 in:321487=38900000/121 [rec/s] out:321408=38890471/121 [rec/s]\n","2021-11-10 13:31:59,960 INFO streaming.PipeMapRed: R/W/S=39000000/38991368/0 in:319672=39000000/122 [rec/s] out:319601=38991368/122 [rec/s]\n","2021-11-10 13:32:00,245 INFO streaming.PipeMapRed: R/W/S=39100000/39081820/0 in:320491=39100000/122 [rec/s] out:320342=39081820/122 [rec/s]\n","2021-11-10 13:32:00,538 INFO streaming.PipeMapRed: R/W/S=39200000/39184717/0 in:321311=39200000/122 [rec/s] out:321186=39184717/122 [rec/s]\n","2021-11-10 13:32:00,850 INFO streaming.PipeMapRed: R/W/S=39300000/39287530/0 in:319512=39300000/123 [rec/s] out:319410=39287530/123 [rec/s]\n","2021-11-10 13:32:01,158 INFO streaming.PipeMapRed: R/W/S=39400000/39387494/0 in:320325=39400000/123 [rec/s] out:320223=39387494/123 [rec/s]\n","2021-11-10 13:32:01,456 INFO streaming.PipeMapRed: R/W/S=39500000/39487756/0 in:321138=39500000/123 [rec/s] out:321038=39487762/123 [rec/s]\n","2021-11-10 13:32:01,847 INFO streaming.PipeMapRed: R/W/S=39600000/39588502/0 in:319354=39600000/124 [rec/s] out:319262=39588502/124 [rec/s]\n","2021-11-10 13:32:02,176 INFO streaming.PipeMapRed: R/W/S=39700000/39690370/0 in:320161=39700000/124 [rec/s] out:320083=39690370/124 [rec/s]\n","2021-11-10 13:32:02,470 INFO streaming.PipeMapRed: R/W/S=39800000/39785858/0 in:320967=39800000/124 [rec/s] out:320853=39785858/124 [rec/s]\n","2021-11-10 13:32:02,555 INFO mapred.LocalJobRunner: Records R/W=38460884/38444963 > reduce\n","2021-11-10 13:32:02,757 INFO streaming.PipeMapRed: R/W/S=39900000/39880600/0 in:321774=39900000/124 [rec/s] out:321617=39880600/124 [rec/s]\n","2021-11-10 13:32:03,080 INFO streaming.PipeMapRed: R/W/S=40000000/39984667/0 in:320000=40000000/125 [rec/s] out:319877=39984667/125 [rec/s]\n","2021-11-10 13:32:03,372 INFO streaming.PipeMapRed: R/W/S=40100000/40085377/0 in:320800=40100000/125 [rec/s] out:320683=40085377/125 [rec/s]\n","2021-11-10 13:32:03,666 INFO streaming.PipeMapRed: R/W/S=40200000/40187206/0 in:321600=40200000/125 [rec/s] out:321497=40187206/125 [rec/s]\n","2021-11-10 13:32:03,955 INFO streaming.PipeMapRed: R/W/S=40300000/40287543/0 in:319841=40300000/126 [rec/s] out:319742=40287543/126 [rec/s]\n","2021-11-10 13:32:04,266 INFO streaming.PipeMapRed: R/W/S=40400000/40389558/0 in:320634=40400000/126 [rec/s] out:320552=40389558/126 [rec/s]\n","2021-11-10 13:32:04,563 INFO streaming.PipeMapRed: R/W/S=40500000/40491387/0 in:321428=40500000/126 [rec/s] out:321360=40491387/126 [rec/s]\n","2021-11-10 13:32:04,810 INFO streaming.PipeMapRed: R/W/S=40600000/40578759/0 in:322222=40600000/126 [rec/s] out:322053=40578759/126 [rec/s]\n","2021-11-10 13:32:05,136 INFO streaming.PipeMapRed: R/W/S=40700000/40682550/0 in:320472=40700000/127 [rec/s] out:320335=40682550/127 [rec/s]\n","2021-11-10 13:32:05,434 INFO streaming.PipeMapRed: R/W/S=40800000/40784674/0 in:321259=40800000/127 [rec/s] out:321139=40784675/127 [rec/s]\n","2021-11-10 13:32:05,727 INFO streaming.PipeMapRed: R/W/S=40900000/40884716/0 in:322047=40900000/127 [rec/s] out:321926=40884716/127 [rec/s]\n","2021-11-10 13:32:06,041 INFO streaming.PipeMapRed: R/W/S=41000000/40986545/0 in:320312=41000000/128 [rec/s] out:320207=40986545/128 [rec/s]\n","2021-11-10 13:32:06,352 INFO streaming.PipeMapRed: R/W/S=41100000/41088361/0 in:321093=41100000/128 [rec/s] out:321002=41088361/128 [rec/s]\n","2021-11-10 13:32:06,647 INFO streaming.PipeMapRed: R/W/S=41200000/41190388/0 in:321875=41200000/128 [rec/s] out:321799=41190388/128 [rec/s]\n","2021-11-10 13:32:06,920 INFO streaming.PipeMapRed: R/W/S=41300000/41280655/0 in:320155=41300000/129 [rec/s] out:320005=41280655/129 [rec/s]\n","2021-11-10 13:32:07,249 INFO streaming.PipeMapRed: R/W/S=41400000/41381497/0 in:320930=41400000/129 [rec/s] out:320786=41381501/129 [rec/s]\n","2021-11-10 13:32:07,544 INFO streaming.PipeMapRed: R/W/S=41500000/41483194/0 in:321705=41500000/129 [rec/s] out:321575=41483194/129 [rec/s]\n","2021-11-10 13:32:07,844 INFO streaming.PipeMapRed: R/W/S=41600000/41583587/0 in:320000=41600000/130 [rec/s] out:319873=41583587/130 [rec/s]\n","2021-11-10 13:32:08,139 INFO streaming.PipeMapRed: R/W/S=41700000/41685547/0 in:320769=41700000/130 [rec/s] out:320658=41685547/130 [rec/s]\n","2021-11-10 13:32:08,332 INFO streaming.PipeMapRed: Records R/W=41766685/41751569\n","2021-11-10 13:32:08,445 INFO streaming.PipeMapRed: R/W/S=41800000/41787487/0 in:321538=41800000/130 [rec/s] out:321442=41787487/130 [rec/s]\n","2021-11-10 13:32:08,556 INFO mapred.LocalJobRunner: Records R/W=41766685/41751569 > reduce\n","2021-11-10 13:32:08,770 INFO streaming.PipeMapRed: R/W/S=41900000/41889391/0 in:322307=41900000/130 [rec/s] out:322226=41889391/130 [rec/s]\n","2021-11-10 13:32:09,105 INFO streaming.PipeMapRed: R/W/S=42000000/41991407/0 in:320610=42000000/131 [rec/s] out:320545=41991407/131 [rec/s]\n","2021-11-10 13:32:09,381 INFO streaming.PipeMapRed: R/W/S=42100000/42080190/0 in:321374=42100000/131 [rec/s] out:321222=42080190/131 [rec/s]\n","2021-11-10 13:32:09,687 INFO streaming.PipeMapRed: R/W/S=42200000/42182756/0 in:322137=42200000/131 [rec/s] out:322005=42182756/131 [rec/s]\n","2021-11-10 13:32:09,975 INFO streaming.PipeMapRed: R/W/S=42300000/42284264/0 in:320454=42300000/132 [rec/s] out:320335=42284264/132 [rec/s]\n","2021-11-10 13:32:10,294 INFO streaming.PipeMapRed: R/W/S=42400000/42385387/0 in:321212=42400000/132 [rec/s] out:321101=42385387/132 [rec/s]\n","2021-11-10 13:32:10,594 INFO streaming.PipeMapRed: R/W/S=42500000/42486536/0 in:321969=42500000/132 [rec/s] out:321867=42486536/132 [rec/s]\n","2021-11-10 13:32:10,923 INFO streaming.PipeMapRed: R/W/S=42600000/42589139/0 in:320300=42600000/133 [rec/s] out:320219=42589139/133 [rec/s]\n","2021-11-10 13:32:11,227 INFO streaming.PipeMapRed: R/W/S=42700000/42690409/0 in:321052=42700000/133 [rec/s] out:320980=42690409/133 [rec/s]\n","2021-11-10 13:32:11,542 INFO streaming.PipeMapRed: R/W/S=42800000/42780115/0 in:321804=42800000/133 [rec/s] out:321655=42780115/133 [rec/s]\n","2021-11-10 13:32:11,843 INFO streaming.PipeMapRed: R/W/S=42900000/42881198/0 in:320149=42900000/134 [rec/s] out:320008=42881200/134 [rec/s]\n","2021-11-10 13:32:12,163 INFO streaming.PipeMapRed: R/W/S=43000000/42989368/0 in:320895=43000000/134 [rec/s] out:320816=42989368/134 [rec/s]\n","2021-11-10 13:32:12,464 INFO streaming.PipeMapRed: R/W/S=43100000/43083643/0 in:321641=43100000/134 [rec/s] out:321519=43083647/134 [rec/s]\n","2021-11-10 13:32:12,757 INFO streaming.PipeMapRed: R/W/S=43200000/43186872/0 in:322388=43200000/134 [rec/s] out:322290=43186872/134 [rec/s]\n","2021-11-10 13:32:13,072 INFO streaming.PipeMapRed: R/W/S=43300000/43287460/0 in:320740=43300000/135 [rec/s] out:320647=43287460/135 [rec/s]\n","2021-11-10 13:32:13,372 INFO streaming.PipeMapRed: R/W/S=43400000/43389411/0 in:321481=43400000/135 [rec/s] out:321403=43389411/135 [rec/s]\n","2021-11-10 13:32:13,659 INFO streaming.PipeMapRed: R/W/S=43500000/43491428/0 in:322222=43500000/135 [rec/s] out:322158=43491432/135 [rec/s]\n","2021-11-10 13:32:13,935 INFO streaming.PipeMapRed: R/W/S=43600000/43584476/0 in:320588=43600000/136 [rec/s] out:320474=43584480/136 [rec/s]\n","2021-11-10 13:32:14,248 INFO streaming.PipeMapRed: R/W/S=43700000/43688282/0 in:321323=43700000/136 [rec/s] out:321237=43688288/136 [rec/s]\n","2021-11-10 13:32:14,537 INFO streaming.PipeMapRed: R/W/S=43800000/43782739/0 in:322058=43800000/136 [rec/s] out:321931=43782739/136 [rec/s]\n","2021-11-10 13:32:14,557 INFO mapred.LocalJobRunner: Records R/W=41766685/41751569 > reduce\n","2021-11-10 13:32:14,846 INFO streaming.PipeMapRed: R/W/S=43900000/43884568/0 in:320437=43900000/137 [rec/s] out:320325=43884568/137 [rec/s]\n","2021-11-10 13:32:15,156 INFO streaming.PipeMapRed: R/W/S=44000000/43987181/0 in:321167=44000000/137 [rec/s] out:321074=43987181/137 [rec/s]\n","2021-11-10 13:32:15,459 INFO streaming.PipeMapRed: R/W/S=44100000/44088296/0 in:321897=44100000/137 [rec/s] out:321812=44088296/137 [rec/s]\n","2021-11-10 13:32:15,745 INFO streaming.PipeMapRed: R/W/S=44200000/44190642/0 in:322627=44200000/137 [rec/s] out:322559=44190646/137 [rec/s]\n","2021-11-10 13:32:16,057 INFO streaming.PipeMapRed: R/W/S=44300000/44280321/0 in:321014=44300000/138 [rec/s] out:320871=44280321/138 [rec/s]\n","2021-11-10 13:32:16,378 INFO streaming.PipeMapRed: R/W/S=44400000/44382999/0 in:321739=44400000/138 [rec/s] out:321615=44382999/138 [rec/s]\n","2021-11-10 13:32:16,674 INFO streaming.PipeMapRed: R/W/S=44500000/44483606/0 in:322463=44500000/138 [rec/s] out:322344=44483606/138 [rec/s]\n","2021-11-10 13:32:16,972 INFO streaming.PipeMapRed: R/W/S=44600000/44585571/0 in:320863=44600000/139 [rec/s] out:320759=44585575/139 [rec/s]\n","2021-11-10 13:32:17,308 INFO streaming.PipeMapRed: R/W/S=44700000/44686705/0 in:321582=44700000/139 [rec/s] out:321487=44686705/139 [rec/s]\n","2021-11-10 13:32:17,629 INFO streaming.PipeMapRed: R/W/S=44800000/44787415/0 in:322302=44800000/139 [rec/s] out:322211=44787415/139 [rec/s]\n","2021-11-10 13:32:17,953 INFO streaming.PipeMapRed: R/W/S=44900000/44889430/0 in:320714=44900000/140 [rec/s] out:320638=44889430/140 [rec/s]\n","2021-11-10 13:32:18,280 INFO streaming.PipeMapRed: R/W/S=45000000/44991446/0 in:321428=45000000/140 [rec/s] out:321367=44991446/140 [rec/s]\n","2021-11-10 13:32:18,333 INFO streaming.PipeMapRed: Records R/W=45023614/45008061\n","2021-11-10 13:32:18,583 INFO streaming.PipeMapRed: R/W/S=45100000/45081152/0 in:322142=45100000/140 [rec/s] out:322008=45081152/140 [rec/s]\n","2021-11-10 13:32:18,893 INFO streaming.PipeMapRed: R/W/S=45200000/45182875/0 in:320567=45200000/141 [rec/s] out:320445=45182880/141 [rec/s]\n","2021-11-10 13:32:19,215 INFO streaming.PipeMapRed: R/W/S=45300000/45283878/0 in:321276=45300000/141 [rec/s] out:321162=45283878/141 [rec/s]\n","2021-11-10 13:32:19,524 INFO streaming.PipeMapRed: R/W/S=45400000/45386266/0 in:321985=45400000/141 [rec/s] out:321888=45386266/141 [rec/s]\n","2021-11-10 13:32:19,822 INFO streaming.PipeMapRed: R/W/S=45500000/45486940/0 in:320422=45500000/142 [rec/s] out:320330=45486944/142 [rec/s]\n","2021-11-10 13:32:20,125 INFO streaming.PipeMapRed: R/W/S=45600000/45588358/0 in:321126=45600000/142 [rec/s] out:321044=45588358/142 [rec/s]\n","2021-11-10 13:32:20,421 INFO streaming.PipeMapRed: R/W/S=45700000/45690375/0 in:321830=45700000/142 [rec/s] out:321763=45690375/142 [rec/s]\n","2021-11-10 13:32:20,558 INFO mapred.LocalJobRunner: Records R/W=45023614/45008061 > reduce\n","2021-11-10 13:32:20,728 INFO streaming.PipeMapRed: R/W/S=45800000/45784630/0 in:322535=45800000/142 [rec/s] out:322426=45784630/142 [rec/s]\n","2021-11-10 13:32:21,030 INFO streaming.PipeMapRed: R/W/S=45900000/45882729/0 in:320979=45900000/143 [rec/s] out:320858=45882729/143 [rec/s]\n","2021-11-10 13:32:21,328 INFO streaming.PipeMapRed: R/W/S=46000000/45982320/0 in:321678=46000000/143 [rec/s] out:321554=45982324/143 [rec/s]\n","2021-11-10 13:32:21,632 INFO streaming.PipeMapRed: R/W/S=46100000/46085307/0 in:322377=46100000/143 [rec/s] out:322274=46085311/143 [rec/s]\n","2021-11-10 13:32:21,932 INFO streaming.PipeMapRed: R/W/S=46200000/46185605/0 in:320833=46200000/144 [rec/s] out:320733=46185605/144 [rec/s]\n","2021-11-10 13:32:22,230 INFO streaming.PipeMapRed: R/W/S=46300000/46287434/0 in:321527=46300000/144 [rec/s] out:321440=46287434/144 [rec/s]\n","2021-11-10 13:32:22,542 INFO streaming.PipeMapRed: R/W/S=46400000/46390009/0 in:322222=46400000/144 [rec/s] out:322152=46390009/144 [rec/s]\n","2021-11-10 13:32:22,844 INFO streaming.PipeMapRed: R/W/S=46500000/46491279/0 in:320689=46500000/145 [rec/s] out:320629=46491279/145 [rec/s]\n","2021-11-10 13:32:23,121 INFO streaming.PipeMapRed: R/W/S=46600000/46581358/0 in:321379=46600000/145 [rec/s] out:321250=46581358/145 [rec/s]\n","2021-11-10 13:32:23,415 INFO streaming.PipeMapRed: R/W/S=46700000/46681136/0 in:322068=46700000/145 [rec/s] out:321938=46681136/145 [rec/s]\n","2021-11-10 13:32:23,778 INFO streaming.PipeMapRed: R/W/S=46800000/46782965/0 in:322758=46800000/145 [rec/s] out:322641=46782965/145 [rec/s]\n","2021-11-10 13:32:24,074 INFO streaming.PipeMapRed: R/W/S=46900000/46884604/0 in:321232=46900000/146 [rec/s] out:321127=46884604/146 [rec/s]\n","2021-11-10 13:32:24,369 INFO streaming.PipeMapRed: R/W/S=47000000/46986436/0 in:321917=47000000/146 [rec/s] out:321824=46986436/146 [rec/s]\n","2021-11-10 13:32:24,681 INFO streaming.PipeMapRed: R/W/S=47100000/47088452/0 in:322602=47100000/146 [rec/s] out:322523=47088452/146 [rec/s]\n","2021-11-10 13:32:24,979 INFO streaming.PipeMapRed: R/W/S=47200000/47190347/0 in:321088=47200000/147 [rec/s] out:321022=47190347/147 [rec/s]\n","2021-11-10 13:32:25,246 INFO streaming.PipeMapRed: R/W/S=47300000/47277749/0 in:321768=47300000/147 [rec/s] out:321617=47277749/147 [rec/s]\n","2021-11-10 13:32:25,542 INFO streaming.PipeMapRed: R/W/S=47400000/47381177/0 in:322448=47400000/147 [rec/s] out:322320=47381180/147 [rec/s]\n","2021-11-10 13:32:25,836 INFO streaming.PipeMapRed: R/W/S=47500000/47481594/0 in:320945=47500000/148 [rec/s] out:320821=47481594/148 [rec/s]\n","2021-11-10 13:32:26,142 INFO streaming.PipeMapRed: R/W/S=47600000/47587899/0 in:321621=47600000/148 [rec/s] out:321539=47587899/148 [rec/s]\n","2021-11-10 13:32:26,436 INFO streaming.PipeMapRed: R/W/S=47700000/47685453/0 in:322297=47700000/148 [rec/s] out:322199=47685453/148 [rec/s]\n","2021-11-10 13:32:26,559 INFO mapred.LocalJobRunner: Records R/W=45023614/45008061 > reduce\n","2021-11-10 13:32:26,741 INFO streaming.PipeMapRed: R/W/S=47800000/47787454/0 in:322972=47800000/148 [rec/s] out:322888=47787454/148 [rec/s]\n","2021-11-10 13:32:27,039 INFO streaming.PipeMapRed: R/W/S=47900000/47889469/0 in:321476=47900000/149 [rec/s] out:321405=47889469/149 [rec/s]\n","2021-11-10 13:32:27,333 INFO streaming.PipeMapRed: R/W/S=48000000/47991298/0 in:322147=48000000/149 [rec/s] out:322089=47991298/149 [rec/s]\n","2021-11-10 13:32:27,592 INFO streaming.PipeMapRed: R/W/S=48100000/48080781/0 in:322818=48100000/149 [rec/s] out:322690=48080816/149 [rec/s]\n","2021-11-10 13:32:27,922 INFO streaming.PipeMapRed: R/W/S=48200000/48182834/0 in:321333=48200000/150 [rec/s] out:321218=48182834/150 [rec/s]\n","2021-11-10 13:32:28,222 INFO streaming.PipeMapRed: R/W/S=48300000/48284849/0 in:322000=48300000/150 [rec/s] out:321898=48284849/150 [rec/s]\n","2021-11-10 13:32:28,334 INFO streaming.PipeMapRed: Records R/W=48334598/48321760\n","2021-11-10 13:32:28,516 INFO streaming.PipeMapRed: R/W/S=48400000/48384151/0 in:322666=48400000/150 [rec/s] out:322561=48384151/150 [rec/s]\n","2021-11-10 13:32:28,907 INFO streaming.PipeMapRed: R/W/S=48500000/48487463/0 in:321192=48500000/151 [rec/s] out:321109=48487463/151 [rec/s]\n","2021-11-10 13:32:29,211 INFO streaming.PipeMapRed: R/W/S=48600000/48588471/0 in:321854=48600000/151 [rec/s] out:321777=48588471/151 [rec/s]\n","2021-11-10 13:32:29,514 INFO streaming.PipeMapRed: R/W/S=48700000/48690673/0 in:322516=48700000/151 [rec/s] out:322454=48690673/151 [rec/s]\n","2021-11-10 13:32:29,816 INFO streaming.PipeMapRed: R/W/S=48800000/48781499/0 in:323178=48800000/151 [rec/s] out:323056=48781499/151 [rec/s]\n","2021-11-10 13:32:30,199 INFO streaming.PipeMapRed: R/W/S=48900000/48879971/0 in:321710=48900000/152 [rec/s] out:321578=48879971/152 [rec/s]\n","2021-11-10 13:32:30,634 INFO streaming.PipeMapRed: R/W/S=49000000/48983851/0 in:322368=49000000/152 [rec/s] out:322262=48983851/152 [rec/s]\n","2021-11-10 13:32:30,956 INFO streaming.PipeMapRed: R/W/S=49100000/49085943/0 in:320915=49100000/153 [rec/s] out:320823=49085948/153 [rec/s]\n","2021-11-10 13:32:31,257 INFO streaming.PipeMapRed: R/W/S=49200000/49185458/0 in:321568=49200000/153 [rec/s] out:321473=49185458/153 [rec/s]\n","2021-11-10 13:32:31,560 INFO streaming.PipeMapRed: R/W/S=49300000/49287473/0 in:322222=49300000/153 [rec/s] out:322140=49287473/153 [rec/s]\n","2021-11-10 13:32:31,879 INFO streaming.PipeMapRed: R/W/S=49400000/49389396/0 in:320779=49400000/154 [rec/s] out:320710=49389396/154 [rec/s]\n","2021-11-10 13:32:32,193 INFO streaming.PipeMapRed: R/W/S=49500000/49491318/0 in:321428=49500000/154 [rec/s] out:321372=49491318/154 [rec/s]\n","2021-11-10 13:32:32,482 INFO streaming.PipeMapRed: R/W/S=49600000/49586992/0 in:322077=49600000/154 [rec/s] out:321993=49586992/154 [rec/s]\n","2021-11-10 13:32:32,560 INFO mapred.LocalJobRunner: Records R/W=48334598/48321760 > reduce\n","2021-11-10 13:32:32,761 INFO streaming.PipeMapRed: R/W/S=49700000/49680615/0 in:322727=49700000/154 [rec/s] out:322601=49680615/154 [rec/s]\n","2021-11-10 13:32:33,081 INFO streaming.PipeMapRed: R/W/S=49800000/49786174/0 in:321290=49800000/155 [rec/s] out:321201=49786174/155 [rec/s]\n","2021-11-10 13:32:33,361 INFO streaming.PipeMapRed: R/W/S=49900000/49884683/0 in:321935=49900000/155 [rec/s] out:321836=49884683/155 [rec/s]\n","2021-11-10 13:32:33,677 INFO streaming.PipeMapRed: R/W/S=50000000/49986475/0 in:322580=50000000/155 [rec/s] out:322493=49986475/155 [rec/s]\n","2021-11-10 13:32:34,081 INFO streaming.PipeMapRed: R/W/S=50100000/50089235/0 in:321153=50100000/156 [rec/s] out:321084=50089237/156 [rec/s]\n","2021-11-10 13:32:34,401 INFO streaming.PipeMapRed: R/W/S=50200000/50190320/0 in:321794=50200000/156 [rec/s] out:321732=50190320/156 [rec/s]\n","2021-11-10 13:32:34,657 INFO streaming.PipeMapRed: R/W/S=50300000/50277788/0 in:322435=50300000/156 [rec/s] out:322293=50277788/156 [rec/s]\n","2021-11-10 13:32:34,994 INFO streaming.PipeMapRed: R/W/S=50400000/50382042/0 in:321019=50400000/157 [rec/s] out:320904=50382042/157 [rec/s]\n","2021-11-10 13:32:35,301 INFO streaming.PipeMapRed: R/W/S=50500000/50483743/0 in:321656=50500000/157 [rec/s] out:321552=50483743/157 [rec/s]\n","2021-11-10 13:32:35,609 INFO streaming.PipeMapRed: R/W/S=50600000/50583648/0 in:322292=50600000/157 [rec/s] out:322188=50583648/157 [rec/s]\n","2021-11-10 13:32:35,937 INFO streaming.PipeMapRed: R/W/S=50700000/50688834/0 in:320886=50700000/158 [rec/s] out:320815=50688834/158 [rec/s]\n","2021-11-10 13:32:36,313 INFO streaming.PipeMapRed: R/W/S=50800000/50787427/0 in:321518=50800000/158 [rec/s] out:321439=50787427/158 [rec/s]\n","2021-11-10 13:32:36,604 INFO streaming.PipeMapRed: R/W/S=50900000/50889881/0 in:322151=50900000/158 [rec/s] out:322087=50889881/158 [rec/s]\n","2021-11-10 13:32:36,942 INFO streaming.PipeMapRed: R/W/S=51000000/50991220/0 in:320754=51000000/159 [rec/s] out:320699=50991220/159 [rec/s]\n","2021-11-10 13:32:37,209 INFO streaming.PipeMapRed: R/W/S=51100000/51080109/0 in:321383=51100000/159 [rec/s] out:321258=51080109/159 [rec/s]\n","2021-11-10 13:32:37,518 INFO streaming.PipeMapRed: R/W/S=51200000/51182686/0 in:322012=51200000/159 [rec/s] out:321903=51182686/159 [rec/s]\n","2021-11-10 13:32:37,847 INFO streaming.PipeMapRed: R/W/S=51300000/51290856/0 in:320625=51300000/160 [rec/s] out:320567=51290856/160 [rec/s]\n","2021-11-10 13:32:38,145 INFO streaming.PipeMapRed: R/W/S=51400000/51384441/0 in:321250=51400000/160 [rec/s] out:321152=51384441/160 [rec/s]\n","2021-11-10 13:32:38,335 INFO streaming.PipeMapRed: Records R/W=51465643/51448823\n","2021-11-10 13:32:38,464 INFO streaming.PipeMapRed: R/W/S=51500000/51487614/0 in:321875=51500000/160 [rec/s] out:321797=51487614/160 [rec/s]\n","2021-11-10 13:32:38,561 INFO mapred.LocalJobRunner: Records R/W=51465643/51448823 > reduce\n","2021-11-10 13:32:38,773 INFO streaming.PipeMapRed: R/W/S=51600000/51589282/0 in:322500=51600000/160 [rec/s] out:322433=51589282/160 [rec/s]\n","2021-11-10 13:32:39,089 INFO streaming.PipeMapRed: R/W/S=51700000/51690339/0 in:321118=51700000/161 [rec/s] out:321058=51690339/161 [rec/s]\n","2021-11-10 13:32:39,354 INFO streaming.PipeMapRed: R/W/S=51800000/51777808/0 in:321739=51800000/161 [rec/s] out:321601=51777808/161 [rec/s]\n","2021-11-10 13:32:39,671 INFO streaming.PipeMapRed: R/W/S=51900000/51882248/0 in:322360=51900000/161 [rec/s] out:322249=51882248/161 [rec/s]\n","2021-11-10 13:32:39,977 INFO streaming.PipeMapRed: R/W/S=52000000/51981652/0 in:320987=52000000/162 [rec/s] out:320874=51981652/162 [rec/s]\n","2021-11-10 13:32:40,298 INFO streaming.PipeMapRed: R/W/S=52100000/52084109/0 in:321604=52100000/162 [rec/s] out:321506=52084112/162 [rec/s]\n","2021-11-10 13:32:40,594 INFO streaming.PipeMapRed: R/W/S=52200000/52185310/0 in:322222=52200000/162 [rec/s] out:322131=52185310/162 [rec/s]\n","2021-11-10 13:32:40,894 INFO streaming.PipeMapRed: R/W/S=52300000/52287379/0 in:320858=52300000/163 [rec/s] out:320781=52287394/163 [rec/s]\n","2021-11-10 13:32:41,208 INFO streaming.PipeMapRed: R/W/S=52400000/52389528/0 in:321472=52400000/163 [rec/s] out:321408=52389528/163 [rec/s]\n","2021-11-10 13:32:41,541 INFO streaming.PipeMapRed: R/W/S=52500000/52490922/0 in:322085=52500000/163 [rec/s] out:322030=52490922/163 [rec/s]\n","2021-11-10 13:32:41,823 INFO streaming.PipeMapRed: R/W/S=52600000/52580536/0 in:320731=52600000/164 [rec/s] out:320613=52580540/164 [rec/s]\n","2021-11-10 13:32:42,143 INFO streaming.PipeMapRed: R/W/S=52700000/52682333/0 in:321341=52700000/164 [rec/s] out:321233=52682333/164 [rec/s]\n","2021-11-10 13:32:42,444 INFO streaming.PipeMapRed: R/W/S=52800000/52783487/0 in:321951=52800000/164 [rec/s] out:321850=52783487/164 [rec/s]\n","2021-11-10 13:32:42,739 INFO streaming.PipeMapRed: R/W/S=52900000/52885618/0 in:322560=52900000/164 [rec/s] out:322473=52885627/164 [rec/s]\n","2021-11-10 13:32:43,082 INFO streaming.PipeMapRed: R/W/S=53000000/52986514/0 in:321212=53000000/165 [rec/s] out:321130=52986514/165 [rec/s]\n","2021-11-10 13:32:43,389 INFO streaming.PipeMapRed: R/W/S=53100000/53086929/0 in:321818=53100000/165 [rec/s] out:321738=53086929/165 [rec/s]\n","2021-11-10 13:32:43,711 INFO streaming.PipeMapRed: R/W/S=53200000/53191478/0 in:322424=53200000/165 [rec/s] out:322372=53191478/165 [rec/s]\n","2021-11-10 13:32:44,001 INFO streaming.PipeMapRed: R/W/S=53300000/53281930/0 in:321084=53300000/166 [rec/s] out:320975=53281930/166 [rec/s]\n","2021-11-10 13:32:44,323 INFO streaming.PipeMapRed: R/W/S=53400000/53382176/0 in:321686=53400000/166 [rec/s] out:321579=53382181/166 [rec/s]\n","2021-11-10 13:32:44,563 INFO mapred.LocalJobRunner: Records R/W=51465643/51448823 > reduce\n","2021-11-10 13:32:44,643 INFO streaming.PipeMapRed: R/W/S=53500000/53483482/0 in:322289=53500000/166 [rec/s] out:322189=53483485/166 [rec/s]\n","2021-11-10 13:32:44,968 INFO streaming.PipeMapRed: R/W/S=53600000/53583592/0 in:320958=53600000/167 [rec/s] out:320860=53583621/167 [rec/s]\n","2021-11-10 13:32:45,284 INFO streaming.PipeMapRed: R/W/S=53700000/53685586/0 in:321556=53700000/167 [rec/s] out:321470=53685619/167 [rec/s]\n","2021-11-10 13:32:45,620 INFO streaming.PipeMapRed: R/W/S=53800000/53789024/0 in:322155=53800000/167 [rec/s] out:322089=53789024/167 [rec/s]\n","2021-11-10 13:32:45,922 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-10 13:32:45,923 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-10 13:32:45,939 INFO mapred.Task: Task:attempt_local1867248162_0001_r_000000_0 is done. And is in the process of committing\n","2021-11-10 13:32:45,943 INFO mapred.LocalJobRunner: Records R/W=51465643/51448823 > reduce\n","2021-11-10 13:32:45,943 INFO mapred.Task: Task attempt_local1867248162_0001_r_000000_0 is allowed to commit now\n","2021-11-10 13:32:45,960 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1867248162_0001_r_000000_0' to hdfs://localhost:9000/user/job/req2/output\n","2021-11-10 13:32:45,961 INFO mapred.LocalJobRunner: Records R/W=51465643/51448823 > reduce\n","2021-11-10 13:32:45,961 INFO mapred.Task: Task 'attempt_local1867248162_0001_r_000000_0' done.\n","2021-11-10 13:32:45,962 INFO mapred.Task: Final Counters for attempt_local1867248162_0001_r_000000_0: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=1184910419\n","\t\tFILE: Number of bytes written=1185361347\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=7868883275\n","\t\tHDFS: Number of bytes written=2369467848\n","\t\tHDFS: Number of read operations=126\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=3\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=592367316\n","\t\tReduce input records=53851542\n","\t\tReduce output records=53851542\n","\t\tSpilled Records=53851542\n","\t\tShuffled Maps =59\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=59\n","\t\tGC time elapsed (ms)=220\n","\t\tTotal committed heap usage (bytes)=686817280\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=2369467848\n","2021-11-10 13:32:45,962 INFO mapred.LocalJobRunner: Finishing task: attempt_local1867248162_0001_r_000000_0\n","2021-11-10 13:32:45,962 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2021-11-10 13:32:46,065 INFO mapreduce.Job:  map 100% reduce 100%\n","2021-11-10 13:32:47,065 INFO mapreduce.Job: Job job_local1867248162_0001 completed successfully\n","2021-11-10 13:32:47,096 INFO mapreduce.Job: Counters: 36\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=1191427600\n","\t\tFILE: Number of bytes written=19067017985\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=245391307414\n","\t\tHDFS: Number of bytes written=2369467848\n","\t\tHDFS: Number of read operations=3843\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=62\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=53851542\n","\t\tMap output records=53851542\n","\t\tMap output bytes=484663878\n","\t\tMap output materialized bytes=592367316\n","\t\tInput split bytes=5841\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=592367316\n","\t\tReduce input records=53851542\n","\t\tReduce output records=53851542\n","\t\tSpilled Records=107703084\n","\t\tShuffled Maps =59\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=59\n","\t\tGC time elapsed (ms)=625\n","\t\tTotal committed heap usage (bytes)=41209036800\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=7868883275\n","\tFile Output Format Counters \n","\t\tBytes Written=2369467848\n","2021-11-10 13:32:47,096 INFO streaming.StreamJob: Output directory: /user//job/req2/output\n"]}]},{"cell_type":"code","metadata":{"id":"BtdnM3ePUrqi"},"source":["!${hdfs} dfs  -copyToLocal  /user/$USER/job/req2/output  /content/drive/MyDrive/mp1-bigdata/req2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAhUJnnnUimG","executionInfo":{"status":"ok","timestamp":1636551500026,"user_tz":-120,"elapsed":4147,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"e41a1df1-94b1-4c76-9756-20cff396376f"},"source":["!/usr/local/hadoop-3.3.0/bin/hdfs  dfs  -rm  -r /user/$USER/job/req2/output\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted /user/job/req2/output\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KW1Eax3AYScQ","executionInfo":{"status":"ok","timestamp":1636581215993,"user_tz":-120,"elapsed":603,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"4212db77-a3c7-4670-cfc9-c345f466a2c6"},"source":["# viewing the data\n","!head -n 20 /content/drive/MyDrive/mp1-bigdata/req2/output2/part-00000 >> sample.txt\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["head: cannot open '/content/drive/MyDrive/mp1-bigdata/req2/output2/part-00000' for reading: No such file or directory\n"]}]},{"cell_type":"markdown","metadata":{"id":"FtzKiOBLrItv"},"source":["### controversiality vs comment on posts rate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qcVo3T3brN-q","executionInfo":{"status":"ok","timestamp":1636748341486,"user_tz":-120,"elapsed":31670,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"b7edb2e5-3f23-484f-e595-473820ea6674"},"source":["!${hadoop_stream}  -input /user/$USER/job/input/ -output /user/$USER/job/req2-posts/output -file /content/drive/MyDrive/mp1-bigdata/req2-posts/mapper.py  -file /content/drive/MyDrive/mp1-bigdata/req2-posts/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-11-12 20:18:30,589 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","packageJobJar: [/content/drive/MyDrive/mp1-bigdata/req2-posts/mapper.py, /content/drive/MyDrive/mp1-bigdata/req2-posts/reducer.py] [] /tmp/streamjob10674997409642706291.jar tmpDir=null\n","2021-11-12 20:18:31,866 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2021-11-12 20:18:31,998 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2021-11-12 20:18:31,998 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2021-11-12 20:18:32,016 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2021-11-12 20:18:32,461 INFO mapred.FileInputFormat: Total input files to process : 1\n","2021-11-12 20:18:32,487 INFO mapreduce.JobSubmitter: number of splits:5\n","2021-11-12 20:18:32,773 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local29580245_0001\n","2021-11-12 20:18:32,773 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2021-11-12 20:18:33,133 INFO mapred.LocalDistributedCacheManager: Localized file:/content/drive/MyDrive/mp1-bigdata/req2-posts/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local29580245_0001_0bca0563-0862-4327-ac1e-3bf5e607c45d/mapper.py\n","2021-11-12 20:18:33,167 INFO mapred.LocalDistributedCacheManager: Localized file:/content/drive/MyDrive/mp1-bigdata/req2-posts/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local29580245_0001_01e422d5-ee84-4d1a-a94e-43cb3c1b4e3b/reducer.py\n","2021-11-12 20:18:33,277 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2021-11-12 20:18:33,279 INFO mapreduce.Job: Running job: job_local29580245_0001\n","2021-11-12 20:18:33,287 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2021-11-12 20:18:33,289 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2021-11-12 20:18:33,296 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-12 20:18:33,296 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-12 20:18:33,385 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2021-11-12 20:18:33,389 INFO mapred.LocalJobRunner: Starting task: attempt_local29580245_0001_m_000000_0\n","2021-11-12 20:18:33,435 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-12 20:18:33,438 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-12 20:18:33,464 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-12 20:18:33,474 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/sample.txt:0+134217728\n","2021-11-12 20:18:33,521 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-12 20:18:33,718 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-12 20:18:33,718 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-12 20:18:33,718 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-12 20:18:33,718 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-12 20:18:33,718 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-12 20:18:33,722 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-12 20:18:33,733 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-12 20:18:33,744 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2021-11-12 20:18:33,744 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2021-11-12 20:18:33,745 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2021-11-12 20:18:33,745 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2021-11-12 20:18:33,746 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2021-11-12 20:18:33,747 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2021-11-12 20:18:33,748 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2021-11-12 20:18:33,749 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2021-11-12 20:18:33,749 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2021-11-12 20:18:33,750 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2021-11-12 20:18:33,750 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2021-11-12 20:18:33,751 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2021-11-12 20:18:33,975 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:33,981 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:33,996 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:34,063 INFO streaming.PipeMapRed: Records R/W=611/1\n","2021-11-12 20:18:34,081 INFO streaming.PipeMapRed: R/W/S=1000/441/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:34,286 INFO mapreduce.Job: Job job_local29580245_0001 running in uber mode : false\n","2021-11-12 20:18:34,287 INFO mapreduce.Job:  map 0% reduce 0%\n","2021-11-12 20:18:34,396 INFO streaming.PipeMapRed: R/W/S=10000/9696/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:36,049 INFO streaming.PipeMapRed: R/W/S=100000/99605/0 in:50000=100000/2 [rec/s] out:49802=99605/2 [rec/s]\n","2021-11-12 20:18:37,754 INFO streaming.PipeMapRed: R/W/S=200000/199673/0 in:66666=200000/3 [rec/s] out:66557=199673/3 [rec/s]\n","2021-11-12 20:18:38,625 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-12 20:18:38,626 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-12 20:18:38,630 INFO mapred.LocalJobRunner: \n","2021-11-12 20:18:38,630 INFO mapred.MapTask: Starting flush of map output\n","2021-11-12 20:18:38,630 INFO mapred.MapTask: Spilling map output\n","2021-11-12 20:18:38,630 INFO mapred.MapTask: bufstart = 0; bufend = 4617567; bufvoid = 104857600\n","2021-11-12 20:18:38,633 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25271740(101086960); length = 942657/6553600\n","2021-11-12 20:18:39,697 INFO mapred.MapTask: Finished spill 0\n","2021-11-12 20:18:39,713 INFO mapred.Task: Task:attempt_local29580245_0001_m_000000_0 is done. And is in the process of committing\n","2021-11-12 20:18:39,719 INFO mapred.LocalJobRunner: Records R/W=611/1\n","2021-11-12 20:18:39,720 INFO mapred.Task: Task 'attempt_local29580245_0001_m_000000_0' done.\n","2021-11-12 20:18:39,732 INFO mapred.Task: Final Counters for attempt_local29580245_0001_m_000000_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=2021\n","\t\tFILE: Number of bytes written=5701832\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=134221824\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=5\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=235665\n","\t\tMap output records=235665\n","\t\tMap output bytes=4617567\n","\t\tMap output materialized bytes=5088903\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=235665\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=38\n","\t\tTotal committed heap usage (bytes)=367001600\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-12 20:18:39,732 INFO mapred.LocalJobRunner: Finishing task: attempt_local29580245_0001_m_000000_0\n","2021-11-12 20:18:39,732 INFO mapred.LocalJobRunner: Starting task: attempt_local29580245_0001_m_000001_0\n","2021-11-12 20:18:39,734 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-12 20:18:39,734 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-12 20:18:39,735 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-12 20:18:39,737 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/sample.txt:134217728+134217728\n","2021-11-12 20:18:39,762 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-12 20:18:39,816 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-12 20:18:39,820 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-12 20:18:39,829 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-12 20:18:39,829 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-12 20:18:39,829 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-12 20:18:39,841 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-12 20:18:39,862 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-12 20:18:39,886 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:39,887 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:39,889 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:40,004 INFO streaming.PipeMapRed: Records R/W=718/1\n","2021-11-12 20:18:40,015 INFO streaming.PipeMapRed: R/W/S=1000/440/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:40,236 INFO streaming.PipeMapRed: R/W/S=10000/9702/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:40,296 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-12 20:18:41,849 INFO streaming.PipeMapRed: R/W/S=100000/99660/0 in:100000=100000/1 [rec/s] out:99660=99660/1 [rec/s]\n","2021-11-12 20:18:43,498 INFO streaming.PipeMapRed: R/W/S=200000/199343/0 in:66666=200000/3 [rec/s] out:66447=199343/3 [rec/s]\n","2021-11-12 20:18:44,075 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-12 20:18:44,075 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-12 20:18:44,076 INFO mapred.LocalJobRunner: \n","2021-11-12 20:18:44,077 INFO mapred.MapTask: Starting flush of map output\n","2021-11-12 20:18:44,077 INFO mapred.MapTask: Spilling map output\n","2021-11-12 20:18:44,077 INFO mapred.MapTask: bufstart = 0; bufend = 4613117; bufvoid = 104857600\n","2021-11-12 20:18:44,077 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25272100(101088400); length = 942297/6553600\n","2021-11-12 20:18:44,299 INFO mapreduce.Job:  map 20% reduce 0%\n","2021-11-12 20:18:44,415 INFO mapred.MapTask: Finished spill 0\n","2021-11-12 20:18:44,422 INFO mapred.Task: Task:attempt_local29580245_0001_m_000001_0 is done. And is in the process of committing\n","2021-11-12 20:18:44,428 INFO mapred.LocalJobRunner: Records R/W=718/1\n","2021-11-12 20:18:44,428 INFO mapred.Task: Task 'attempt_local29580245_0001_m_000001_0' done.\n","2021-11-12 20:18:44,429 INFO mapred.Task: Final Counters for attempt_local29580245_0001_m_000001_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=2535\n","\t\tFILE: Number of bytes written=10786137\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=268443648\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=7\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=235575\n","\t\tMap output records=235575\n","\t\tMap output bytes=4613117\n","\t\tMap output materialized bytes=5084273\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=235575\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=20\n","\t\tTotal committed heap usage (bytes)=514850816\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-12 20:18:44,429 INFO mapred.LocalJobRunner: Finishing task: attempt_local29580245_0001_m_000001_0\n","2021-11-12 20:18:44,430 INFO mapred.LocalJobRunner: Starting task: attempt_local29580245_0001_m_000002_0\n","2021-11-12 20:18:44,433 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-12 20:18:44,434 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-12 20:18:44,435 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-12 20:18:44,437 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/sample.txt:268435456+134217728\n","2021-11-12 20:18:44,451 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-12 20:18:44,580 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-12 20:18:44,580 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-12 20:18:44,580 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-12 20:18:44,580 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-12 20:18:44,580 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-12 20:18:44,581 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-12 20:18:44,589 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-12 20:18:44,621 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:44,621 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:44,621 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:44,732 INFO streaming.PipeMapRed: Records R/W=694/1\n","2021-11-12 20:18:44,743 INFO streaming.PipeMapRed: R/W/S=1000/441/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:44,954 INFO streaming.PipeMapRed: R/W/S=10000/9709/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:45,299 INFO mapreduce.Job:  map 100% reduce 0%\n","2021-11-12 20:18:46,381 INFO streaming.PipeMapRed: R/W/S=100000/99787/0 in:100000=100000/1 [rec/s] out:99787=99787/1 [rec/s]\n","2021-11-12 20:18:48,028 INFO streaming.PipeMapRed: R/W/S=200000/199590/0 in:66666=200000/3 [rec/s] out:66530=199590/3 [rec/s]\n","2021-11-12 20:18:48,496 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-12 20:18:48,497 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-12 20:18:48,497 INFO mapred.LocalJobRunner: \n","2021-11-12 20:18:48,497 INFO mapred.MapTask: Starting flush of map output\n","2021-11-12 20:18:48,498 INFO mapred.MapTask: Spilling map output\n","2021-11-12 20:18:48,498 INFO mapred.MapTask: bufstart = 0; bufend = 4505038; bufvoid = 104857600\n","2021-11-12 20:18:48,498 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25293156(101172624); length = 921241/6553600\n","2021-11-12 20:18:48,761 INFO mapred.MapTask: Finished spill 0\n","2021-11-12 20:18:48,764 INFO mapred.Task: Task:attempt_local29580245_0001_m_000002_0 is done. And is in the process of committing\n","2021-11-12 20:18:48,768 INFO mapred.LocalJobRunner: Records R/W=694/1\n","2021-11-12 20:18:48,768 INFO mapred.Task: Task 'attempt_local29580245_0001_m_000002_0' done.\n","2021-11-12 20:18:48,769 INFO mapred.Task: Final Counters for attempt_local29580245_0001_m_000002_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=3049\n","\t\tFILE: Number of bytes written=15751835\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=402665472\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=9\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=230311\n","\t\tMap output records=230311\n","\t\tMap output bytes=4505038\n","\t\tMap output materialized bytes=4965666\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=230311\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=25\n","\t\tTotal committed heap usage (bytes)=514850816\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-12 20:18:48,769 INFO mapred.LocalJobRunner: Finishing task: attempt_local29580245_0001_m_000002_0\n","2021-11-12 20:18:48,769 INFO mapred.LocalJobRunner: Starting task: attempt_local29580245_0001_m_000003_0\n","2021-11-12 20:18:48,772 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-12 20:18:48,772 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-12 20:18:48,773 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-12 20:18:48,774 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/sample.txt:402653184+134217728\n","2021-11-12 20:18:48,795 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-12 20:18:48,813 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-12 20:18:48,813 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-12 20:18:48,813 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-12 20:18:48,813 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-12 20:18:48,813 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-12 20:18:48,814 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-12 20:18:48,824 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-12 20:18:48,845 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:48,845 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:48,845 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:48,956 INFO streaming.PipeMapRed: Records R/W=710/1\n","2021-11-12 20:18:48,963 INFO streaming.PipeMapRed: R/W/S=1000/441/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:49,094 INFO streaming.PipeMapRed: R/W/S=10000/9718/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:50,488 INFO streaming.PipeMapRed: R/W/S=100000/99735/0 in:100000=100000/1 [rec/s] out:99735=99735/1 [rec/s]\n","2021-11-12 20:18:52,041 INFO streaming.PipeMapRed: R/W/S=200000/199830/0 in:66666=200000/3 [rec/s] out:66610=199830/3 [rec/s]\n","2021-11-12 20:18:52,539 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-12 20:18:52,540 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-12 20:18:52,540 INFO mapred.LocalJobRunner: \n","2021-11-12 20:18:52,541 INFO mapred.MapTask: Starting flush of map output\n","2021-11-12 20:18:52,541 INFO mapred.MapTask: Spilling map output\n","2021-11-12 20:18:52,541 INFO mapred.MapTask: bufstart = 0; bufend = 4546219; bufvoid = 104857600\n","2021-11-12 20:18:52,541 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25285580(101142320); length = 928817/6553600\n","2021-11-12 20:18:52,829 INFO mapred.MapTask: Finished spill 0\n","2021-11-12 20:18:52,842 INFO mapred.Task: Task:attempt_local29580245_0001_m_000003_0 is done. And is in the process of committing\n","2021-11-12 20:18:52,850 INFO mapred.LocalJobRunner: Records R/W=710/1\n","2021-11-12 20:18:52,850 INFO mapred.Task: Task 'attempt_local29580245_0001_m_000003_0' done.\n","2021-11-12 20:18:52,851 INFO mapred.Task: Final Counters for attempt_local29580245_0001_m_000003_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=3563\n","\t\tFILE: Number of bytes written=20762502\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=536887296\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=11\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=232205\n","\t\tMap output records=232205\n","\t\tMap output bytes=4546219\n","\t\tMap output materialized bytes=5010635\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=232205\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=514850816\n","\tFile Input Format Counters \n","\t\tBytes Read=134221824\n","2021-11-12 20:18:52,852 INFO mapred.LocalJobRunner: Finishing task: attempt_local29580245_0001_m_000003_0\n","2021-11-12 20:18:52,852 INFO mapred.LocalJobRunner: Starting task: attempt_local29580245_0001_m_000004_0\n","2021-11-12 20:18:52,854 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-12 20:18:52,854 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-12 20:18:52,855 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-12 20:18:52,856 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/job/input/sample.txt:536870912+38144132\n","2021-11-12 20:18:52,876 INFO mapred.MapTask: numReduceTasks: 1\n","2021-11-12 20:18:52,919 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2021-11-12 20:18:52,919 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2021-11-12 20:18:52,919 INFO mapred.MapTask: soft limit at 83886080\n","2021-11-12 20:18:52,919 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2021-11-12 20:18:52,919 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2021-11-12 20:18:52,920 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2021-11-12 20:18:52,931 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n","2021-11-12 20:18:52,958 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:52,958 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:52,958 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:53,058 INFO streaming.PipeMapRed: Records R/W=654/1\n","2021-11-12 20:18:53,067 INFO streaming.PipeMapRed: R/W/S=1000/441/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:53,202 INFO streaming.PipeMapRed: R/W/S=10000/9697/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:54,084 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-12 20:18:54,085 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-12 20:18:54,085 INFO mapred.LocalJobRunner: \n","2021-11-12 20:18:54,085 INFO mapred.MapTask: Starting flush of map output\n","2021-11-12 20:18:54,086 INFO mapred.MapTask: Spilling map output\n","2021-11-12 20:18:54,086 INFO mapred.MapTask: bufstart = 0; bufend = 1297019; bufvoid = 104857600\n","2021-11-12 20:18:54,086 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25949424(103797696); length = 264973/6553600\n","2021-11-12 20:18:54,154 INFO mapred.MapTask: Finished spill 0\n","2021-11-12 20:18:54,156 INFO mapred.Task: Task:attempt_local29580245_0001_m_000004_0 is done. And is in the process of committing\n","2021-11-12 20:18:54,160 INFO mapred.LocalJobRunner: Records R/W=654/1\n","2021-11-12 20:18:54,160 INFO mapred.Task: Task 'attempt_local29580245_0001_m_000004_0' done.\n","2021-11-12 20:18:54,161 INFO mapred.Task: Final Counters for attempt_local29580245_0001_m_000004_0: Counters: 23\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=4077\n","\t\tFILE: Number of bytes written=22192047\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=575031428\n","\t\tHDFS: Number of bytes written=0\n","\t\tHDFS: Number of read operations=13\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=1\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=66244\n","\t\tMap output records=66244\n","\t\tMap output bytes=1297019\n","\t\tMap output materialized bytes=1429513\n","\t\tInput split bytes=99\n","\t\tCombine input records=0\n","\t\tSpilled Records=66244\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=16\n","\t\tTotal committed heap usage (bytes)=514850816\n","\tFile Input Format Counters \n","\t\tBytes Read=38144132\n","2021-11-12 20:18:54,161 INFO mapred.LocalJobRunner: Finishing task: attempt_local29580245_0001_m_000004_0\n","2021-11-12 20:18:54,161 INFO mapred.LocalJobRunner: map task executor complete.\n","2021-11-12 20:18:54,167 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2021-11-12 20:18:54,172 INFO mapred.LocalJobRunner: Starting task: attempt_local29580245_0001_r_000000_0\n","2021-11-12 20:18:54,185 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2021-11-12 20:18:54,185 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2021-11-12 20:18:54,186 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2021-11-12 20:18:54,191 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@41cff9ce\n","2021-11-12 20:18:54,192 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2021-11-12 20:18:54,211 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2021-11-12 20:18:54,213 INFO reduce.EventFetcher: attempt_local29580245_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2021-11-12 20:18:54,249 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local29580245_0001_m_000002_0 decomp: 4965662 len: 4965666 to MEMORY\n","2021-11-12 20:18:54,256 INFO reduce.InMemoryMapOutput: Read 4965662 bytes from map-output for attempt_local29580245_0001_m_000002_0\n","2021-11-12 20:18:54,258 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4965662, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4965662\n","2021-11-12 20:18:54,263 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local29580245_0001_m_000003_0 decomp: 5010631 len: 5010635 to MEMORY\n","2021-11-12 20:18:54,269 INFO reduce.InMemoryMapOutput: Read 5010631 bytes from map-output for attempt_local29580245_0001_m_000003_0\n","2021-11-12 20:18:54,270 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5010631, inMemoryMapOutputs.size() -> 2, commitMemory -> 4965662, usedMemory ->9976293\n","2021-11-12 20:18:54,278 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local29580245_0001_m_000000_0 decomp: 5088899 len: 5088903 to MEMORY\n","2021-11-12 20:18:54,284 INFO reduce.InMemoryMapOutput: Read 5088899 bytes from map-output for attempt_local29580245_0001_m_000000_0\n","2021-11-12 20:18:54,284 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5088899, inMemoryMapOutputs.size() -> 3, commitMemory -> 9976293, usedMemory ->15065192\n","2021-11-12 20:18:54,286 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local29580245_0001_m_000004_0 decomp: 1429509 len: 1429513 to MEMORY\n","2021-11-12 20:18:54,289 INFO reduce.InMemoryMapOutput: Read 1429509 bytes from map-output for attempt_local29580245_0001_m_000004_0\n","2021-11-12 20:18:54,289 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1429509, inMemoryMapOutputs.size() -> 4, commitMemory -> 15065192, usedMemory ->16494701\n","2021-11-12 20:18:54,294 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local29580245_0001_m_000001_0 decomp: 5084269 len: 5084273 to MEMORY\n","2021-11-12 20:18:54,299 INFO reduce.InMemoryMapOutput: Read 5084269 bytes from map-output for attempt_local29580245_0001_m_000001_0\n","2021-11-12 20:18:54,299 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5084269, inMemoryMapOutputs.size() -> 5, commitMemory -> 16494701, usedMemory ->21578970\n","2021-11-12 20:18:54,301 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2021-11-12 20:18:54,302 INFO mapred.LocalJobRunner: 5 / 5 copied.\n","2021-11-12 20:18:54,303 INFO reduce.MergeManagerImpl: finalMerge called with 5 in-memory map-outputs and 0 on-disk map-outputs\n","2021-11-12 20:18:54,312 INFO mapred.Merger: Merging 5 sorted segments\n","2021-11-12 20:18:54,312 INFO mapred.Merger: Down to the last merge-pass, with 5 segments left of total size: 21578865 bytes\n","2021-11-12 20:18:54,961 INFO reduce.MergeManagerImpl: Merged 5 segments, 21578970 bytes to disk to satisfy reduce memory limit\n","2021-11-12 20:18:54,961 INFO reduce.MergeManagerImpl: Merging 1 files, 21578966 bytes from disk\n","2021-11-12 20:18:54,962 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2021-11-12 20:18:54,962 INFO mapred.Merger: Merging 1 sorted segments\n","2021-11-12 20:18:54,963 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 21578941 bytes\n","2021-11-12 20:18:54,964 INFO mapred.LocalJobRunner: 5 / 5 copied.\n","2021-11-12 20:18:54,973 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n","2021-11-12 20:18:54,977 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2021-11-12 20:18:54,978 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2021-11-12 20:18:55,050 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:55,053 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:55,055 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:55,066 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:55,207 INFO streaming.PipeMapRed: Records R/W=6554/1\n","2021-11-12 20:18:55,259 INFO streaming.PipeMapRed: R/W/S=10000/3574/0 in:NA [rec/s] out:NA [rec/s]\n","2021-11-12 20:18:56,086 INFO streaming.PipeMapRed: R/W/S=100000/76750/0 in:100000=100000/1 [rec/s] out:76750=76750/1 [rec/s]\n","2021-11-12 20:18:56,653 INFO streaming.PipeMapRed: R/W/S=200000/149082/0 in:200000=200000/1 [rec/s] out:149082=149082/1 [rec/s]\n","2021-11-12 20:18:57,013 INFO streaming.PipeMapRed: R/W/S=300000/223522/0 in:150000=300000/2 [rec/s] out:111761=223522/2 [rec/s]\n","2021-11-12 20:18:57,397 INFO streaming.PipeMapRed: R/W/S=400000/303642/0 in:200000=400000/2 [rec/s] out:151821=303642/2 [rec/s]\n","2021-11-12 20:18:57,734 INFO streaming.PipeMapRed: R/W/S=500000/375767/0 in:250000=500000/2 [rec/s] out:187883=375767/2 [rec/s]\n","2021-11-12 20:18:58,118 INFO streaming.PipeMapRed: R/W/S=600000/454065/0 in:200000=600000/3 [rec/s] out:151355=454065/3 [rec/s]\n","2021-11-12 20:18:58,348 INFO streaming.PipeMapRed: R/W/S=700000/483151/0 in:233333=700000/3 [rec/s] out:161050=483151/3 [rec/s]\n","2021-11-12 20:18:58,552 INFO streaming.PipeMapRed: R/W/S=800000/502305/0 in:266666=800000/3 [rec/s] out:167435=502305/3 [rec/s]\n","2021-11-12 20:18:58,783 INFO streaming.PipeMapRed: R/W/S=900000/522106/0 in:300000=900000/3 [rec/s] out:174035=522106/3 [rec/s]\n","2021-11-12 20:18:59,009 INFO streaming.PipeMapRed: R/W/S=1000000/543637/0 in:250000=1000000/4 [rec/s] out:135909=543637/4 [rec/s]\n","2021-11-12 20:18:59,021 INFO streaming.PipeMapRed: MRErrorThread done\n","2021-11-12 20:18:59,022 INFO streaming.PipeMapRed: mapRedFinished\n","2021-11-12 20:18:59,444 INFO mapred.Task: Task:attempt_local29580245_0001_r_000000_0 is done. And is in the process of committing\n","2021-11-12 20:18:59,447 INFO mapred.LocalJobRunner: 5 / 5 copied.\n","2021-11-12 20:18:59,448 INFO mapred.Task: Task attempt_local29580245_0001_r_000000_0 is allowed to commit now\n","2021-11-12 20:18:59,475 INFO output.FileOutputCommitter: Saved output of task 'attempt_local29580245_0001_r_000000_0' to hdfs://localhost:9000/user/job/req2-posts/output\n","2021-11-12 20:18:59,476 INFO mapred.LocalJobRunner: Records R/W=6554/1 > reduce\n","2021-11-12 20:18:59,477 INFO mapred.Task: Task 'attempt_local29580245_0001_r_000000_0' done.\n","2021-11-12 20:18:59,478 INFO mapred.Task: Final Counters for attempt_local29580245_0001_r_000000_0: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=43162193\n","\t\tFILE: Number of bytes written=43771013\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=575031428\n","\t\tHDFS: Number of bytes written=21168309\n","\t\tHDFS: Number of read operations=18\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=3\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=545197\n","\t\tReduce shuffle bytes=21578990\n","\t\tReduce input records=1000000\n","\t\tReduce output records=545198\n","\t\tSpilled Records=1000000\n","\t\tShuffled Maps =5\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=5\n","\t\tGC time elapsed (ms)=31\n","\t\tTotal committed heap usage (bytes)=514850816\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=21168309\n","2021-11-12 20:18:59,478 INFO mapred.LocalJobRunner: Finishing task: attempt_local29580245_0001_r_000000_0\n","2021-11-12 20:18:59,478 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2021-11-12 20:19:00,309 INFO mapreduce.Job:  map 100% reduce 100%\n","2021-11-12 20:19:00,310 INFO mapreduce.Job: Job job_local29580245_0001 completed successfully\n","2021-11-12 20:19:00,331 INFO mapreduce.Job: Counters: 36\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=43177438\n","\t\tFILE: Number of bytes written=118965366\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\t\tHDFS: Number of bytes read=2492281096\n","\t\tHDFS: Number of bytes written=21168309\n","\t\tHDFS: Number of read operations=63\n","\t\tHDFS: Number of large read operations=0\n","\t\tHDFS: Number of write operations=8\n","\t\tHDFS: Number of bytes read erasure-coded=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1000000\n","\t\tMap output records=1000000\n","\t\tMap output bytes=19578960\n","\t\tMap output materialized bytes=21578990\n","\t\tInput split bytes=495\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=545197\n","\t\tReduce shuffle bytes=21578990\n","\t\tReduce input records=1000000\n","\t\tReduce output records=545198\n","\t\tSpilled Records=2000000\n","\t\tShuffled Maps =5\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=5\n","\t\tGC time elapsed (ms)=130\n","\t\tTotal committed heap usage (bytes)=2941255680\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=575031428\n","\tFile Output Format Counters \n","\t\tBytes Written=21168309\n","2021-11-12 20:19:00,331 INFO streaming.StreamJob: Output directory: /user//job/req2-posts/output\n"]}]},{"cell_type":"code","metadata":{"id":"0vRplFuhtMR9"},"source":["!${hdfs} dfs -copyToLocal  /user/$USER/job/req2-posts/output  /content/drive/MyDrive/mp1-bigdata/req2-posts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SDuFAlHOuQsB","executionInfo":{"status":"ok","timestamp":1636748306026,"user_tz":-120,"elapsed":3237,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"a8e90e49-55db-40b3-ed2c-bfa15eee3da0"},"source":["!${hdfs} dfs -rm -r  /user/$USER/job/req2-posts/output "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted /user/job/req2-posts/output\n"]}]},{"cell_type":"markdown","metadata":{"id":"M6Yd-EVi9-WL"},"source":["## Trying things out - debugging"]},{"cell_type":"code","metadata":{"id":"wg1Vd3IAxtox","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637611528103,"user_tz":-120,"elapsed":989,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhDaGRMxyeD3t85VD8VHhTPNz3NiYujdbYaaDwHMw=s64","userId":"07034748551478156399"}},"outputId":"64157480-bd30-464d-d714-105838e007a9"},"source":["!head -n 10 /content/drive/MyDrive/mp1-bigdata/req1/output3/part-00000>> r1-o.txt\n","!head -n 10 /content/drive/MyDrive/mp1-bigdata/req2/output/part-00000>> r2-o.txt\n","!head -n 10 /content/drive/MyDrive/mp1-bigdata/req2-posts/output3/part-00000>> r2-posts-o.txt\n","\n","\n","!head -n 10 /content/drive/MyDrive/mp1-bigdata/req3/output3/part-00000>> r3-o.txt\n","!head -n 10 /content/drive/MyDrive/mp1-bigdata/creq1/output3/part-00000>> c-r1-o.txt\n","!head -n 10 /content/drive/MyDrive/mp1-bigdata/creq1/output3/part-00000>> c-r2-o.txt\n","\n","#!head -n 10 /content/drive/MyDrive/mp1-bigdata/raw_data/RC_2015-01 >> sample.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["head: cannot open '/content/drive/MyDrive/mp1-bigdata/req2/output/part-00000' for reading: No such file or directory\n","head: cannot open '/content/drive/MyDrive/mp1-bigdata/req2-posts/output3/part-00000' for reading: No such file or directory\n","head: cannot open '/content/drive/MyDrive/mp1-bigdata/req3/output3/part-00000' for reading: No such file or directory\n","head: cannot open '/content/drive/MyDrive/mp1-bigdata/creq1/output3/part-00000' for reading: No such file or directory\n","head: cannot open '/content/drive/MyDrive/mp1-bigdata/creq1/output3/part-00000' for reading: No such file or directory\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mv6kwtE_obOM","executionInfo":{"status":"ok","timestamp":1636745482708,"user_tz":-120,"elapsed":532,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"506cd4b2-847a-439e-e23c-d3a62b6f801f"},"source":["open('sample.txt','rb').readline()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["b'{\"score_hidden\":false,\"name\":\"t1_cnas8zv\",\"link_id\":\"t3_2qyr1a\",\"body\":\"Most of us have some family members like this. *Most* of my family is like this. \",\"downs\":0,\"created_utc\":\"1420070400\",\"score\":14,\"author\":\"YoungModern\",\"distinguished\":null,\"id\":\"cnas8zv\",\"archived\":false,\"parent_id\":\"t3_2qyr1a\",\"subreddit\":\"exmormon\",\"author_flair_css_class\":null,\"author_flair_text\":null,\"gilded\":0,\"retrieved_on\":1425124282,\"ups\":14,\"controversiality\":0,\"subreddit_id\":\"t5_2r0gj\",\"edited\":false}\\n'"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVKH93Iy2mNg","executionInfo":{"status":"ok","timestamp":1636747329850,"user_tz":-120,"elapsed":905,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"51ebe60b-d250-4588-e1af-40ee7ada602d"},"source":["%%file mapper.py\n","#!/usr/bin/python3\n","import json\n","import sys\n","\n","for line in sys.stdin:\n","    line= json.loads(line.strip())\n","    \n","    print(line['parent_id'], line['controversiality'],1 ,sep='|~|' )\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting mapper.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFmRz3KQD3hq","executionInfo":{"status":"ok","timestamp":1636749118955,"user_tz":-120,"elapsed":369,"user":{"displayName":"Asmaa Ismail 201-600-780","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8zPBL3JX1f9EmdMGz3juwHzf_M9foX6f1cASwxg=s64","userId":"07034748551478156399"}},"outputId":"2fb5b3be-ad18-43f3-9de0-da10de698be4"},"source":["%%file reducer.py\n","#!/usr/bin/python3\n","\n","import sys\n","import json\n","\n","parent = None\n","count = 0\n","contr_sum = 0\n","print('parent_id (post):                reply frequency:                      contraversiality: ')\n","for line in sys.stdin:\n","    parent_key, contr,value = line.strip().split('|~|')\n","    if parent is None:\n","        parent = parent_key\n","    elif parent != parent_key:\n","        print(parent, count,contr_sum, sep='\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t')\n","        parent = parent_key\n","        contr_sum=0\n","        count = 0\n","    count += int(value)\n","    contr_sum +=int(contr)\n","print(parent, count, contr_sum, sep='\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting reducer.py\n"]}]},{"cell_type":"code","metadata":{"id":"h7SESQDPq0Cr"},"source":["!cp mapper.py  /content/drive/MyDrive/mp1-bigdata/req2-posts/\n","!cp reducer.py  /content/drive/MyDrive/mp1-bigdata/req2-posts/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"not_xi954nYk"},"source":["!python mapper.py <sample.txt > out.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOwjPTjW2CbV"},"source":["!python reducer.py <out.txt >>final_out.txt"],"execution_count":null,"outputs":[]}]}